[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EFSA Tutorial",
    "section": "",
    "text": "Warning\n\n\n\nTHIS PAGE IS CURRENTLY UNDER CONSTRUCTION\n\n\n \n\n\n\n\n\n \n\nSTART HERE\n\n \n\nLearn more about principles and methods for uncertainty analysis and communication of uncertainty in scientific assessments."
  },
  {
    "objectID": "implementationexpressions.html",
    "href": "implementationexpressions.html",
    "title": "Expressions of Uncertainty",
    "section": "",
    "text": "Here we will have the interactive data table of examples. Currently, we plan to have the following fields in the table:\n\nTitle\nDOI\nPanel\nExpression of uncertainty (from the nine expressions)\nQuantitative method for uncertainty analysis\nQualitative method for uncertainty analysis\nDescription\n\nThe dataframe below is provided as a placeholder."
  },
  {
    "objectID": "uaguidance/dividing.html",
    "href": "uaguidance/dividing.html",
    "title": "Dividing the assessment into parts",
    "section": "",
    "text": "Often an assessment will comprise a number of main parts (e.g. exposure and hazard in a chemical risk assessment) and smaller, subsidiary parts (e.g. individual parameters, studies, or lines of evidence within the exposure or hazard assessment).\nThe uncertainty analysis may also be divided into parts. Assessors should choose at what level to conduct it:\n\nEvaluate all uncertainties collectively, for the assessment as a whole.\nDivide the uncertainty analysis into parts, which evaluate uncertainties separately in major parts of the scientific assessment (e.g. exposure and hazard in a risk assessment). Then, combine the parts of the uncertainty analysis and include also any other identified uncertainties that relate to other parts of the scientific assessment as a whole, so as to characterise the overall uncertainty.\nDivide the uncertainty analysis into still smaller parts, corresponding to still smaller parts of the scientific assessment (e.g. every input of a calculation or model). Evaluate uncertainty collectively within each of the smaller parts, combine them into the main parts, and combine those to characterise overall uncertainty for the whole assessment.\n\nIf the uncertainty analysis will be divided into parts, assessors will need to combine them to characterise overall uncertainty. Assessors should define in advance how the parts will be combined, as this will increase transparency and rigour. It is recommended to use a conceptual model diagram (see glossary for explanation) to show how the parts will be combined. The parts may be combined by expert judgement (Section 12.6), or by calculation (Sections 13, 14 or 15) if assessors quantify the uncertainty for each part and can specify an appropriate quantitative or logical model to combine them. Calculation is likely to give more reliable results, but should be weighed against the additional work involved.\nAssessors should judge what is best suited to the needs of each assessment. For example, it may be more efficient to evaluate uncertainty for different parts separately if they require different expertise (e.g. toxicity and exposure). Evaluating all uncertainties collectively (first option in point (2) above) is generally quicker and superficially simpler but requires integrating them all subjectively by expert judgement, which may be less reliable than evaluating different parts of the uncertainty analysis separately, if they are then combined by calculation. For this reason, it is recommended to treat separately those parts of the assessment that are affected by larger uncertainties (identified by a simple initial prioritisation, see Section 8).\nWhen a part of the scientific assessment is treated separately in the uncertainty analysis, it is not necessary to evaluate immediately all of the uncertainties affecting it; some of them can be set to one side and considered later as part of the overall characterisation of uncertainty, if this is more convenient for the assessor. However, it is recommended that only the lesser uncertainties are deferred to the overall characterisation, since it will be more reliable to combine the larger uncertainties by calculation.\nWhen the scientific assessment includes a quantitative or logical model, assessors may find it convenient to quantify uncertainty separately for every parameter of the model. In such cases, it will still be necessary to identify additional uncertainties that are not quantified within the model, e.g. uncertainties relating to the structure of the model (see Section 7.2) and take them into account in the characterisation of overall uncertainty (Section 16). In other cases, assessors might find it sufficient to analyse all the uncertainties affecting a model collectively (simplest option in point (2) above), or for major parts of the model without separating the individual parameters (intermediate option in point (2))."
  },
  {
    "objectID": "uaguidance/reporting.html",
    "href": "uaguidance/reporting.html",
    "title": "Reporting uncertainty analysis",
    "section": "",
    "text": "Section 17"
  },
  {
    "objectID": "uaguidance/figure7.html",
    "href": "uaguidance/figure7.html",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([< BACK]) --> B[Perform the</br>scientific assessment</br>and uncertainty analysis</br>together] \n  B --> K[Quantify uncertainty</br>for each part of</br>the uncertainty analysis]\n  B --> C[Evaluate uncertainty</br>for each part,</br>qualitatively or quantitatively]\n  K --> L[Combine the parts</br>using a suitable</br>logic model]\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Decide how</br>to express</br>the uncertainty] --> E[Elicit</br>a probability judgement</br>for the overall</br>uncertainty]\n    E --> F[Check for</br>and describe</br>any unquantified</br>uncertainties]\n    M[Take account</br>of the contribution</br>of any additional</br>uncertainties] --> N[Check for</br>and describe any</br>unquantified uncertainties]\n  end\n  L-->M\n    C --> D\n    F --> G[[Report conclusion</br>in form needed</br>by decision makers,</br>and detailed analysis</br>in opinion</br>or annex]]\n    N --> G  \n  click A \"figure5.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure3.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uaguidance/figure10.html",
    "href": "uaguidance/figure10.html",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([< BACK]) --> B[Perform the</br>scientific assessment</br>and uncertainty analysis</br>together] \n  B --> C[Do you want</br>to try the simpler</br>option of</br>using only bounded</br>probabilities?]\n  C --Yes-->D[Quantify uncertainty</br>for each part of</br>the uncertainty analysis,</br>using probability bounds for</br>both uncertainty and variability]\n  D--> F[Combine the parts</br>by probability bounds</br> calculation]\n  C --No -->E[Quantify uncertainty</br>for each part of</br>the uncertainty analysis,</br>using 2D distributions</br>for variable quantities]\n  E --> G[Combine the parts</br>by 2D Monte Carlo</br>simulation]\n    subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    H[Take account</br>of the contribution</br>of any additional</br>uncertainties] --> I[Check for</br>and describe any</br>unquantified uncertainties]\n  end\n  F & G --> ov\n  ov--> J[[Report conclusion</br>in form needed</br>by decision makers,</br>and detailed analysis</br>in opinion</br>or annex]]\n  click A \"figure5.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uaguidance/prioritizing.html",
    "href": "uaguidance/prioritizing.html",
    "title": "Prioritizing uncertainties",
    "section": "",
    "text": "Prioritising sources of uncertainty may be useful at different stages of the assessment and uncertainty analysis. In the early stages, it can be used to select more important uncertainties to be analysed by more refined methods, e.g. to be evaluated individually rather than collectively, to be expressed with probabilities or distributions rather than bounds, to be elicited by more rather than less formal methods, etc. Prioritisation can also be used during the course of an assessment, to identify parts of the assessment where it might be beneficial to search for more data, use more complex models, or invite additional experts. At the end of the assessment, it may be useful to prioritise uncertainties to identify potential areas for further research.\nPrioritisation, at any stage of the assessment, should be based on the contribution of individual sources of uncertainty to the uncertainty of the assessment as a whole. This is determined by a combination of the magnitude of each uncertainty and how much it affects the result of the assessment, both of which need to be taken into account [SO5.7].\nThe relative influence of different uncertainties can be assessed in a simple and approximate way using qualitative methods based on expert judgement. An ordinal scale can be used to express expert judgements of the magnitude and/or direction of impact of each uncertainty on the question or quantity of interest, as in ‘uncertainty tables’ [SO10.5 and 10.6]. Or separate ordinal scales could be used to express judgements of the magnitude of each uncertainty and its influence, as in the Numeral, Unit, Spread, Assessment and Pedigree (NUSAP) approach [SO10.4].\nWhen the assessment involves a calculation or quantitative model, the contributions of uncertainties about the model inputs can be assessed rigorously by sensitivity analysis. These range from simple ‘what if’ calculations and ‘minimal assessment’ (EFSA, 2014a) to sophisticated sensitivity analyses [see SO12] for which specialist help might be required (Section 1.7). The influence of uncertainties relating to choices regarding the structure of the model or assessment may need to be addressed by repeating the assessment with alternative choices. Prioritisation at the early stages of an assessment must necessarily be done by expert judgement or by sensitivity analysis using a preliminary model, as the assessment model is still under development."
  },
  {
    "objectID": "uaguidance/figure1.html",
    "href": "uaguidance/figure1.html",
    "title": "Uncertainty analysis for standardised assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TD\n  A([< BACK]) --> B{Check every part</br>of assessment</br>for nonstandard</br>uncertainties}\n  B -- None identified --> C[[Document in the opinion that</br>non-standard uncertainties</br>were checked for</br>and none were identified]]\n  B --One or more identified --> Z([NEXT >])\n  click A \"../uncertaintyanalysis.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure2.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uaguidance/overall.html",
    "href": "uaguidance/overall.html",
    "title": "Characterising overall uncertainty",
    "section": "",
    "text": "Section 16"
  },
  {
    "objectID": "uaguidance/figure11.html",
    "href": "uaguidance/figure11.html",
    "title": "Uncertainty analysis for urgent assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([< BACK]) --> B[Conduct the assessment,</br>listing uncertainties</br>you identify] \n  B --> C[Ensure the question</br>or quantity of interest</br>is well-defined]\n  C --> D[Decide how to</br>express the uncertainty]\n  D --> E[Quickly check</br>the assessment for</br>additional uncertainties]\n  E --> ov\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n F[Elicit</br>a probability judgement</br>for the overall</br>uncertainty] --> G[Check for</br>and describe</br>any unquantified</br>uncertainties]\n  end\n    ov --> H[[Report conclusion</br>in form needed</br>by decision makers,</br>and detailed analysis</br>in opinion</br>or annex]]\n  \n  click A \"../uncertaintyanalysis.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uaguidance/identifying.html",
    "href": "uaguidance/identifying.html",
    "title": "Identifying uncertainties",
    "section": "",
    "text": "Standard uncertainties are those that are considered (implicitly or explicitly) to be addressed by the provisions of a standardised procedure or standardised assessment element. For example, uncertainties due to within and between species differences in toxicity are often addressed by a default factor of 100 in chemical risk assessment.\nAll other uncertainties are non-standard uncertainties. These include any deviations from a standardised procedure or standardised assessment element that lead to uncertainty regarding the result of the procedure.\n\nBoth standard and non-standard uncertainties may be found in any type of assessment, but the proportions vary. It is recommended that EFSA’s Panels include lists of standard uncertainties within the documentation for standard procedures, as this will help assessors to distinguish standard and non-standard uncertainties."
  },
  {
    "objectID": "uaguidance/identifying.html#procedure-for-identifying-uncertainties",
    "href": "uaguidance/identifying.html#procedure-for-identifying-uncertainties",
    "title": "Identifying uncertainties",
    "section": "Procedure for identifying uncertainties",
    "text": "Procedure for identifying uncertainties\n\nEvery assessment must say what sources of uncertainty have been identified.\nAssessors should systematically examine every part of their assessment for uncertainties, including both the inputs to the assessment (e.g. data, estimates, other evidence) and the methods used in the assessment (e.g. statistical methods, calculations or models, reasoning, expert judgement), to minimise the risk that important uncertainties are overlooked.\nUncertainties affecting assessment inputs are identified when appraising the evidence retrieved from literature or from existing databases.\nUncertainties affecting the methods used in the assessment are generally not addressed by existing frameworks for evidence appraisal. It is therefore recommended that assessors use the right column of Table 1 (referring to [SO8.1] for details and explanation) as a guide to what types of uncertainty to look for in the methods of their assessment.\nAssessors are advised to avoid spending excessive time trying to match uncertainties to the types listed in Table 1 or other frameworks: the purpose of the lists is to facilitate identification of uncertainties, not to classify them.\nAssessors should determine which of the uncertainties they identify in an assessment are standard and which are non-standard (Section 7.1), as this will affect their treatment in subsequent stages of the uncertainty analysis."
  },
  {
    "objectID": "uaguidance/futureinvestigation.html",
    "href": "uaguidance/futureinvestigation.html",
    "title": "Prioritising uncertainties for future investigation",
    "section": "",
    "text": "Section 8"
  },
  {
    "objectID": "uaguidance/characterizing.html",
    "href": "uaguidance/characterizing.html",
    "title": "Characterising uncertainty for parts of the uncertainty analysis",
    "section": "",
    "text": "Section 11-12"
  },
  {
    "objectID": "uaguidance/figure2.html",
    "href": "uaguidance/figure2.html",
    "title": "Standardized Assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([< BACK]) --> B{Are assessors able</br>to evaluate</br>the non-standard</br> uncertainties</br> collectively?}\n      B -- Yes --> C[Define the question</br>or quantity of interest]\n      C --> ov\n  B -- No --> Z([NEXT >])\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Decide</br>how to express</br>the uncertainty] --> E[Elicit</br>a probability judgement</br>for the overall</br>uncertainty]\n    E --> F[Check for</br>and describe</br>any unquantified</br>uncertainties]\n  end\n    ov --> G[[Report conclusion</br>in form needed</br>by decision makers,</br>and detailed analysis</br>in opinion</br>or annex]]\n  \n  click A \"figure1.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure3.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uaguidance/figure3.html",
    "href": "uaguidance/figure3.html",
    "title": "Standardized Assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Take account</br>of the contribution</br>of any additional uncertainties] --> E[Check for and</br>describe any </br>unquantified uncertainties]\n  end\n  A([< BACK]) --> B[Divide the uncertainty analysis</br>into convenient parts,</br>define the question or</br>quantity of interest </br>for each part, and</br>an appropriate calculation</br>for combining the parts]\n  B --> C[Elicit probability</br> bounds for each part,</br> and combine probability</br>bounds by calculation]\n  C --> ov\n  ov --> F{Is the result</br>expected to be sufficient</br>for decision-making?}\n  F -- Yes--> G[[Report conclusion</br>in form needed by</br>decision-makers,</br>and detailed analysis</br>in opinion or annex]]\n  F -- No --> Z([NEXT >])\n  click A \"figure2.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure4.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uaguidance/figure5.html",
    "href": "uaguidance/figure5.html",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TD\n  A([< BACK]) -->  B[Plan your</br>scientific assessment</br>in your usual manner]\n  B --> C[Identify uncertainties</br>systematically in all</br>parts of your assessment]\n  C --> D{Do you want</br>to evaluate</br>all uncertainties</br>collectively in</br>a single step?}\n  D--Yes--> E([NEXT >])\n  D--No--> F[Divide the uncertainty analysis</br>into convenient parts,</br>define the question</br>or quantity of interest</br>for each part,</br>and an appropriate conceptual</br>model for combining the parts]\n  F-->G{Do all parts</br>relate to</br>yes/no questions?}\n  G--Yes-->H([NEXT >])\n  G--No--> I{Do any part</br>relate to</br>variable quantities}\n  I --Yes--> K([NEXT >])\n  I --No--> L([NEXT >])\n  click A \"../uncertaintyanalysis.html\" \"Go Back\" _self\n  click E \"figure6.html\" \"Go Forward\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click H \"figure7.html\" \"Go Forward\" _self\n  click L \"figure9.html\" \"Go Forward\" _self\n  click K \"figure10.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,E,H,L,K dark"
  },
  {
    "objectID": "uaguidance/figure12.html",
    "href": "uaguidance/figure12.html",
    "title": "Uncertainty analysis for urgent assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([< BACK]) --> B[Ensure the inputs,</br>methods and outputs</br>of the standard procedure</br>are well-defined] \n  B --> C[Define the class</br>of problems or applications</br>this procedure</br>will be used for]\n  C --> D[Agree the management</br>objective for the</br>standard procedure]\n  D --> E[Design and perform</br>a scientific assessment</br>and uncertainty analysis</br>of the extent to which</br>the standard procedure</br>will achieve the management objective]\n  E --> F{Is there</br>sufficient probability</br>of achieving</br>the management objective</br>to an acceptable extent?}\n  F -- Yes --> G[Document the assessment</br>and uncertainty analysis</br>of the standard procedure]\n  F -- No --> H[Modify the standard</br>procedure in ways that</br>you expect to achieve</br>the management objective]\n  H --> I[Redo the assessment</br>and uncertainty analysis</br>for the modified procedure]\n  I -.-> E\n  G --> J[[Make the standard</br>procedure available</br>for use]]\n  \n  \n  click A \"../uncertaintyanalysis.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uaguidance/welldefined.html",
    "href": "uaguidance/welldefined.html",
    "title": "Ensuring that the questions or quantities of interest are well-defined",
    "section": "",
    "text": "In order to evaluate uncertainty, the questions and/or quantities of interest for the assessment must be well-defined. This applies both to the assessment as a whole and to different parts of the uncertainty analysis, if it is separated into parts. Any ambiguity in the definition of questions or quantities of interest will add extra uncertainty and make the evaluation more difficult. When a question or quantity of interest is not already well-defined for the purpose of scientific assessment, assessors should define it well for the purpose of uncertainty analysis.\nA quantity or question of interest is well-defined if, at least in principle, it could be determined in such a way that assessors would be sure to agree on the answer. A practical way to achieve this is by specifying an experiment, study or procedure that could be undertaken, at least in principle, and which would determine the true answer for the question or quantity with certainty [see SO5.1 for more discussion]. For example:\n\na well-defined measure for a quantity of interest, and the time, population or location, and conditions (e.g. status quo or with specified management actions) for which the measure will be considered;\nfor a question of interest, the presence or absence of one or more clearly-defined states, conditions, mechanisms, etc., of interest for the assessment, and the time, population or location, and conditions (e.g. status quo or with specified management actions) for which this will be considered;\nthe result of a clearly-defined scientific study, procedure or calculation, which is established (e.g. in legislation or guidance) as being relevant for the assessment.\n\nWhen drafting the definition of each question or quantity of interest, check each word in turn. Identify words that are ambiguous (e.g. high), or imply a risk management judgement (e.g. negligible, safe). Replace or define them with words that are, as far as possible, unambiguous and free of risk management connotations or, where appropriate, with numbers.\nSometimes the Terms of Reference for an assessment are very open, e.g. requesting a review of the literature on an area of science. In such cases, assessors should seek to ensure the conclusions they produce either refer to well-defined quantities, or contain welldefined statements that can be considered as answers to well-defined questions, in one of the three forms listed above (point 2, options a–c). This is necessary both for transparency and so that assessors can evaluate and express the uncertainty associated with their conclusions."
  },
  {
    "objectID": "uaguidance/figure6.html",
    "href": "uaguidance/figure6.html",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([< BACK]) --> B[Conduct the assessment</br>as planned, noting</br>any further</br>uncertainties you identify] \n  B --> C[Ensure the question</br>or quantity of interest</br>is well-defined]\n  C --> ov\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Decide</br>how to express</br>the uncertainty] --> E[Elicit</br>a probability judgement</br>for the overall</br>uncertainty]\n    E --> F[Check for</br>and describe</br>any unquantified</br>uncertainties]\n  end\n    ov --> G[[Report conclusion</br>in form needed</br>by decision makers,</br>and detailed analysis</br>in opinion</br>or annex]]\n  \n  click A \"figure5.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure3.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uaguidance/figure9.html",
    "href": "uaguidance/figure9.html",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([< BACK]) --> B[Perform the</br>scientific assessment</br>and uncertainty analysis</br>together] \n  B --> C[Do you want</br>to try the simpler</br>option of</br>using only bounded</br>probabilities?]\n  C --Yes-->D[Obtain probability</br>bounds for each</br>part of the</br>uncertainty analysis]\n  D--> F[Combine the parts</br>by probability bounds</br> calculation]\n  C --No -->E[Obtain a probability</br>or distribution for</br>each part of the</br>uncertainty analysis]\n  E --> G[Combine the parts</br>by 1D Monte Carlo</br>simulation]\n    subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    H[Take account</br>of the contribution</br>of any additional</br>uncertainties] --> I[Check for</br>and describe any</br>unquantified uncertainties]\n  end\n  F & G --> ov\n  ov--> J[[Report conclusion</br>in form needed</br>by decision makers,</br>and detailed analysis</br>in opinion</br>or annex]]\n  click A \"figure5.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure3.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uaguidance/figure4.html",
    "href": "uaguidance/figure4.html",
    "title": "Standardized Assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Take account</br>of the contribution</br>of any additional uncertainties] --> E[Check for and</br>describe any </br>unquantified uncertainties]\n  end\n  A([< BACK]) --> B[Obtain a probability</br> or a distribution</br> for each part of the</br> uncertainty analysis]\n  B --> C[Combine the parts</br> using the calculation</br> chosen earlier]\n  C --> ov\n  ov --> G[[Report conclusion</br>in form needed by</br>decision-makers,</br>and detailed analysis</br>in opinion or annex]]\n  \n  click A \"figure3.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C \"figure3.html\" \"See Section 17 regarding reporting.\"\n  click Z \"figure4.html\"\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uaguidance/combining.html",
    "href": "uaguidance/combining.html",
    "title": "Combining uncertainty from different parts of the uncertainty analysis",
    "section": "",
    "text": "Section 13-15"
  },
  {
    "objectID": "comguidance/communicator.html",
    "href": "comguidance/communicator.html",
    "title": "for Communicators",
    "section": "",
    "text": "1. Did the scientific assessment for this message identify any non-standard uncertainties?\n\n\n\n\n\n\nYesNo\n\n\nIf an unqualified conclusion is required, follow the guidance for unqualified conclusions (see Question 5 here). If an unqualified conclusion is not required, state the result of the standardised procedure in the form expressed by the assessors. Also communicate the uncertainty expressions for this message, consulting the respective guidance boxes.\n\n\nReport the conclusion as expressed by assessors and state that a standardised assessment procedure was followed that takes account of standard uncertainties, and no non-standard uncertainties were identified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Does the scientific output list or describe the sources or causes of any uncertainties affecting this message?\n\n\n\n\n\n\nEntryInformed\n\n\n\nState that uncertainties exist, using the wording in the scientific output.\n\n\nExample: “The experts identified limitations in the data on exposure and toxic effects of ZEN and its modified forms” Based on EFSA 2017;15(7):4851\n\n\n\n\nState that uncertainties exist, using the wording in the scientific output.\nInclude in the message a brief description of the sources of uncertainty that have the biggest impact on the respective key messages. (If necessary, consult the assessors to identify these.)\n\n\nExample: “The experts identified limitations in the data on exposure and toxic effects of ZEN and its modified forms, for example (…)” Based on EFSA 2017;15(7):4851\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Is the direction and/or degree of uncertainty expressed verbally (i.e. with terms like ‘low’, ‘medium’, ‘high’) or with symbols (e.g. ‘+’ or ‘-’)?\n\n\n\n\n\n\nEntryInformed\n\n\n\nAvoid altering the wordings used by assessors to describe the direction and/or degree of uncertainty, or factors contributing to uncertainty (see Question 2 above). Always check the rewording with the assessors if you do.\nState clearly what outcomes and conditions this expression of uncertainty refers to (see Question 1 above).\nMake clear that any uncertainty referred to in the communication has been taken into account in the assessment conclusion.\n\n\nExample: “The Panel noted that there was very high uncertainty about the exposure estimates and took this into account in its conclusion that there is no health concern” Based on EFSA 2017;15(7):4851\n\n\n\nAs for entry level, with the following differences: - Before communicating the uncertainty expression, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment. – Optionally, mention specific methods that were used in evaluating the uncertainty. – Optionally, mention factors contributing to the overall uncertainty, including the relative importance of individual sources of uncertainty and things like the relevance and reliability of evidence (e.g. in weight of evidence assessments). – Clearly distinguish individual sources of uncertainty from overall uncertainty about the assessment conclusions\n\nExample: “The Panel noted that a high proportion of measurements of ZEN and its modified forms in feed were below the limit of detection, leading to very high uncertainty when estimating exposure” Based on EFSA 2017;15(7):4851\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. Does this message report an inconclusive assessment outcome?\n\n\n\n\n\n\nEntryInformed\n\n\n\nCommunicate clearly that EFSA is unable to give any conclusion on the quantity or question of interest to which this message refers. If the assessment is inconclusive, this implies that nothing further can be said and therefore the communication should avoid using language that might suggest otherwise.\nIndicate very briefly the sources of uncertainty that contribute most to this outcome (e.g. lack of data, poor quality or limited relevance of data).\n\n\nExample: “EFSA’s experts could not reach a conclusion on the risk for cattle, ducks, goats, horses, rabbits, mink and cats because of a lack of data” Based on EFSA 2017;15(7):4851\n\n\n\n\nDescribe the main sources of uncertainty in more detail, but concisely (following the guidance in Question 2 above).\nInconclusive assessments are especially likely to include options or requirements for obtaining further data. Communicate these as instructed in UC Section 3.1.6.\n\n\nExample: “EFSA’s experts could not reach a conclusion on the risk for cattle, ducks, goats, horses, rabbits, mink and cats due to limitations in available data on exposure and toxic effects of ZEN and itsmodified forms, for example (…)” Based on EFSA 2017;15(7):4851\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. Do risk managers expect or does legislation require that this message should be expressed as an unqualified conclusion, without any expression of uncertainty?\n\n\n\n\n\n\nEntryInformed\n\n\n\nReport the unqualified conclusion for this message using the same wording as the assessors.\n\n\nExample: “EFSA’s experts concluded that the exposure to feed containing ZEN ‘in farm situations’ is a low health risk for sheep, dog, pig and fish, and an extremely low health risk for poultry” Based on EFSA 2017;15(7):4851\nIn this example, the word ‘low’ refers to the conclusion on the level of health risk. There is no expression of uncertainty about this – no indication that the risk might be other than ‘low’, i.e. an unqualified conclusion about the level of risk.\n\n\n\n\nAs for the entry level.\nOptionally, describe briefly how the assessment was made (i.e. what evidence and methods were used to arrive at the conclusions).\nBriefly describe some examples of uncertainties affecting the assessment for this message, as identified in your completed template, consulting Box 4 for guidance on how to communicate this.\nIf the assessment contains any verbal or numerical expression of the impact of the uncertainties as identified in your template, follow the respective guidance in Boxes 6–9 below.\nSay that the assessors took the uncertainties into account when reaching their conclusion(s) for this message.\n\n\nExample: “Following the standard assessment procedure (or ‘Using the evaluation system agreed for contaminants in feed’), experts estimated that high exposure to feed containing ZEN is below the reference value for a health risk for sheep, dog, pig and fish, and well below the reference value for chicken and turkeys. They therefore concluded that the exposure to feed containing ZEN ‘in farm situations’ is a low health risk for sheep,dog,pig and fish,and an extremely low health risk for poultry. In reaching this conclusion, the experts took account of limitations in the data on exposure and toxic effects of ZEN and its modified forms, for example (…)” Based on EFSA 2017;15(7):4851\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6. Is the uncertainty described with numbers as a precise probability?\n\n\n\n\n\n\nEntryInformed\n\n\n\nState clearly what the probability refers to, including whether it refers to a numerical estimate or a qualitative conclusion. When the probability refers to a numerical estimate, also state the range of the quantity that the probability refers to (see example below).\n\n\nExample: “The Panel estimates that, under current regulations, the total number of infested tulips in in greenhouses in the EU is 60,000. Based on what is known, the Panel is 50% certain that the number is between 10,000 and 200,000 infested plants.” Based on EFSA 2017;15(8):4879\n\n\n\nAs for the entry level, with the following differences: - Before giving the probability, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account when assessing their level of certainty. – Optionally, mention specific methods that were used in quantifying the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\n\nExample: “The Panel performed its assessment using a mathematical model of the entry of nematodes into the EU and their establishment and spread in greenhouse tulips. Uncertainty on the factors represented in the model was quantified by expert judgement, taking into account the limitations of the available data. The Panel estimates…[continue as for entry level]” Based on EFSA 2017;15(8):4879\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7. Is the uncertainty described with numbers as an approximate probability?\n\n\n\n\n\n\nEntryInformed\n\n\n\nState clearly what the probability refers to, including whether it refers to a numerical estimate or a qualitative conclusion. When the probability refers to a numerical estimate, also state the range of the quantity that the probability refers to.\nAn approximate probability may comprise a range of probabilities chosen by the assessors from the approximate probability scale Table 1, or a different range of probabilities specified by the assessors.\nAlways communicate the quantitative range of probabilities because this expresses the assessors’ conclusion without ambiguity. If a verbal expression is also used, present the quantitative probability first (e.g. ‘66–90% certain (likely)’) because it has been shown that this order leads to more consistent understanding than if the verbal expression is presented first (see UC Section 3.1)\nTo avoid inconsistency and misunderstanding, do not use the verbal terms in Table 1 to refer to any probabilities or ranges of probabilities other than those shown in this table.\n\n\nExample: “The experts considered it 66–90% certain (likely) that the increasing proportion of elderly and susceptible people has contributed to the rise in Listeria cases” Based on EFSA 2018;16(1):5134\n\n\n\nAs for entry level, with the following differences: - Before giving the probability, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account when assessing their level of certainty. - Optionally, mention specific methods that were used in quantifying the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\n\nExample: “Experts began work on the Scientific Opinion after the 2015 EU summary report on foodborne zoonotic diseases identified an increasing trend of listeriosis over the period 2009–2013. The Panel performed a statistical analysis, which confirmed the increasing trend, and developed a mathematical model of the factors influencing the incidence of infections. Considering the modelling results and the degree of support from indicator data, the experts…’ [continue as for entry level]” Based on EFSA 2018;16(1):5134\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8. Is the uncertainty described with numbers as a one-dimensional probability distribution?\n\n\n\n\n\n\nEntryInformed\n\n\n\nA probability distribution quantifies uncertainty regarding a quantity that has a single true value (not a variable). Graphical representations of distributions are difficult to interpret for non-technical audiences. Therefore, for entry and informed levels, only communicate selected results extracted from the distribution as recommended below.\nState clearly what the distribution refers to (see Section 3.1 General guidance, point 2).\nProvide the central estimate (mean or median), the P5–P95 range (within which it is 90% certain that the true value lies) and/or the P25–P75 range (within which it is 50% certain the true value lies), expressed in such a way that the meaning of the ranges is clear. If it is critical to understanding the message, also give an idea about the form of the distribution behind the range (likelihood associated with particular values/outcomes). If the assessors have provided different quantiles (e.g. P1–P99, see example below), use these instead.\n\n\nExample: “Experts estimated that, on average in Europe in 2015, 16 out of 100 slaughtered dairy cows were pregnant. Their assessment is based on limited data, but the experts are 50% certain that the European average for 2015 is between 9 and 27, and 98% certain it is between 2 and 60” Based on EFSA 2017;15(5):4782\nNote that the example refers to a quantity that is uncertain but not variable: although the average will vary between countries and over time, ‘the European average for 2015’ has a single true value, which is uncertain. For guidance on communicating quantities that are both variable and uncertain, see Question 9 below.\n\n\nIn addition, or alternatively: if a regulatory (reference) value exists, or a value of particular interest for other reasons (e.g. more than zero occurrence of an adverse outcome), provide the probability of exceeding that value. (See Question 9 for an example of this.)\n\n\n\nAs for the entry level with the following differences:\n\nBefore giving the results, describe examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account in their assessment.\n\nOptionally, mention specific methods that were used to quantify the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\n\nConsider providing a visual representation of the uncertainty if this is expected to be useful for understanding, e.g. to aid understanding of the distribution of probability around the central estimate. Use a box plot for this at the informed level. (Do not use graphical representations of full distributions, such as PDF or CDF, in communications for non-technical audiences because these are easily misunderstood.) When using a box plot, explain clearly that it represents the uncertainty, as they are commonly used to represent variability and people may misinterpret them in that way. When including a graphic, provide this in addition to the entry level representation (see above) and not instead of it.\n\nLabel the graphs (including axes, legends, and units) appropriately so that, as far as possible, people will understand them on their own. Accompany every visual with sufficient textual explanation that the informed-level audience will understand it.\n\n\nExample: “Ten experts from different EU countries each surveyed a sample of slaughterhouses in their country to gather information on the prevalence of animals being pregnant at slaughter in 2015. Six of those experts used the survey results and other available evidence to estimate the average prevalence in Europe for different species in 2015, taking account of the uncertainties involved. The Panel’s conclusions are based on the results. Experts estimated… [continue as in entry-level communication]” Based on EFSA 2017;15(5):4782\n\nWhen communicating using box or box-and-whisker plots:\n\nAlways accompany box plots with explanations of each element they contain (including central estimate, box and whiskers).\nIf there is a value or quantity that is of particular interest to risk managers or the public (e.g. a regulatory/reference value), communicate the probability of the true value being above or below this (depending which is of interest) alongside the box plot, following the specific guidance in Question 6.\n\n\n\n\n\n\nFigure 1: Example of a box-and-whiskers plot\n\n\n\n\n\nExample: “Figure 1 is a box plot summarising the combined judgement of six experts about the average number of pregnant dairy cows per 100 cows slaughtered in the EU in 2015. The horizontal line inside the box is the median estimate: the true average value is considered equally likely to be above or below this estimate. The box and whiskers represent the experts’ collective uncertainty about the EU average in 2015 (not variation between samples of animals). There is 50% certainty that the true average is in the box and 98% certainty it is between the whiskers. There is still a 2% chance that the true average is outside the whiskers” Based on EFSA 2017;15(5):4782\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9. Is the uncertainty described with numbers as a two-dimensional probability distribution?\n\n\n\n\n\n\nEntryInformed\n\n\n\nThe term “2D probability distribution” refers here to a distribution that quantifies both variability and uncertainty for the same quantity (e.g. uncertainty about the variability of exposure in a population). Such distributions are difficult to interpret even for technical audiences. Therefore, for entry and informed levels only communicate selected results extracted from the 2D distribution.\nCheck with the assessors which results from the 2D assessment are likely to interest the audience, normally one or both of the following:\n\nThe median estimate for a specified quantile of variability, e.g. the 95th percentile, together with a P2.5 and P97.5 (or other quantiles) to represent its uncertainty.\nThe median estimate for the frequency of exceeding (or being below) a specified value of the quantity, e.g. the proportion of a population that exceeds a regulatory reference value, together with a P2.5 and P97.5 (or other quantiles) to represent its uncertainty.\n\nObtain the selected results from the assessors. The range of values between the two quantiles has a specified probability (95% for P2.5 and P97.5 in the example below), so communicate this following the approach in Question 6.\n\n\nExample: “The Panel estimated that,if intake of carbendazim from apples and apple products including apple juice was measured for 10,000 UK toddlers on single day schosen at random, including instances where no apples or apple products were consumed, 10 of those intakes would exceed the safe level.The experts are 95% certain that the number of intakes exceeding the safe level would be between 1 and 40 per 10,000” Based on EFSA 2007;5(8):538\nThe scientific opinion expressed the results in ‘toddlerdays’, which is not readily understandable to non-technical audiences, so it has been reformulated. Other expressions, e.g. number of toddlers per year, might be preferable but are not derivable without additional assumptions. In real communications, the meaning of ‘safe level’ also needs explaining.)\n\n\n\n\nBefore giving the results, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account in their assessment.\nOptionally, mention specific methods that were used in quantifying the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\nProvide a visual representation of the uncertainty if possible, especially if the uncertainty information is part of the key messages. Use a box plot for this showing median, P5, P25, P75 and P95 for the specific result(s) selected from the 2D distribution. Ask the scientific officer to provide the box plot for the selected result, and communicate it as indicated in Quesion 8.\nDo not use graphical representations of full 2D distributions in entry or informed level communications as many people misunderstand them.\n\n\nExample: “The Panel developed a mathematical model of the exposure of toddlers to carbendazim in apples and apple products. They used UK data on the occurrence of carbendazim in apples and on consumption of apples and apple products by toddlers. The model computed different eating patterns among toddlers by simulating apple consumption and carbendazim intake for 10,000 ‘toddler-days’ (10,000 random samples from a survey of daily apple consumption by UK toddlers, including records where no apples or apple products were consumed). The simulation also calculated five types of uncertainty affecting the model, e.g. limited measurements of occurrence and consumption, limitations in the precision of the occurrence data. Using this model, the Panel estimated that the number of ‘toddler-days’ in which more than the safe level for carbendazim is ingested from apples and apple products is 10 per 10,000 toddler-days. However, this takes account of only the five sources of uncertainty that were quantified: other uncertainties were taken into account separately by expert judgement” Based on EFSA 2007;5(8):538\nNote: after the above text provide information on the experts’ judgement about other uncertainties which the model did not quantify, then go back to to this Questionnaire to identify the type of expression used and locate the corresponding specific guidance for communication."
  },
  {
    "objectID": "comguidance/assessor.html",
    "href": "comguidance/assessor.html",
    "title": "for Assessors",
    "section": "",
    "text": "Template for identifying messages with associated uncertainty expressions, and specific guidance for their communication:\n\n\n\n\n\n\n\n1. Did the scientific assessment for this message identify any non-standard uncertainties?\n\n\n\n\n\n\nYesNo\n\n\nIf an unqualified conclusion is required, follow the guidance for unqualified conclusions (see Question 5 here). If an unqualified conclusion is not required, state the result of the standardised procedure in the form expressed by the assessors. Also communicate the uncertainty expressions for this message, consulting the respective guidance boxes.\n\n\nReport the conclusion as expressed by assessors and state that a standardised assessment procedure was followed that takes account of standard uncertainties, and no non-standard uncertainties were identified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Does the scientific output list or describe the sources or causes of any uncertainties affecting this message?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nState that uncertainties exist, using the wording in the scientific output.\nInclude in the message a brief description of the sources of uncertainty that have the biggest impact on the respective key messages. (If necessary, consult the assessors to identify these.)\n\n\nExample: “The experts identified limitations in the data on exposure and toxic effects of ZEN and its modified forms, for example (…)” Based on EFSA 2017;15(7):4851\n\n\n\n\nWhen documenting sources of uncertainty in the assessment report, assessors should include brief text descriptions suitable for subsequent use in communications to informed audiences without using specialist technical terms. - Assessors should try to identify which sources of uncertainty have most influence on their conclusions, either by qualitative assessment or by influence or sensitivity analysis UA.\nWhere there is conflicting evidence on an issue, this is a source of uncertainty which must be documented and taken into account in uncertainty analysis, and may be assessed using a weight of evidence approach WE.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Is the direction and/or degree of uncertainty expressed verbally (i.e. with terms like ‘low’, ‘medium’, ‘high’) or with symbols (e.g. ‘+’ or ‘-’)?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nAvoid altering the wordings used by assessors to describe the direction and/or degree of uncertainty, or factors contributing to uncertainty (Box 2). Always check the rewording with the assessors if you do.\nState clearly what outcomes and conditions this expression of uncertainty refers to.\nMake clear that any uncertainty referred to in the communication has been taken into account in the assessment conclusion.\nBefore communicating the uncertainty expression, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment.\n\nOptionally, mention specific methods that were used in evaluating the uncertainty.\n\nOptionally, mention factors contributing to the overall uncertainty, including the relative importance of individual sources of uncertainty and things like the relevance and reliability of evidence (e.g. in weight of evidence assessments, see WE).\n\nClearly distinguish individual sources of uncertainty from overall uncertainty about the assessment conclusions.\n\n\nExample: “The Panel noted that a high proportion of measurements of ZEN and its modified forms in feed were below the limit of detection, leading to very high uncertainty when estimating exposure” Based on EFSA 2017;15(7):4851\n\n\n\n\nIf using “+” and “−” or other symbols to indicate the direction and magnitude of uncertainty, accompany these with quantitative definitions of their meaning, as discussed in SO, Annex B5.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. Does this message report an inconclusive assessment outcome?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nDescribe the main sources of uncertainty in more detail, but concisely, following the guidance in Question 2 above.\nInconclusive assessments are especially likely to include options or requirements for obtaining further data. Communicate these as instructed in UC Section 3.1.6.\n\n\nExample: “EFSA’s experts could not reach a conclusion on the risk for cattle, ducks, goats, horses, rabbits, mink and cats due to limitations in available data on exposure and toxic effects of ZEN and itsmodified forms, for example (…)” Based on EFSA 2017;15(7):4851\n\n\n\n\nWhen explaining why the assessment is inconclusive, include a description of the key sources of uncertainty that are responsible for this.\n\nIf the assessment is not totally uncertain, try to express what the science can say and quantify the uncertainty unless the risk manager/legislation requires that only unqualified conclusions be given.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. Do risk managers expect or does legislation require that this message should be expressed as an unqualified conclusion, without any expression of uncertainty?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nReport the unqualified conclusion for this message using the same wording as the assessors.\nOptionally, describe briefly how the assessment was made (i.e. what evidence and methods were used to arrive at the conclusions).\nBriefly describe some examples of uncertainties affecting the assessment for this message, as identified in your completed template, consulting Box 4 for guidance on how to communicate this.\nIf the assessment contains any verbal or numerical expression of the impact of the uncertainties as identified in your template, follow the respective guidance in Boxes 6–9 below.\nSay that the assessors took the uncertainties into account when reaching their conclusion(s) for this message.\n\n\nExample: “Following the standard assessment procedure (or ‘Using the evaluation system agreed for contaminants in feed’), experts estimated that high exposure to feed containing ZEN is below the reference value for a health risk for sheep, dog, pig and fish, and well below the reference value for chicken and turkeys. They therefore concluded that the exposure to feed containing ZEN ‘in farm situations’ is a low health risk for sheep,dog,pig and fish,and an extremely low health risk for poultry. In reaching this conclusion, the experts took account of limitations in the data on exposure and toxic effects of ZEN and its modified forms, for example (…)” Based on EFSA 2017;15(7):4851\n\n\n\n\nProvide the information needed for the FAQ required at the entry level communications (see above).\nSpecify what level of certainty is associated with each unqualified conclusion. Risk managers can explain why that level of certainty is appropriate for decisionmaking, if considered necessary. Make this information available to interested parties in suitable ways, e.g. in an FAQ and/or in documentation or guidance on the assessment methodology.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6. Is the uncertainty described with numbers as a precise probability?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nState clearly what the probability refers to, including whether it refers to a numerical estimate or a qualitative conclusion. When the probability refers to a numerical estimate, also state the range of the quantity that the probability refers to (see example below).\nBefore giving the probability, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account when assessing their level of certainty.\nOptionally, mention specific methods that were used in quantifying the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\n\n\nExample: “The Panel performed its assessment using a mathematical model of the entry of nematodes into the EU and their establishment and spread in greenhouse tulips. Uncertainty on the factors represented in the model was quantified by expert judgement, taking into account the limitations of the available data. The Panel estimates…[continue as for entry level]” Based on EFSA 2017;15(8):4879\n\n\n\n\nNo specific guidance for assessors other than the general guidance for assessors in UC Section 3.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7. Is the uncertainty described with numbers as an approximate probability?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nState clearly what the probability refers to, including whether it refers to a numerical estimate or a qualitative conclusion. When the probability refers to a numerical estimate, also state the range of the quantity that the probability refers to.\nAn approximate probability may comprise a range of probabilities chosen by the assessors from the approximate probability scale Table 1, or a different range of probabilities specified by the assessors.\nAlways communicate the quantitative range of probabilities because this expresses the assessors’ conclusion without ambiguity. If a verbal expression is also used, present the quantitative probability first (e.g. ‘66–90% certain (likely)’) because it has been shown that this order leads to more consistent understanding than if the verbal expression is presented first (see UC Section 3.1)\nTo avoid inconsistency and misunderstanding, do not use the verbal terms in Table 1 to refer to any probabilities or ranges of probabilities other than those shown in this table.\nBefore giving the probability, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account when assessing their level of certainty.\nOptionally, mention specific methods that were used in quantifying the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\n\n\nExample: “Experts began work on the Scientific Opinion after the 2015 EU summary report on foodborne zoonotic diseases identified an increasing trend of listeriosis over the period 2009–2013. The Panel performed a statistical analysis, which confirmed the increasing trend, and developed a mathematical model of the factors influencing the incidence of infections. Considering the modelling results and the degree of support from indicator data, the experts…’ [continue as for entry level]” Based on EFSA 2018;16(1):5134\n\n\n\n\nUse different probabilities or ranges from those shown in Table 1 if they better express your judgement UA. In such cases, avoid accompanying it with any verbal probability expression because a harmonised interpretation exists only for the terms in Table 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8. Is the uncertainty described with numbers as a one-dimensional probability distribution?\n\n\n\n\n\n\nInformedTechnical\n\n\nAs for the entry level with the following differences:\n\nBefore giving the results, describe examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account in their assessment.\n\nOptionally, mention specific methods that were used to quantify the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\n\nConsider providing a visual representation of the uncertainty if this is expected to be useful for understanding, e.g. to aid understanding of the distribution of probability around the central estimate. Use a box plot for this at the informed level. (Do not use graphical representations of full distributions, such as PDF or CDF, in communications for non-technical audiences because these are easily misunderstood.) When using a box plot, explain clearly that it represents the uncertainty, as they are commonly used to represent variability and people may misinterpret them in that way. When including a graphic, provide this in addition to the entry level representation (see above) and not instead of it.\n\nLabel the graphs (including axes, legends, and units) appropriately so that, as far as possible, people will understand them on their own. Accompany every visual with sufficient textual explanation that the informed-level audience will understand it.\n\n\nExample: “Ten experts from different EU countries each surveyed a sample of slaughterhouses in their country to gather information on the prevalence of animals being pregnant at slaughter in 2015. Six of those experts used the survey results and other available evidence to estimate the average prevalence in Europe for different species in 2015, taking account of the uncertainties involved. The Panel’s conclusions are based on the results. Experts estimated… [continue as in entry-level communication]” Based on EFSA 2017;15(5):4782\n\nWhen communicating using box or box-and-whisker plots:\n\nAlways accompany box plots with explanations of each element they contain (including central estimate, box and whiskers).\nIf there is a value or quantity that is of particular interest to risk managers or the public (e.g. a regulatory/reference value), communicate the probability of the true value being above or below this (depending which is of interest) alongside the box plot, following the specific guidance in Question 6.\n\n\n\n\n\n\nFigure 1: Example of a box-and-whiskers plot\n\n\n\n\n\nExample: “Figure 1 is a box plot summarising the combined judgement of six experts about the average number of pregnant dairy cows per 100 cows slaughtered in the EU in 2015. The horizontal line inside the box is the median estimate: the true average value is considered equally likely to be above or below this estimate. The box and whiskers represent the experts’ collective uncertainty about the EU average in 2015 (not variation between samples of animals). There is 50% certainty that the true average is in the box and 98% certainty it is between the whiskers. There is still a 2% chance that the true average is outside the whiskers” Based on EFSA 2017;15(5):4782\n\n\n\n\nInclude a table of the distribution and a box plot.\nProvide P5, P25, P50, P75 and P95 ranges. In addition, if there are values of specific interest to the public/risk managers (e.g. a reference dose/value), then provide the probability for the true value being above or below this (depending which is of interest, e.g. a health concern).\nState which sources of uncertainty are considered in the distribution and provide a qualitative or quantitative description of uncertainties not considered in the distribution (e.g. uncertainty about the quality and representativeness of entry data, assumptions in modelling exposure or assumptions about the distribution of different parameters of a model).\nState clearly how each distribution was obtained, and in particular whether it was derived by statistical analysis, mechanistic modelling, expert judgement or a combination of these.\n\nOptional:\n\nProvide a PDF graph of the distribution if communicating only the quantiles would fail to indicate something important about the distribution, e.g. bimodal, skewed.\nAccompany the PDF with a CDF graph of the distribution if this is useful for technical readers of the assessment (e.g. to enable them to read off approximate estimates for quantiles other than those reported explicitly). Using the same horizontal scale, plot the CDF above the PDF and clearly mark the location of the central estimate (and optionally the P5, P25, P75 and P95) on both curves.\nAccompany a PDF or CDF with an explanatory text expressed in the simplest terms possible. If there is a value or quantity that is of particular interest to risk managers or the public (e.g. a regulatory/reference value), explicitly mark it on both curves. Explain clearly that the distribution represents uncertainty about the quantity of interest, for which there is a single true value, as distributions are more commonly used to represent variability and people may misinterpret them in that way.\n\n\n\n\n\n\nFigure 2: Example of a PDF graph\n\n\n\n\n\nExample: “The red line in Figure 2 shows a probability distribution quantifying uncertainty about how many dairy cows out of a hundred on average are pregnant when slaughtered in the EU in 2015, i.e. the prevalence of being pregnant when slaughtered. The height of the red curve shows the relative likelihood of the prevalence values in each part of the horizontal axis. The central (median) estimate is 16 out of 100, with 50% certainty that the European average for 2015 is between 9 and 27, and 98% certainty it is between 2 and 60 (as shown on the graph)”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9. Is the uncertainty described with numbers as a two-dimensional probability distribution?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nBefore giving the results, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account in their assessment.\nOptionally, mention specific methods that were used in quantifying the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\nProvide a visual representation of the uncertainty if possible, especially if the uncertainty information is part of the key messages. Use a box plot for this showing median, P5, P25, P75 and P95 for the specific result(s) selected from the 2D distribution. Ask the scientific officer to provide the box plot for the selected result, and communicate it as indicated in Quesion 8.\nDo not use graphical representations of full 2D distributions in entry or informed level communications as many people misunderstand them.\n\n\nExample: “The Panel developed a mathematical model of the exposure of toddlers to carbendazim in apples and apple products. They used UK data on the occurrence of carbendazim in apples and on consumption of apples and apple products by toddlers. The model computed different eating patterns among toddlers by simulating apple consumption and carbendazim intake for 10,000 ‘toddler-days’ (10,000 random samples from a survey of daily apple consumption by UK toddlers, including records where no apples or apple products were consumed). The simulation also calculated five types of uncertainty affecting the model, e.g. limited measurements of occurrence and consumption, limitations in the precision of the occurrence data. Using this model, the Panel estimated that the number of ‘toddler-days’ in which more than the safe level for carbendazim is ingested from apples and apple products is 10 per 10,000 toddler-days. However, this takes account of only the five sources of uncertainty that were quantified: other uncertainties were taken into account separately by expert judgement” Based on EFSA 2007;5(8):538\nNote: after the above text provide information on the experts’ judgement about other uncertainties which the model did not quantify, then go back to to this Questionnaire to identify the type of expression used and locate the corresponding specific guidance for communication.\n\n\n\n\nProvide a box plot and/or relevant table containing quantiles for selected results from the distribution. • Provide a description of the relative magnitude of variability and uncertainty.\nConsider including a graphical representation of the 2D distribution in the technical-level communication (EFSA scientific output). When this is carried out, proceed as follows:\n\nUse a 2D CDF rather than a 2D CCDF unless there are strong reasons for using the latter, as there is some evidence that CCDFs are less well understood.\nEnsure the graph is well formatted, with clear labelling of axes and values, and markers to show the specific results that were selected for use in the entry-level communication (see above). If helpful for communication, also request a box plot for the selected result and display it alongside the 2D graph.\nProvide sufficient textual explanation for the 2D graph so the intended audience will understand it.\nExplain that the 2D graph represents both variability and uncertainty for the quantity of interest. Consider whether it is helpful to include accompanying text on the relative magnitudes of variability and uncertainty and, if so, request this from the scientific officer.\n\n\n\n\n\n\n\nFigure 3: Example of graphical representation of 2D distribution\n\n\n\n\n\nExample: “A graph showing the results of the model for the 2D probability assessment is presented in Figure 3. Thehorizontalaxisis the amount of carbendazim from apples and/or apple products ingested per toddler-day : the vertical solid red line shows the safe level for this. The vertical axis is the number of ‘toddler-days’ per 10,000 on which carbendazim intake exceeded any given level on the horizontal axis. The median estimate for this is shown by the solid curve: each point on the curve shows the number of toddlerdays (on the vertical axis) on which ingestion of carbendazim exceeded the level shown below that point on the horizontal axis. The dashed curves show the upper and lower 95% confidence interval for the estimates: i.e. there is 95% certainty that any selected estimate lies between the dashed curves. However, this takes into account only the five sources of uncertainty that were quantified: other uncertainties were considered separately by expert judgement. From the results shown in the graph, the Panel estimated that the number of toddler-daysonwhichmorethanthesafelevelforcarbendazimisingested from apples and apple products is 10 per 10,000, and is 95% certain the number is between 1 and 40 per 10,000. These results are indicated by the horizontal dotted lines shown in the graph’.” Based on EFSA 2007;5(8):538\nNote: after the second paragraph above, provide information on the experts’ judgement about the other uncertainties, go back to this Questionnaire to identify the type of expression used and locate the corresponding specific guidance for communication. Note: EFSA outputs rarely use 2D graphs. The graphical example in Figure 3 is unusual because the vertical axis is inverted. Also, the example contains a second distribution (shown in grey) that is not referred to in the text above. Take care to use a format that facilitates understanding of the key results."
  },
  {
    "objectID": "pmguidance/methods_quantitative.html",
    "href": "pmguidance/methods_quantitative.html",
    "title": "Quantitative methods",
    "section": "",
    "text": ":::"
  },
  {
    "objectID": "pmguidance/methods_sources_methodology.html",
    "href": "pmguidance/methods_sources_methodology.html",
    "title": "Questions to identify sources of uncertainty affecting assessment methodology",
    "section": "",
    "text": "1. Ambiguity\n\n\n\n\n\n\nIf the assessment combines inputs using mathematical or statistical model(s) that were developed by others, are all aspects of them adequately described, or are multiple interpretations possible?\n\n\n\n\n\n\n\n\n\n\n2. Excluded factors\n\n\n\n\n\n\nAre any potentially relevant factors or processes excluded? (e.g. excluded modifying factors, omitted sources of additional exposure or risk).\n\n\n\n\n\n\n\n\n\n\n3. Distribution choice\n\n\n\n\n\n\nAre distributions used to represent variable quantities? If so, how closely does the chosen form of distribution (normal, lognormal, etc.) represent the real pattern of variation? What alternative distributions could be considered?\n\n\n\n\n\n\n\n\n\n\n4. Use of fixed values\n\n\n\n\n\n\nDoes the assessment include fixed values representing quantities that are variable or uncertain, e.g. default values or conservative assumptions? If so, are the chosen values appropriate for the needs of the assessment, such that when considered together they provide an appropriate and known degree of conservatism in the overall assessment?\n\n\n\n\n\n\n\n\n\n\n5. Relationship between parts of the assessment\n\n\n\n\n\n\nIf the assessment model or reasoning represents a real process, how well does it represent it? If it is a reasoned argument, how strong is the reasoning? Are there alternative structures that could be considered? Are there dependencies between variables affecting the question or quantity of interest? How different might they be from what is assumed in the assessment?\n\n\n\n\n\n\n\n\n\n\n6. Evidence for the structure of the assessment\n\n\n\n\n\n\nWhat is the nature, quantity, relevance, reliability and quality of data or evidence available to support the structure of the model or reasoning used in the assessment? Where the assessment or uncertainty analysis is divided into parts, is the division into parts and the way they are subsequently combined appropriate?\n\n\n\n\n\n\n\n\n\n\n7. Uncertainties relating to the process for dealing with evidence from the literature\n\n\n\n\n\n\nWas a structured approach used to identify relevant literature? How appropriate were the search criteria and the list of sources examined? Was a structured approach used to appraise evidence? How appropriate were the criteria used for this? How consistently were they applied? Were studies filtered or prioritised for detailed appraisal? Was any potentially relevant evidence set aside or excluded? If so, its potential contribution should be considered as part of the characterisation of overall uncertainty.\n\n\n\n\n\n\n\n\n\n\n8. Expert judgement\n\n\n\n\n\n\nIdentify where expert judgement was used: in obtaining and interpreting estimates based on statistical analysis of data, in obtaining estimates by expert elicitation, in choices about assessment methods, models and reasoning? How many experts participated, how relevant and extensive was their expertise and experience for making them, and to what extent did they agree? Was a structured elicitation methodology used and, if so, how formal and rigorous was the procedure?\n\n\n\n\n\n\n\n\n\n\n9. Calibration or validation with independent data\n\n\n\n\n\n\nHas the assessment, or any component of it, been calibrated or validated by comparison with independent information? If so, consider the following: What uncertainties affect the independent information? Assess this by considering all the questions listed above for assessing the uncertainty of inputs How closely does the independent information agree with the assessment output or component to which it pertains, taking account of the uncertainty of each? What are the implications of this for your uncertainty about the assessment?\n\n\n\n\n\n\n\n\n\n\n10. Dependency between sources of uncertainty\n\n\n\n\n\n\nAre there dependencies between any of the sources of uncertainty affecting the assessment and/or its inputs, or regarding factors that are excluded? If you learned more about any of them, would it alter your uncertainty about one or more of the others?\n\n\n\n\n\n\n\n\n\n\n11. Other uncertainties\n\n\n\n\n\n\nAre there any uncertainties about assessment methods or structure, due to lack of data or knowledge gaps, which are not covered by other categories above?"
  },
  {
    "objectID": "pmguidance/methods_sources_inputs.html",
    "href": "pmguidance/methods_sources_inputs.html",
    "title": "Questions to identify sources of uncertainty affecting inputs",
    "section": "",
    "text": "1. Ambiguity\n\n\n\n\n\n\nAre all necessary aspects of any data, evidence, assumptions or scenarios used in the assessment (including the quantities measured, the subjects or objects on which measurements are made, and the time and location of measurements) adequately described, or are multiple interpretations possible?\n\n\n\n\n\n\n\n\n\n\n2. Accuracy and precision of the measures\n\n\n\n\n\n\nHow accurate and precise are methods/tools used to measure data (e.g. analytical methods, questionnaire). How adequate are any data quality assurance procedures and data validation that were followed?\n\n\n\n\n\n\n\n\n\n\n3. Sampling uncertainty\n\n\n\n\n\n\nIs the input based on measurements or observations on a sample from a larger population? If yes: How was the sample collected? Was stratification needed or applied? Was the sampling biased in any way, e.g. by intentional or unintentional targeting of sampling? How large was the sample? How does this affect the uncertainty of the estimates used in the assessment?\n\n\n\n\n\n\n\n\n\n\n4. Missing data within studies\n\n\n\n\n\n\nWhat is the frequency of missing data within the studies that are available? Is the mechanism causing the missing data random, or may it have introduced bias or imbalance among experimental groups (if any)? Was imputation of missing data performed, and did it use sound methodologies?\n\n\n\n\n\n\n\n\n\n\n5. Missing studies\n\n\n\n\n\n\nIs all the evidence needed to answer the assessment question available? Are the published studies reflecting all the available evidence? Where required studies are specified in guidance or legislation, are they all provided?\n\n\n\n\n\n\n\n\n\n\n6. Assumptions about inputs\n\n\n\n\n\n\nIs the input partly or wholly based on assumptions, such standard scenarios or default values? If so, what is the nature, quantity, relevance, reliability and quality of data or evidence available to support those assumptions?\n\n\n\n\n\n\n\n\n\n\n7. Statistical estimates\n\n\n\n\n\n\nDoes the input include a statistical measure of uncertainty (e.g. confidence interval)? If so, what uncertainties does this quantify, and what other uncertainties need to be considered? Is the statistical analysis used to produce the evidence appropriate and adequate? Are the implicit and explicit assumptions done in the statistical analysis expected to influence the results.\n\n\n\n\n\n\n\n\n\n\n8. Extrapolation uncertainty (e.g. limitations in external validity)\n\n\n\n\n\n\nAre any data, evidence, assumptions and scenarios used in the assessment (including the quantities they address, and the subjects or objects, time and location to which that quantity refers) directly relevant to what is needed for the assessment, or is some extrapolation or read across required? If the input is based on measurements or observations on a sample from a population, how closely relevant is the sampled population to the population or subpopulation of interest for the assessment? Is some extrapolation implied?\n\n\n\n\n\n\n\n\n\n\n9. Other uncertainties\n\n\n\n\n\n\nIs the input affected by any other sources of uncertainty that you can identify, or other reasons why the input might differ from the real quantity or effect it represents?"
  },
  {
    "objectID": "pmguidance/pmanalysis_expressions.html",
    "href": "pmguidance/pmanalysis_expressions.html",
    "title": "Uncertainty expressions",
    "section": "",
    "text": "An expression of uncertainty requires two components:\n\nexpression of the range of possible true answers to a question of interest, or a range of possible true values for a quantity of interest, and\nexpression of the probabilities of the different answers or values.\n\n\n\n\n\n\n\nQuantitative approaches express one or both of these components on a numerical scale.\n\n\n\n\n\n\nGeneralExamples\n\n\n\nA complete quantitative expression of uncertainty would specify all the answers or values that are considered possible and probabilities for them all.\nPartial quantitative expression provides only partial information on the probabilities and in some cases partial information on the possibilities (specifying a selection of possible answers or values).\nPartial quantitative expression requires less information or judgements but may be sufficient for decision-making in some assessments, whereas other cases may require fuller quantitative expression.\n\n\n\nDescriptive expression: Uncertainty described in narrative text or characterised using verbal terms without any quantitative definition.\nOrdinal scale: Uncertainty described by ordered categories, where the magnitude of the difference between categories is not quantified.\n\n\n\n\n\n\n\n\n\n\n\n\nQualitative approaches express them using words, categories or labels.\n\n\n\n\n\n\nGeneralExamples\n\n\n\nThey may rank the magnitudes of different uncertainties, and are sometimes given numeric labels, but they do not quantify the magnitudes of the uncertainties nor their impact on an assessment conclusion.\n\n\n\nIndividual values: Uncertainty partially quantified by specifying some possible values, without specifying what other values are possible or setting upper or lower limits.\nBound: Uncertainty partially quantified by specifying either an upper limit or a lower limit on a quantitative scale, but not both.\nRange: Uncertainty partially quantified by specifying both a lower and upper limit on a quantitative scale, without expressing the probabilities of different values within the limits.\nProbability: Uncertainty about a binary outcome (including the answer to a yes/no question) fully quantified by specifying the probability or approximate probability of both possible outcomes.\nProbability bound: Uncertainty about a non-variable quantity partially quantified by specifying a bound or range with an accompanying probability or approximate probability.\nDistribution: Uncertainty about a non-variable quantity fully quantified by specifying the probability of all possible values on a quantitative scale."
  },
  {
    "objectID": "pmguidance/pmanalysis_expressions.html#the-principal-reasons-for-preferring-quantitative-expressions-of-uncertainty",
    "href": "pmguidance/pmanalysis_expressions.html#the-principal-reasons-for-preferring-quantitative-expressions-of-uncertainty",
    "title": "Uncertainty expressions",
    "section": "The principal reasons for preferring quantitative expressions of uncertainty",
    "text": "The principal reasons for preferring quantitative expressions of uncertainty\n\nQualitative expressions are ambiguous.\nDecision-making often depends on quantitative comparisons, for example, whether a risk exceeds some acceptable level, or whether benefits outweigh costs.\nIf assessors provide only a single answer or estimate and a qualitative expression of the uncertainty, decision-makers will have to make their own quantitative interpretation of how different the real answer or value might be. This judgement is better made by assessors, since they are better placed to understand the sources of uncertainty affecting the assessment and judge their effect on its conclusion.\nQualitative expressions often imply, or may be interpreted as implying, judgements about the implications of uncertainty for decision-making, which are outside the remit of EFSA.\nAssessors may assess uncertainty differently yet agree on a single qualitative expression, because they interpret it differently.\nExpressing uncertainties in terms of their quantitative impact on the assessment conclusion will reveal differences of opinion between experts working together on an assessment, enabling a more rigorous discussion and hence improving the quality of the final conclusion.\nIt has been demonstrated that people often perform poorly at judging combinations of probabilities. This implies they may perform poorly at judging how multiple uncertainties in an assessment combine. It may therefore be more reliable to divide the uncertainty analysis into parts and quantify uncertainty separately for those parts containing important sources of uncertainty, so that they can be combined by calculation.\nQuantifying uncertainty enables decision-makers to weigh the probabilities of different consequences against other relevant considerations."
  },
  {
    "objectID": "pmguidance/pmanalysis_expressions.html#common-concerns-and-objections-to-quantitative-expression-of-uncertainty",
    "href": "pmguidance/pmanalysis_expressions.html#common-concerns-and-objections-to-quantitative-expression-of-uncertainty",
    "title": "Uncertainty expressions",
    "section": "Common concerns and objections to quantitative expression of uncertainty",
    "text": "Common concerns and objections to quantitative expression of uncertainty\nCommon concerns and objections to quantitative expression of uncertainty and how they are addressed by the approach developed in this document and the accompanying Guidance.\n\nCommon concerns"
  },
  {
    "objectID": "pmguidance/pmanalysis_keyprinciples.html",
    "href": "pmguidance/pmanalysis_keyprinciples.html",
    "title": "Key principles",
    "section": "",
    "text": "Key principles when using standardised procedures\n\n\n\n\n\n\n\nWhat is a standardised assessment procedure?\n\n\n\nStandardised assessment procedures with accepted provision for uncertainty are common in many areas of EFSA’s work and are subject to periodic review.\nMost standardised procedures involve deterministic calculations using a combination of standard study data, default assessment factors and default values.\n\n\nUsing a standardised procedure can greatly simplify uncertainty analysis in routine assessments.\nThe documentation or guidance for a standardised procedure should specify\n\nthe question or quantity of interest,\nthe standardised elements of the procedure,\nthe type and quality of case-specific data to be provided and\nthe generic sources of uncertainty considered when calibrating the level of conservatism.\n\nIt is the responsibility of assessors to check the applicability of all these elements to each new assessment and check for any non-standard sources of uncertainty relevant to the question under assessment.\nAny deviations that would increase the uncertainties considered in the calibration or introduce additional sources of uncertainty, will mean that it cannot be assumed that the calibrated level of conservatism and certainty will be achieved for that assessment.\nTherefore, assessors should check for non-standard uncertainties in every assessment using a standardised procedure.\n\nIn assessments where none are identified, it is sufficient to record that a check was made and none were found. When non-standard uncertainties are present, a simple evaluation of their impact may be sufficient for decision-making, depending on how much scope was left for non-standard uncertainties when calibrating the standardised procedure.\nWhere the non-standard uncertainties are substantial or the standardised assessment procedure is not applicable, the assessors may need to carry out a case-specific assessment and uncertainty analysis.\n\n\n\nKey principles for development or review of a standardised procedure\n\nThe level of conservatism provided by each standardised procedure should be assessed by an appropriate uncertainty analysis.\n\nThis is to ensure they provide an appropriate degree of coverage for the sources of uncertainty that are generally associated with the class of assessments to which they apply.\n\nThe uncertainty analysis for the development or review of a standardised procedure\n\nis conducted as for a case-specific assessment to evaluate the probability of meeting the defined requirements, but where the procedure is adjusted if necessary to achieve an appropriate level of probability.\nThis requires defining the management objective for the procedure, and how often and/or to what extent that objective should be achieved in the future standardised assessments where the procedure will be used.\nThe uncertainty analysis should consider all relevant uncertainties, including uncertainties about how the standard study designs used to generate data, and any default factors, assumptions, scenarios and calculations used in the assessment, relate to conditions and processes in the real world.\nConsultation with decision-makers will be required to confirm that the level of conservatism is appropriate.\n\n\n\nKey principles for urgent assessments\n\n\n\n\n\n\n\nWhat is an urgent assessment?\n\n\n\nA scientific assessment requested to be completed within an unusually short period of time.\n\n\nIn some situations, e.g. emergencies, EFSA may be required to provide an urgent assessment in very limited time and the approach taken must be adapted accordingly.\n\nUncertainty is generally increased in such situations, and may be a major driver for decision-making.\nCharacterisation of uncertainty is therefore still necessary, despite the urgency of the assessment.\nThe approach to providing it must be scaled to fit within the time and resources available.\n\nEvery uncertainty analysis should express in quantitative terms the combined effect of as many as possible of the identified sources of uncertainty affecting each assessment.\n\nWhen time is severely limited, this may have to be done by a streamlined expert judgement procedure in which the contributions of all identified sources of uncertainty are evaluated and combined collectively, without dividing the uncertainty analysis into parts.\nThis initial assessment may need to be followed by more refined assessment and uncertainty analysis, including more detailed consideration of the most important sources of uncertainty, after the initial assessment has been delivered to decision-makers."
  },
  {
    "objectID": "pmguidance/methods_qualitative.html",
    "href": "pmguidance/methods_qualitative.html",
    "title": "Qualitative methods",
    "section": "",
    "text": ":::"
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html",
    "href": "pmguidance/pmanalysis_background.html",
    "title": "Background",
    "section": "",
    "text": "Uncertainty refers to all types of limitations in available knowledge that affect the range and probability of possible answers to an assessment question.\nAvailable knowledge refers here to the knowledge (evidence, data, etc.) available to assessors at the time the assessment is conducted and within the time and resources agreed for the assessment.\n\n\n\n\n\n\nTip\n\n\n\n\nSometimes ‘uncertainty’ is used to refer to a source of uncertainty, and sometimes to its impact on the conclusion of an assessment.\n\n\n\n\n\nUncertainty analysis is defined as the process of identifying and characterising uncertainty about questions of interest and/or quantities of interest in a scientific assessment.\nA question or quantity of interest may be the subject of the assessment as a whole, i.e. that which is required by the ToR for the assessment, or it may be the subject of a subsidiary part of the assessment which contributes to addressing the ToR (e.g. exposure and hazard assessment are subsidiary parts of risk assessment)."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#questions-for-assessment-by-efsa",
    "href": "pmguidance/pmanalysis_background.html#questions-for-assessment-by-efsa",
    "title": "Background",
    "section": "Questions for assessment by EFSA",
    "text": "Questions for assessment by EFSA\n\nQuestions for assessment by EFSA may be posed by the European Commission, the European Parliament, and EU Member State or by EFSA itself.\nMany questions to EFSA request assessment of consequences or current policy, conditions, practice or of consequences in alternative scenarios, e.g. under different risk management options.\nIt is important that the scenarios and consequences of interest are well-defined. ## Basic principles  Basic principles for addressing uncertainty in risk analysis are stated in the Codex Working Principles for Risk Analysis:\n\n‘Constraints, uncertainties and assumptions having an impact on the risk assessment should be explicitly considered at each step in the risk assessment and documented in a transparent manner’\n‘Responsibility for resolving the impact of uncertainty on the risk management decision lies with the risk manager, not the risk assessors’\n\nThese principles apply equally to the treatment of uncertainty in all areas of science and decision-making."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#responsibilities",
    "href": "pmguidance/pmanalysis_background.html#responsibilities",
    "title": "Background",
    "section": "Responsibilities",
    "text": "Responsibilities\nIn general,\n\nassessors are responsible for characterising uncertainty and\ndecision-makers are responsible for resolving the impact of uncertainty on decisions. Resolving the impact on decisions means deciding whether and in what way decision-making should be altered to take account of the uncertainty."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#rational",
    "href": "pmguidance/pmanalysis_background.html#rational",
    "title": "Background",
    "section": "Rational",
    "text": "Rational\nThe rational for this division of roles is:\n\nassessing scientific uncertainty requires scientific expertise, while\nresolving the impact of uncertainty on decision-making involves weighing the scientific assessment against other considerations, such as economics, law and societal values, which require different expertise and are also subject to uncertainty."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#interaction",
    "href": "pmguidance/pmanalysis_background.html#interaction",
    "title": "Background",
    "section": "Interaction",
    "text": "Interaction\nAlthough risk assessment and risk management are conceptually distinct activities,\n\ninteraction between assessors and risk managers with regard to the specification of the question for assessment and expression of uncertainty in conclusions is useful."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#uncertainty-is-always-present",
    "href": "pmguidance/pmanalysis_background.html#uncertainty-is-always-present",
    "title": "Background",
    "section": "Uncertainty is always present",
    "text": "Uncertainty is always present\n\nUncertainty refers to limitations in knowledge, which are always present to some degree.\nDecision-makers need to know the range of possible answers, so they can consider whether any of them would imply risk of undesirable management consequences (e.g. adverse effects)."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#uncertainty-analysis-is-always-required",
    "href": "pmguidance/pmanalysis_background.html#uncertainty-analysis-is-always-required",
    "title": "Background",
    "section": "Uncertainty analysis is always required",
    "text": "Uncertainty analysis is always required\n\nWhy?\n\nAll EFSA scientific assessments require at least a basic analysis of uncertainty.\n\nQuestions are posed to EFSA because the requestor does not know or is uncertain of the answer and that the amount of uncertainty affects decisions or actions they need to take.\nThe requestor seeks scientific advice from EFSA because they anticipate that this may reduce the uncertainty, or at least provide a more expert assessment of it.\nIf the uncertainty of the answer did not matter, then it would not be rational or economically justified for the requestor to pose the question to EFSA – the requestor would simply use their own judgement, or even a random guess.\nSo the fact that the question was asked implies that the amount of uncertainty matters for decision-making, and it follows that information about uncertainty is a necessary part of EFSA’s response.\nThis logic applies regardless of the nature or subject of the question, therefore providing information on uncertainty is relevant in all cases.\n\n\n\nImplication\nIt follows that uncertainty analysis is needed in all EFSA scientific assessments, though the form and extent of that analysis and the form in which the conclusions are expressed should be adapted to the needs of each case, in consultation with decision-makers."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#acceptable-level-of-uncertainty",
    "href": "pmguidance/pmanalysis_background.html#acceptable-level-of-uncertainty",
    "title": "Background",
    "section": "Acceptable level of uncertainty",
    "text": "Acceptable level of uncertainty\n\nComplete certainty is never possible.\nDeciding how much certainty is required or, equivalently, what level of uncertainty would warrant precautionary action, is the responsibility of decision-makers, not assessors.\nIt may be helpful if the decision-makers can specify in advance how much uncertainty is acceptable for a particular question because it has implications for what outputs should be produced from uncertainty analysis.\nOften, however, the decision-makers may not be able to specify in advance the level of certainty that is sought or the level of uncertainty that is acceptable, e.g. because this may vary from case to case depending on the costs and benefits involved. Another option is for assessors to provide results for multiple levels of certainty, so that decision-makers can consider at a later stage what level of uncertainty to accept."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#practical-certainty",
    "href": "pmguidance/pmanalysis_background.html#practical-certainty",
    "title": "Background",
    "section": "“Practical certainty”",
    "text": "“Practical certainty”\n\nAlternatively, partial information on uncertainty may be sufficient for the decision-makers provided it meets or exceeds their required level of certainty.\n\nFor some types of assessment, e.g. for regulated products, decision-makers need EFSA to provide an unqualified positive or negative conclusion to comply with the requirements of legislation, or of procedures established to implement legislation.\nIn general, the underlying assessment will be subject to at least some uncertainty, as is all scientific assessment. In such cases, therefore, the positive or negative conclusion refers to whether the level of certainty is sufficient for the purpose of decision-making, i.e. whether the assessment provides ‘practical certainty’"
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#main-sources-of-uncertainty",
    "href": "pmguidance/pmanalysis_background.html#main-sources-of-uncertainty",
    "title": "Background",
    "section": "Main sources of uncertainty",
    "text": "Main sources of uncertainty\n\nInformation on the magnitude of uncertainty and the main sources of uncertainty is also important to inform decisions about whether it would be worthwhile to invest in obtaining further data or conducting more analysis, with the aim of reducing uncertainty."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#fit-for-purpose",
    "href": "pmguidance/pmanalysis_background.html#fit-for-purpose",
    "title": "Background",
    "section": "Fit-for-purpose",
    "text": "Fit-for-purpose\nTo be fit for purpose, EFSA’s guidance on uncertainty analysis includes options for different levels of resource and different timescales, and methods that can be implemented at different levels of detail/refinement, to fit different timescales and levels of resource."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#refine-or-reduce",
    "href": "pmguidance/pmanalysis_background.html#refine-or-reduce",
    "title": "Background",
    "section": "Refine or reduce",
    "text": "Refine or reduce\nDecisions on how far to refine the assessment and whether to obtain additional data may be taken by assessors when they fall within the time and resources agreed for the assessment."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#suffient-for-decision-making",
    "href": "pmguidance/pmanalysis_background.html#suffient-for-decision-making",
    "title": "Background",
    "section": "Suffient for decision-making",
    "text": "Suffient for decision-making\nUltimately, it is for decision-makers to decide when the characterisation of uncertainty is sufficient for decision-making and when further refinement is needed, taking into account the time and costs involved."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#ranges-and-probabilities-when-question-well-defined",
    "href": "pmguidance/pmanalysis_background.html#ranges-and-probabilities-when-question-well-defined",
    "title": "Background",
    "section": "Ranges and probabilities when question well-defined",
    "text": "Ranges and probabilities when question well-defined\n\nRanges and probabilities are the natural metric for quantifying uncertainty and can be applied to any well-defined question or quantity of interest.\nThe question for assessment, or at least the eventual conclusion, needs to be well-defined, in order for its uncertainty to be assessed."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#qualitative-terms-defined-by-quantitative-expression",
    "href": "pmguidance/pmanalysis_background.html#qualitative-terms-defined-by-quantitative-expression",
    "title": "Background",
    "section": "Qualitative terms defined by quantitative expression",
    "text": "Qualitative terms defined by quantitative expression\n\nIf qualitative terms are used to describe the degree of uncertainty, they should be clearly defined with objective scientific criteria.\nSpecifically, the definition should identify the quantitative expression of uncertainty associated with the qualitative term as is done, for example, in the EFSA approximate probability scale"
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#firm-conclusion",
    "href": "pmguidance/pmanalysis_background.html#firm-conclusion",
    "title": "Background",
    "section": "“Firm” conclusion",
    "text": "“Firm” conclusion\n\nFor some types of assessment, decision-makers need EFSA to provide an unqualified positive or negative conclusion. The positive or negative conclusion does not imply that there is complete certainty, since this is never achieved, but that the level of certainty is sufficient for the purpose of decision-making.\nIn such cases, the assessment conclusion and summary may simply report the positive or negative conclusion but, for transparency, the justification for the conclusion should be documented somewhere, e.g. in the body of the assessment or an annex."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#inconclusive",
    "href": "pmguidance/pmanalysis_background.html#inconclusive",
    "title": "Background",
    "section": "Inconclusive",
    "text": "Inconclusive\nIf the level of certainty is not sufficient, then either the uncertainty should be expressed quantitatively, or assessors should report that their assessment is inconclusive and that they ‘cannot conclude’ on the question."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#language-of-no-quantification",
    "href": "pmguidance/pmanalysis_background.html#language-of-no-quantification",
    "title": "Background",
    "section": "Language of no quantification",
    "text": "Language of no quantification\n\nWhen assessors do not quantify uncertainty, they must report that the probability of different answers is unknown and avoid using any language that could be interpreted as implying a probability statement as this would be misleading."
  },
  {
    "objectID": "pmguidance/pmanalysis_background.html#avoid-value-expressions",
    "href": "pmguidance/pmanalysis_background.html#avoid-value-expressions",
    "title": "Background",
    "section": "Avoid value expressions",
    "text": "Avoid value expressions\n\nThe assessors should avoid any verbal expressions that have risk management connotations in everyday language, such us ‘negligible’ and ‘concern’. When used without further definition, such expressions imply two simultaneous judgements: a judgement about the probability (or approximate probability) of adverse effects, and a judgement about the acceptability of that probability. The first of these judgements is within the remit of assessors, but the latter is not."
  },
  {
    "objectID": "pmguidance/pmanalysis_mainelements.html",
    "href": "pmguidance/pmanalysis_mainelements.html",
    "title": "Main elements of uncertainty analysis",
    "section": "",
    "text": "What is needed depends in part on the general type of the assessment\n\nstandardised assessment\ncase-specific assessment\ndevelopment, review of a standardised procedure or\nurgent assessment),\n\nand partly on the specific needs of the individual assessment, which need to be decided by assessors.\n\n\n\n\n\n\n\nIdentifying uncertainties affecting the assessment\n\n\n\n\n\n\nSource of uncertaintyAlways necessaryGeneric listsStandardised procedures\n\n\nAn individual contribution to uncertainty\n\n\nThis is necessary in every assessment, and should be done in a structured way to minimise the chance of overlooking relevant uncertainties.\n\n\nIt is useful to develop generic lists of standard and non-standard uncertainties.\n\n\nIn assessments that follow standardised procedures, it is only necessary to identify non-standard uncertainties.\n\n\n\n\n\n\n\n\n\n\n\n\nPrioritising uncertainties within the assessment\n\n\n\n\n\n\nPlays an important role in the planning the uncertainty analysis, enabling the assessor to focus detailed analysis on the most important uncertainties and address others collectively when evaluating overall uncertainty. Often prioritisation will be done by expert judgement during the planning process, but in more complex assessments it may be done explicitly, using influence or sensitivity analysis.\n\n\n\n\n\n\n\n\n\n\nDividing the uncertainty analysis into parts (when appropriate)\n\n\n\n\n\n\nIn some assessments, it may be sufficient to characterise overall uncertainty for the whole assessment directly, by expert judgement. In other cases, it may be preferable to evaluate uncertainty for some or all parts of the assessment separately and then combine them, either by calculation or expert judgement.\n\n\n\n\n\n\n\n\n\n\nEnsuring the questions or quantities of interest are well-defined\n\n\n\n\n\n\nThis is necessary in every assessment. Some assessments follow standardised procedures, within which the questions and/or quantities of interest should be predefined. In other assessments, the assessors will need to identify and define the questions and/or quantities of interest case by case.\n\n\n\n\n\n\n\n\n\n\nCharacterising uncertainty for parts of the uncertainty analysis\n\n\n\n\n\n\nThis is needed for assessments where the assessors choose to divide the uncertainty analysis into parts, but may only be done for some of the parts, with the other parts being considered when characterising overall uncertainty.\n\n\n\n\n\n\n\n\n\n\nCombining uncertainty from different parts of the uncertainty analysis\n\n\n\n\n\n\nThis is needed for assessments where the assessors quantify uncertainty separately for two or more parts of the uncertainty analysis.\n\n\n\n\n\n\n\n\n\n\nCharacterising overall uncertainty\n\n\n\n\n\n\nExpressing quantitatively the overall impact of as many as possible of the identified uncertainties, and describing qualitatively any that remain unquantified. This is necessary in all assessments except standardised assessments where no non-standard uncertainties are identified.\n\n\n\n\n\n\n\n\n\n\nPrioritising uncertainties for future investigation\n\n\n\n\n\n\nThis is implicit or explicit in any assessment where recommendations are made for future data collection or research, and may be informed by influence or sensitivity analysis.\n\n\n\n\n\n\n\n\n\n\nReporting uncertainty analysis\n\n\n\n\n\n\nRequired for all assessments, but extremely brief in standardised assessments where no non-standard uncertainties are identified."
  },
  {
    "objectID": "pmguidance/pmanalysis_keyconcepts.html",
    "href": "pmguidance/pmanalysis_keyconcepts.html",
    "title": "Key concepts",
    "section": "",
    "text": "Conditional nature of uncertainty\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty and variability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependencies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModels and model uncertainty\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvidence, agreement, confidence and weight of evidence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluence, sensitivity and prioritisation of uncertainties\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConservative assessments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpert judgement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverall uncertainty\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnquantified uncertainties\n\n\n\n\n\n\n\n\n\n\n\n\nIt is therefore important to quantify the overall impact of as many as possible of the identified uncertainties, and identify any that cannot be quantified. The most direct way to achieve this is to try to quantify the overall impact of all identified uncertainties, as this will reveal any that cannot be quantified.\n\n\n\n\n\n\nConditionality of assessments"
  },
  {
    "objectID": "pmguidance/methods.html",
    "href": "pmguidance/methods.html",
    "title": "Overview of methods for Uncertainty Analysis",
    "section": "",
    "text": "Methods for obtaining expert judgements\n\n\n\n\n\n\n\nUse of expert judgementsFormal to less formal EKE\n\n\nAll scientific assessment involves the use of expert judgement\nWhere suitable data are available, this should be used in preference to relying solely on expert judgement.\nWhen data are strong, uncertainty may be quantified by statistical analysis, and any additional extrapolation or uncertainty addressed by minimal assessment (EFSA EKE GD), or collectively as part of the assessment of overall uncertainty.\nWhen data are weak or diverse, it may be better to quantify uncertainty by expert judgement, supported by consideration of the data.\n\n\n\nFormal expert knowledge elicitation (EKE) have been developed to counter psychological biases and to manage the sharing and aggregation of judgements between experts.\nFormal elicitation requires significant time and resources, so it is not feasible to apply it to every source of uncertainty affecting an assessment. Instead, semi-formal expert knowledge elicitation can be applied on less important sources of uncertainty.\nScientific judgements made, usually by a Working Group of experts preparing the assessment, are referred to in this document as judgements by expert group judgement.\nIn practice, there is not a dichotomy between more and less formal approaches to EKE, but rather a continuum.\n\n\n\n\n\n\n\n\n\n\n\n\nQualitative methods for analysing uncertainty\n\n\n\n\n\n\n\nUse of qualitative methodsExamples\n\n\nQualitative methods characterise uncertainty using descriptive expression or ordinal scales, without quantitative definitions\n\nThey rely on careful use of language and expert judgement.\nQualitative methods may provide a useful aid for experts when making quantitative judgements.\n\n\n\nQualitative methods\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethods for quantifying uncertainty\n\n\n\n\n\n\n\nUse of qualitative methodsExamples\n\n\n\n\n\n\nQuantitative methods\n\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigating influence and sensitivity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacterisation of overall uncertainty\n\n\n\n\n\n\n\nThere are three options for quantifying the overall uncertainty, depending on the context:\n\nOption 1: Make a single judgement of the overall impact of all the identified uncertainties.\nOption 2: Quantify uncertainty separately in some parts of the assessment, combine them by calculation, and then adjust the result of the calculation by expert judgement to account for the additional uncertainties that are not yet included.\nOption 3: Quantify uncertainty separately in some parts of the assessment and combine them by calculation, as in Option 2. Then quantify the contribution of the additional uncertainties separately, by expert judgement, and combine it with the previously quantified uncertainty by calculation.\n\n\n\n\n\n\n\n\n\n\n\nReporting uncertainty analysis in scientific assessments"
  },
  {
    "objectID": "pmguidance/pmcommunication.html",
    "href": "pmguidance/pmcommunication.html",
    "title": "Principles of Uncertainty Communication",
    "section": "",
    "text": "Messages about uncertainty need to be based on information that is provided in EFSA’s scientific assessments. Therefore, this section describes the main types of expressions of uncertainty that are expected to result from uncertainty analyses following EFSA’s Uncertainty Analysis GD.\n\nUnqualified conclusion, with no expression of uncertainty. This occurs in two situations:\n\nWhen a standardised assessment procedure only takes into account standard uncertainties, its outcome may be reported as an unqualified conclusion, without any expression of uncertainty (see UA, SO for more explanation)\nWhen uncertainty is present in an assessment, but decision-makers or legislation requires an unqualified conclusion (e.g. safe, not safe or ‘cannot conclude’), without any expression of uncertainty. In some of these cases, uncertainty expressions may be included elsewhere in the assessment report, e.g. in the detailed results or in an annex\n\nDescription of a source of uncertainty. Verbal description of a source or cause of uncertainty. In some areas of EFSA’s work, there are standard terminologies for describing some types of uncertainties, but often descriptions are specific to the assessment in hand (see SO)\nQualitative description of the direction and/or magnitude of uncertainty using words or symbols. Words or an ordinal scale describing how much a source of uncertainty affects the assessment or its conclusion (e.g. low, medium or high uncertainty; conservative, very conservative or non-conservative; unlikely, likely or very likely; or symbols indicating the direction and magnitude of uncertainty: —, –,-,+, ++, +++) Because the meaning of such expressions is ambiguous, EFSA’s Uncertainty Analysis GD recommends that they should not be used unless they are accompanied by a quantitative definition (see UA)\nInconclusive assessment. This occurs in two situations:\n\nWhen decision-makers or legislation require an unqualified conclusion but assessors judge there is too much uncertainty to give one and report that they cannot conclude. The basis for this uncertainty expression should be documented in the body of the assessment report or an annex, and may include one or more uncertainty expressions\nWhen it is not required that conclusions must be unqualified, but the assessors are unable to give any quantitative expression of uncertainty or, where they judge that their probability for a conclusion could be anywhere between 0% and 100%. This should be accompanied by a qualitative description of the uncertainties (see description of ‘A precise probability’ below)\n\nA precise probability. A single number (in EFSA outputs: a percentage between 0% and 100%) quantifying the likelihood of either:\n\nA specified answer to a question (e.g. a ‘yes’ answer to a ‘yes/no’ question)\nA specified quantity lying in a specified range of values, or above or below a specified value (e.g. 90% probability that between 10 and 100 infected organisms will enter the EU in 2019; 5% probability that more than 100 infected organisms will enter).\n\n\n\nNote that the term “precise” is used here to refer to how the probability is expressed, as a single number, and does not imply that it is actually known with absolute precision, which is not possible\n\n\nAn approximate probability. Any range of probabilities (e.g. 10–20% probability) providing an approximate quantification of likelihood for either:\n\nA specified answer to a question (e.g. a ‘yes’ answer to a ‘yes/no’ question)\nA specified quantity lying in a specified range of values, or above or below a specified value (e.g. 1–10% probability that more than 100 infected organisms will enter the EU in 2019)\n\n\n\nThe probability ranges used in EFSA’s approximate probability scale (see Table 1) are examples of approximate probability expressions. Assessors are not restricted to the ranges in the approximate probability scale and should use whatever ranges best reflect their judgement of the uncertainty (see UA)\n\n\nA probability distribution. A graph showing probabilities for different values of an uncertain quantity that has a single true value (e.g. the average exposure for a population). The graph can be plotted in various formats, most commonly a probability density function (PDF), cumulative distribution function (CDF) or complementary cumulative distribution function (CCDF) (see Section 4.1.4.2 in UC)\nA two-dimensional probability distribution. In this guidance, the term ‘two-dimensional (or 2D) probability distribution’ refers to a distribution that quantifies the uncertainty of a quantity that is variable, i.e. takes multiple true values (e.g. the exposure of different individuals in a population). This is most often plotted as a CDF or CCDF representing the median estimate of the variability, with confidence or probability intervals quantifying the uncertainty around the CDF or CCDF.\n\n\n\n\n\nTable 1:  Approximate probability scale \n \n  \n    Probability term \n    Subjective probability range \n    Additional options \n     \n  \n \n\n  \n    Almost certain \n    99-100% \n    More likely than not: >50% \n    Unable to give any probability: range is 0-100%. Report as 'inconclusive', 'cannot conclude' or 'unknown' \n  \n  \n    Extremely likely \n    95-99% \n   \n   \n  \n  \n    Very likely \n    90-95% \n   \n   \n  \n  \n    Likely \n    66-90% \n   \n   \n  \n  \n    About as likely as \n    33-66% \n     \n   \n  \n  \n    Unlikely \n    10-33% \n   \n   \n  \n  \n    Very unlikely \n    5-10% \n   \n   \n  \n  \n    Extremely unlikely \n    1-5% \n   \n   \n  \n  \n    Almost impossible \n    0-1%"
  },
  {
    "objectID": "pmguidance/pmanalysis_concerns.html",
    "href": "pmguidance/pmanalysis_concerns.html",
    "title": "Common concerns and objections to quantitative expression of uncertainty",
    "section": "",
    "text": "Many, not all, relate to the role of expert judgement in quantifying uncertainty.\nThe EFSA Scientific Committee considered these concerns carefully and concluded that all of them can be addressed, either by improved explanation of the principles involved or through the use of appropriate methods for obtaining and using quantitative expressions.\n\n\n\n\n\n\n\nQuantifying uncertainty requires complex computations, or excessive time or resource:\n\n\n\n\n\n\nmost of the options in the Guidance do not require complex computations, and the methods are scalable to any time and resource limitation, including urgent situations.\n\n\n\n\n\n\n\n\n\n\nQuantifying uncertainty requires extensive data:\n\n\n\n\n\n\nuncertainty can be quantified by expert judgement for any well-defined question or quantity, provided there is at least some relevant evidence.\n\n\n\n\n\n\n\n\n\n\nData are preferable to expert judgement:\n\n\n\n\n\n\nthe Guidance recommends use of relevant data where available.\n\n\n\n\n\n\n\n\n\n\nSubjectivity is unscientific:\n\n\n\n\n\n\nAll judgement is subjective, and judgement is a necessary part of all scientific assessment. Even when good data are available, expert judgement is involved in evaluating and analysing them, and when using them in risk assessment.\n\n\n\n\n\n\n\n\n\n\nSubjective judgements are guesswork and speculation:\n\n\n\n\n\n\nall judgements in EFSA assessments will be based on evidence and reasoning, which will be documented transparently.\n\n\n\n\n\n\n\n\n\n\nExpert judgement is subject to psychological biases:\n\n\n\n\n\n\nEFSA’s guidance on uncertainty analysis and expert knowledge elicitation use methods designed to counter those biases.\n\n\n\n\n\n\n\n\n\n\nQuantitative judgements are over-precise:\n\n\n\n\n\n\nEFSA’s methods produce judgements that reflect the experts’ uncertainty – if they feel they are over-precise, they should adjust them accordingly.\n\n\n\n\n\n\n\n\n\n\nUncertainty is exaggerated:\n\n\n\n\n\n\nidentify your reasons for thinking the uncertainty is exaggerated, and revise your judgements to take them into account.\n\n\n\n\n\n\n\n\n\n\nThere are too many uncertainties:\n\n\n\n\n\n\nwhenever experts draw conclusions, they are necessarily making judgements about all the uncertainties they are aware of. The Guidance provides methods for assessing uncertainties collectively that increase the rigour and transparency of those judgements.\n\n\n\n\n\n\n\n\n\n\nProbability judgements are themselves uncertain:\n\n\n\n\n\n\ntake the uncertainty of your judgement into account as part of the judgement, e.g. by giving a range, or making it wider.\n\n\n\n\n\n\n\n\n\n\nGiving precise quantiles for uncertainty is over-confident:\n\n\n\n\n\n\nthe quantiles will not be treated as precise, but as a step in deriving a distribution for you to review and adjust. If there is concern about the choice of distribution, its impact on the analysis can be assessed by sensitivity analysis. Alternatively, approximate probabilities could be used.\n\n\n\n\n\n\n\n\n\n\nThere are some uncertainties I cannot make a probability judgement for:\n\n\n\n\n\n\nin principle, probability judgements can be given for all well-defined questions or quantities. However, the Guidance recognises that experts may be unable to make probability judgements for some uncertainties, and provides options for dealing with this.\n\n\n\n\n\n\n\n\n\n\nDifferent experts will make different judgements:\n\n\n\n\n\n\nthis is expected and inevitable, whether the judgements are quantitative or not. An advantage of quantitative expression is that those differences are made explicit and can be discussed, leading to better conclusions. These points apply to experts working on the same assessment, and also to different assessments of the same question by different experts or institutions.\n\n\n\n\n\n\n\n\n\n\nI cannot give a probability for whether a model is correct:\n\n\n\n\n\n\nno model is entirely correct. Model uncertainty is better expressed by making a probability judgement for how different the model result might be from the real value.\n\n\n\n\n\n\n\n\n\n\nUncertainty should be addressed by conservative assumptions:\n\n\n\n\n\n\nchoosing a conservative assumption involves two judgements – the probability that the assumption is valid, and the acceptability of that probability. The Guidance improves the rigour and transparency of the first judgement, providing a better basis for the second (which is part of risk management).\n\n\n\n\n\n\n\n\n\n\nProbabilities cannot be given for qualitative conclusions:\n\n\n\n\n\n\nProbability judgements can be made for any well-defined conclusion, and all EFSA conclusions should be well-defined.\n\n\n\n\n\n\n\n\n\n\nYou cannot make judgements about unknown unknowns:\n\n\n\n\n\n\nno such judgements are implied. All scientific advice is conditional on assumptions about unknown unknowns.\n\n\n\n\n\n\n\n\n\n\nUncertainty is unquantifiable by definition:\n\n\n\n\n\n\nthis is the Knightian view. The Guidance uses subjective probability, which Knight recognised as an option.\n\n\n\n\n\n\n\n\n\n\nProbabilities cannot be given unless all the possibilities can be specified:\n\n\n\n\n\n\nprovided an answer to a question is well-defined, a probability judgement can be made for it without specifying or knowing all possible alternative answers. However, assessors should guard against a tendency to underestimate the probability of other answers when they are not differentiated.\n\n\n\n\n\n\n\n\n\n\nNone of the ranges in the approximate probability scale properly represent my judgement:\n\n\n\n\n\n\nspecify a range that does.\n\n\n\n\n\n\n\n\n\n\nLack of evidence:\n\n\n\n\n\n\nif there really is no evidence, no probability judgement can be made – and no scientific conclusion can be drawn.  Another way to see it, is that if the experts can make a conclusion (that is not inconclusive), they should also be able to express their level of certainty about it.\n\n\n\n\n\n\n\n\n\n\nIt is not valid to combine probabilities derived from data with probabilities derived by expert judgement:\n\n\n\n\n\n\nthere is a well-established theoretical basis for using probability calculations to combine probability judgements elicited from experts (including probability judgements informed by non Bayesian statistical analysis) with probabilities obtained from Bayesian statistical analysis of data.\n\n\n\n\n\n\n\n\n\n\nThe result of the uncertainty analysis is incompatible with, or undermines, our conclusion:\n\n\n\n\n\n\nreconsider both the uncertainty analysis and the conclusion, and revise one or both so they (a) match and (b) properly represent what the science supports. A justifiable conclusion takes account of uncertainty, so there should be no inconsistency.\n\n\n\n\n\n\n\n\n\n\nDecision-makers require us to say whether a thing is safe or not safe, not give a probability for being safe:\n\n\n\n\n\n\n‘safe’ implies some acceptable level of certainty, so if that is defined then positive or negative conclusion may be given without qualification.\n\n\n\n\n\n\n\n\n\n\nRisk managers and the public do not want to know about uncertainty:\n\n\n\n\n\n\nactually many do, and as a matter of principle, decision-makers need information on uncertainty to make rational decisions.\n\n\n\n\n\n\n\n\n\n\nCommunicating uncertainty will undermine public confidence in scientific assessment:\n\n\n\n\n\n\nsome evidence supports this, but other evidence suggests communicating uncertainty can increase confidence. EFSA’s approach on communicating uncertainty is designed to achieve the latter."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Video transcript"
  },
  {
    "objectID": "intro.html#about",
    "href": "intro.html#about",
    "title": "Introduction",
    "section": "About",
    "text": "About\nThis tutorial is provided within OC/EFSA/KNOW/2022/01 Lot 2 Trainings on horizontal scientific assessment methodologies EFSA training.\nThis page was created by Ullrika Sahlin and Dmytro Perepolkin."
  },
  {
    "objectID": "communication.html",
    "href": "communication.html",
    "title": "Communication of Uncertainty",
    "section": "",
    "text": "Welcome to the page on Uncertainty Communication!\nThe pupose of the section is to introduce you to the “Guidance on Communication of Uncertainty in Scientific Assessments”. The material for this page is heavy based on the Principles of Uncertainty Communication and we strongly recomend that you start your journey into the world of uncertainty communication there.\nWe divided the material on this page into two sections: there’s a section for uncertainty communicators and another one for the assessors. The presentation of the material also varies by the preparation level of the target audience. The material for the Communicators covers the Entry and Informed levels, while the Assessors are encouraged to refer to guidance for communication on the Informed and Technical levels.\nUse one of the buttons below to proceed to the next page, where you will be presented with a communication checklist and the corresponding recommendations addressing the various aspects of uncertainty communication for the different audience levels.\n\n\nIntroduction to the page. Information about the UC GD and that this is the GD based on the principles from the UC GD and that we recommend looking at those before going through the UC GD page. Instructions on how to explore the page. Recommendations when it can be useful to consult it when you are working with an assessment.\n\nI am:\n\n\n\n\n%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([Communicator])\n  B([Assessor])\n  click A \"comguidance/communicator.html\" \"Go to Communicator page\"\n  click B \"comguidance/assessor.html\" \"Go to Assessor page\"\n\n\n\n\n\n\n\n\nThe introduction section also provides an overview of risk communication in EFSA, covering the main strategies and the criteria for proper audience targeting. We introduce the expressions of uncertainty used at EFSA and invite the user to select one of the two sections to proceed\nThe rest of the content on the page is split between the communicator, and the assessor role. The user has the opportunity to select the other role and explore the content in the respective section. The Communicator section provides guidance for communication on the entry-level and the informed level, while the Assessor section provides guidance for the technical level.\nFinally, both the Communicator and the Assessor roles share the section for the Specific Guidances, consisting of the 9 types of expressions of uncertainty as shown in Figure 4. The content of the Specific Guidances varies depending on the role (the Communicator is shows the entry and informed level and the Assessor is shown the technical level)\nEach of the specific guidances (for assessors) is accompanied by a short text and links to best-practice examples (i.e. where these Specific Guidances were followed) in the implementation section (below).\nWe will also refer to additional information in the Principles and Methods opinion on reporting and communicating uncertainties (Ch 15 and 16)."
  },
  {
    "objectID": "implementationmethods.html",
    "href": "implementationmethods.html",
    "title": "Methods for Uncertainty Analysis",
    "section": "",
    "text": "Introduction to the page. Information about this page collects examples of implementation of the UA and UC GDs. Instructions on how to explore the page. How we have chosen the examples. A reminder that this is an extraction of examples and that there are more on EFSA website.\n\nHere we will have the interactive data table of examples. Currently, we plan to have the following fields in the table:\n\nTitle\nDOI\nPanel\nExpression of uncertainty (from the nine expressions)\nQuantitative method for uncertainty analysis\nQualitative method for uncertainty analysis\nDescription\n\nThe dataframe below is provided as a placeholder."
  },
  {
    "objectID": "principles.html",
    "href": "principles.html",
    "title": "Principles and methods behind the guidances",
    "section": "",
    "text": "Welcome to the page on Principles and Methods for the Uncertainty Analysis and Communication of Uncertainty guidances!\nIn this sections you can learn more about the underlying concepts behind the two main guidance documents.\nThe concepts described on this page are primarily coming from the Principles and Methods Document (pointing to the right), as well as the Section 3 in the Communication Guidance (pointing to the right).\nOn the left hand side you can see the side menu which contains the links to the pages for respective sections. The methods for uncertainty analysis are discussed separately. You might want to check that page out when you are unsure about certain method or want to identify an appropriate method of handling uncertainty appropriate for your situation.\nLook around and make yourself at home here, because you might come back here after you’ve explored the rest of the tutorial for some in-depth insights! See you!"
  },
  {
    "objectID": "uncertaintyanalysis.html",
    "href": "uncertaintyanalysis.html",
    "title": "Uncertainty Analysis Guidance",
    "section": "",
    "text": "The recommended approach to uncertainty analysis depends on the nature of the scientific assessment in hand. The video below introduces the different kinds of assessments and the associated process.\n\nIdentify which of the following types your assessment most corresponds to and then proceed to the corresponding section for guidance specific to that type.\n\n\n\n\n%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([Standardized</br>Assessments])\n  B([Case-specific</br>assessments])\n  click A \"uaguidance/figure1.html\" \"A standardised assessment follows a pre-established standardised procedure that covers every step of the assessment\"\n  click B \"uaguidance/figure5.html\" \"Scientific assessments where there is no pre-established standardised procedure, so the assessors have to develop an assessment plan that is specific to the case in hand. Standardised elements (e.g. default values) may be used for some parts of the assessment, but other parts require case-specific approaches.\"\n\n\n\n\n\n\n\n\n\n\n\n\n%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  C([Urgent</br>assessments])\n  D([Development or revision</br>of guidance documents])\n  click C \"uaguidance/figure11.html\" \"Assessments that, for any reason, must be completed within an unusually short period of time or with unusually limited resources, and therefore require streamlined approaches to both assessment and uncertainty analysis\"\n  click D \"uaguidance/figure12.html\" \"Especially (but not only) those that describe existing standardised procedures or establish new ones\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearn more about:\n\n\n\n\nIdentifying uncertainties affecting the assessment\nPrioritizing uncertainties within the assessment\nDividing the uncertainty analysis into parts\nEnsuring that the questions or quantities of interest are well-defined.\nCharacterizing uncertainty for parts of the uncertainty analysis\nCombining uncertainty from different parts of the uncertainty analysis\nCharacterising overall uncertainty\nPrioritising uncertainties for future investigation.\nReporting uncertainty analysis"
  }
]