[
  {
    "objectID": "principles-and-methods/methods-sources-methodology.html",
    "href": "principles-and-methods/methods-sources-methodology.html",
    "title": "Questions to identify sources of uncertainty affecting assessment methodology",
    "section": "",
    "text": "General types of uncertainty affecting assessment methodology, including how the assessment inputs are combined, together with questions that may help to identify them in specific assessments\n\n\n\n\n\n\n1. Ambiguity\n\n\n\n\n\nIf the assessment combines inputs using mathematical or statistical model(s) that were developed by others, are all aspects of them adequately described, or are multiple interpretations possible?\n\n\n\n\n\n\n\n\n\n2. Excluded factors\n\n\n\n\n\nAre any potentially relevant factors or processes excluded? (e.g. excluded modifying factors, omitted sources of additional exposure or risk).\n\n\n\n\n\n\n\n\n\n3. Distribution choice\n\n\n\n\n\nAre distributions used to represent variable quantities? If so, how closely does the chosen form of distribution (normal, lognormal, etc.) represent the real pattern of variation? What alternative distributions could be considered?\n\n\n\n\n\n\n\n\n\n4. Use of fixed values\n\n\n\n\n\nDoes the assessment include fixed values representing quantities that are variable or uncertain, e.g. default values or conservative assumptions? If so, are the chosen values appropriate for the needs of the assessment, such that when considered together they provide an appropriate and known degree of conservatism in the overall assessment?\n\n\n\n\n\n\n\n\n\n5. Relationship between parts of the assessment\n\n\n\n\n\nIf the assessment model or reasoning represents a real process, how well does it represent it? If it is a reasoned argument, how strong is the reasoning? Are there alternative structures that could be considered? Are there dependencies between variables affecting the question or quantity of interest? How different might they be from what is assumed in the assessment?\n\n\n\n\n\n\n\n\n\n6. Evidence for the structure of the assessment\n\n\n\n\n\nWhat is the nature, quantity, relevance, reliability and quality of data or evidence available to support the structure of the model or reasoning used in the assessment? Where the assessment or uncertainty analysis is divided into parts, is the division into parts and the way they are subsequently combined appropriate?\n\n\n\n\n\n\n\n\n\n7. Uncertainties relating to the process for dealing with evidence from the literature\n\n\n\n\n\nWas a structured approach used to identify relevant literature? How appropriate were the search criteria and the list of sources examined? Was a structured approach used to appraise evidence? How appropriate were the criteria used for this? How consistently were they applied? Were studies filtered or prioritised for detailed appraisal? Was any potentially relevant evidence set aside or excluded? If so, its potential contribution should be considered as part of the characterisation of overall uncertainty.\n\n\n\n\n\n\n\n\n\n8. Expert judgement\n\n\n\n\n\nIdentify where expert judgement was used: in obtaining and interpreting estimates based on statistical analysis of data, in obtaining estimates by expert elicitation, in choices about assessment methods, models and reasoning? How many experts participated, how relevant and extensive was their expertise and experience for making them, and to what extent did they agree? Was a structured elicitation methodology used and, if so, how formal and rigorous was the procedure?\n\n\n\n\n\n\n\n\n\n9. Calibration or validation with independent data\n\n\n\n\n\nHas the assessment, or any component of it, been calibrated or validated by comparison with independent information? If so, consider the following: What uncertainties affect the independent information? Assess this by considering all the questions listed above for assessing the uncertainty of inputs How closely does the independent information agree with the assessment output or component to which it pertains, taking account of the uncertainty of each? What are the implications of this for your uncertainty about the assessment?\n\n\n\n\n\n\n\n\n\n10. Dependency between sources of uncertainty\n\n\n\n\n\nAre there dependencies between any of the sources of uncertainty affecting the assessment and/or its inputs, or regarding factors that are excluded? If you learned more about any of them, would it alter your uncertainty about one or more of the others?\n\n\n\n\n\n\n\n\n\n11. Other uncertainties\n\n\n\n\n\nAre there any uncertainties about assessment methods or structure, due to lack of data or knowledge gaps, which are not covered by other categories above?"
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-concerns.html",
    "href": "principles-and-methods/uncertainty-analysis-concerns.html",
    "title": "Common concerns and objections to quantitative expression of uncertainty",
    "section": "",
    "text": "Many concerns and objections to quantitative expression of uncertainty have been raised by various parties during the public consultation and trial period for this document and in the literature.\nMany, not all, relate to the role of expert judgement in quantifying uncertainty.\nThe EFSA Scientific Committee considered these concerns carefully and concluded that all of them can be addressed, either by improved explanation of the principles involved or through the use of appropriate methods for obtaining and using quantitative expressions.\n\n\n\n\n\n\n\nQuantifying uncertainty requires complex computations, or excessive time or resource:\n\n\n\n\n\nmost of the options in the Guidance do not require complex computations, and the methods are scalable to any time and resource limitation, including urgent situations.\n\n\n\n\n\n\n\n\n\nQuantifying uncertainty requires extensive data:\n\n\n\n\n\nuncertainty can be quantified by expert judgement for any well-defined question or quantity, provided there is at least some relevant evidence.\n\n\n\n\n\n\n\n\n\nData are preferable to expert judgement:\n\n\n\n\n\nthe Guidance recommends use of relevant data where available.\n\n\n\n\n\n\n\n\n\nSubjectivity is unscientific:\n\n\n\n\n\nAll judgement is subjective, and judgement is a necessary part of all scientific assessment. Even when good data are available, expert judgement is involved in evaluating and analysing them, and when using them in risk assessment.\n\n\n\n\n\n\n\n\n\nSubjective judgements are guesswork and speculation:\n\n\n\n\n\nall judgements in EFSA assessments will be based on evidence and reasoning, which will be documented transparently.\n\n\n\n\n\n\n\n\n\nExpert judgement is subject to psychological biases:\n\n\n\n\n\nEFSA’s guidance on uncertainty analysis and expert knowledge elicitation use methods designed to counter those biases.\n\n\n\n\n\n\n\n\n\nQuantitative judgements are over-precise:\n\n\n\n\n\nEFSA’s methods produce judgements that reflect the experts’ uncertainty – if they feel they are over-precise, they should adjust them accordingly.\n\n\n\n\n\n\n\n\n\nUncertainty is exaggerated:\n\n\n\n\n\nidentify your reasons for thinking the uncertainty is exaggerated, and revise your judgements to take them into account.\n\n\n\n\n\n\n\n\n\nThere are too many uncertainties:\n\n\n\n\n\nwhenever experts draw conclusions, they are necessarily making judgements about all the uncertainties they are aware of. The Guidance provides methods for assessing uncertainties collectively that increase the rigour and transparency of those judgements.\n\n\n\n\n\n\n\n\n\nProbability judgements are themselves uncertain:\n\n\n\n\n\ntake the uncertainty of your judgement into account as part of the judgement, e.g. by giving a range, or making it wider.\n\n\n\n\n\n\n\n\n\nGiving precise quantiles for uncertainty is over-confident:\n\n\n\n\n\nthe quantiles will not be treated as precise, but as a step in deriving a distribution for you to review and adjust. If there is concern about the choice of distribution, its impact on the analysis can be assessed by sensitivity analysis. Alternatively, approximate probabilities could be used.\n\n\n\n\n\n\n\n\n\nThere are some uncertainties I cannot make a probability judgement for:\n\n\n\n\n\nin principle, probability judgements can be given for all well-defined questions or quantities. However, the Guidance recognises that experts may be unable to make probability judgements for some uncertainties, and provides options for dealing with this.\n\n\n\n\n\n\n\n\n\nDifferent experts will make different judgements:\n\n\n\n\n\nthis is expected and inevitable, whether the judgements are quantitative or not. An advantage of quantitative expression is that those differences are made explicit and can be discussed, leading to better conclusions. These points apply to experts working on the same assessment, and also to different assessments of the same question by different experts or institutions.\n\n\n\n\n\n\n\n\n\nI cannot give a probability for whether a model is correct:\n\n\n\n\n\nno model is entirely correct. Model uncertainty is better expressed by making a probability judgement for how different the model result might be from the real value.\n\n\n\n\n\n\n\n\n\nUncertainty should be addressed by conservative assumptions:\n\n\n\n\n\nchoosing a conservative assumption involves two judgements – the probability that the assumption is valid, and the acceptability of that probability. The Guidance improves the rigour and transparency of the first judgement, providing a better basis for the second (which is part of risk management).\n\n\n\n\n\n\n\n\n\nProbabilities cannot be given for qualitative conclusions:\n\n\n\n\n\nProbability judgements can be made for any well-defined conclusion, and all EFSA conclusions should be well-defined.\n\n\n\n\n\n\n\n\n\nYou cannot make judgements about unknown unknowns:\n\n\n\n\n\nno such judgements are implied. All scientific advice is conditional on assumptions about unknown unknowns.\n\n\n\n\n\n\n\n\n\nUncertainty is unquantifiable by definition:\n\n\n\n\n\nthis is the Knightian view. The Guidance uses subjective probability, which Knight recognised as an option.\n\n\n\n\n\n\n\n\n\nProbabilities cannot be given unless all the possibilities can be specified:\n\n\n\n\n\nprovided an answer to a question is well-defined, a probability judgement can be made for it without specifying or knowing all possible alternative answers. However, assessors should guard against a tendency to underestimate the probability of other answers when they are not differentiated.\n\n\n\n\n\n\n\n\n\nNone of the ranges in the approximate probability scale properly represent my judgement:\n\n\n\n\n\nspecify a range that does.\n\n\n\n\n\n\n\n\n\nLack of evidence:\n\n\n\n\n\nif there really is no evidence, no probability judgement can be made – and no scientific conclusion can be drawn.  Another way to see it, is that if the experts can make a conclusion (that is not inconclusive), they should also be able to express their level of certainty about it.\n\n\n\n\n\n\n\n\n\nIt is not valid to combine probabilities derived from data with probabilities derived by expert judgement:\n\n\n\n\n\nthere is a well-established theoretical basis for using probability calculations to combine probability judgements elicited from experts (including probability judgements informed by non Bayesian statistical analysis) with probabilities obtained from Bayesian statistical analysis of data.\n\n\n\n\n\n\n\n\n\nThe result of the uncertainty analysis is incompatible with, or undermines, our conclusion:\n\n\n\n\n\nreconsider both the uncertainty analysis and the conclusion, and revise one or both so they (a) match and (b) properly represent what the science supports. A justifiable conclusion takes account of uncertainty, so there should be no inconsistency.\n\n\n\n\n\n\n\n\n\nDecision-makers require us to say whether a thing is safe or not safe, not give a probability for being safe:\n\n\n\n\n\n‘safe’ implies some acceptable level of certainty, so if that is defined then positive or negative conclusion may be given without qualification.\n\n\n\n\n\n\n\n\n\nRisk managers and the public do not want to know about uncertainty:\n\n\n\n\n\nactually many do, and as a matter of principle, decision-makers need information on uncertainty to make rational decisions.\n\n\n\n\n\n\n\n\n\nCommunicating uncertainty will undermine public confidence in scientific assessment:\n\n\n\n\n\nsome evidence supports this, but other evidence suggests communicating uncertainty can increase confidence. EFSA’s approach on communicating uncertainty is designed to achieve the latter."
  },
  {
    "objectID": "principles-and-methods/methods-quantitative.html",
    "href": "principles-and-methods/methods-quantitative.html",
    "title": "Methods to quantify uncertainty",
    "section": "",
    "text": "Expressing uncertainty using probability and alternatives to probability\n\n\n\n\n\n\nSummaryAddressing variabilityDeterministic alternatives to probabilityExpressing uncertainty using possibility\n\n\nFor yes/no questions or binary quantities, uncertainty can be quantitatively expressed by assigning probabilities to the two possible answers. The probabilities must sum to 100%, determining the probability for the other answer. Uncertainty for a non-variable quantity can be fully quantified by specifying a probability distribution, indicating the probability of the true value falling within any given range. Partial quantification of uncertainty can be achieved by specifying a credible interval, a range of values of interest along with the probability of the true value lying within that range. Additional ranges and probabilities provide a more complete quantification. Probabilities and distributions can be derived from expert judgment, statistical analysis of data, or calculations involving other probabilities. Approximate probabilities, expressed as ranges, can be used to simplify the specification process. Probability bounds are special cases of credible intervals, allowing for approximate probabilities. Probability bounds are useful for combining uncertainties of multiple quantities in a deterministic model. Expert judgment and statistical analysis are both valid approaches for obtaining probabilities, with statistical analysis preferred when applicable. Combining statistical results with expert judgment is often recommended. Calculations based on models are discussed for combining uncertainties expressed using probabilities.\n\n\nQuantifying uncertainty about a variable quantity is more challenging than quantifying uncertainty about a quantity with a single uncertain true value. The first step is to define the variable and specify its context or scope. Fully quantifying uncertainty about a variable involves modeling its variability, typically using a statistical model. This model can be a family of probability distributions or a more complex model of variable relationships. Uncertainty about the variability is expressed by using probability distributions to represent uncertainty about parameters in the statistical model. The choice of statistical model also introduces uncertainty, which should be considered in the analysis. For example, in a linear regression model, uncertainty about the parameters affects the uncertainty of percentiles or individual response values. By expressing uncertainty about the parameters using a joint probability distribution, the result is a probability distribution that represents uncertainty about percentiles or individual responses based on the covariate. Partial expression of uncertainty about a variable is possible but may require specialized knowledge and is less commonly used. Full quantification is necessary when the entire distribution of variability is of interest, while partial quantification can be used for specific aspects, such as specified percentiles. The approach taken to address uncertainty about variables has significant implications for calculating uncertainty about the output of a model. This is discussed further in Section 11.4.\n\n\nWhen dealing with a categorical question, if probability is not used to quantify uncertainty, the alternative options are qualitative expression or including the uncertainty in a later expression that combines multiple sources. For an uncertain quantity, the minimum quantitative expression of uncertainty is specifying a range of values, which can be an upper or lower bound. However, a range by itself does not indicate the probability of including the true value or the relative likelihood of different values within the range. To provide a complete expression of uncertainty, a probability or approximate probability for the range must also be provided. If uncertainty is quantified with a range alone, the missing probability information should be provided later in the process, such as when quantifying overall uncertainty. In cases where absolute upper or lower limits are derived from theoretical considerations, such as a concentration not exceeding 100%, a range with absolute limits implies a probability content of 100%. If using such a range, the probability judgment should be explicitly stated, making it a probabilistic expression. Deterministic methods for working with bounds and ranges are discussed in section 11.6.\n\n\nPossibility theory, along with fuzzy logic and fuzzy sets, has been proposed as an alternative approach to quantify uncertainty. It has been used in conjunction with probabilistic methods, such as Monte Carlo, in risk assessment applications. While fuzzy methods have been applied in various contexts, their benefits compared to probability-based methods are still uncertain. The IPCS (2014) Guidance Document briefly discusses fuzzy methods, acknowledging their ability to handle uncertainties arising from vagueness or incomplete information but noting their inability to provide precise estimates of uncertainty or handle random sampling error. Furthermore, the fuzzy/possibility measure lacks an operational definition comparable to subjective probability, as defined by de Finetti (1937) and Savage (1954). Consequently, these methods are not included in our comprehensive assessment of methods."
  },
  {
    "objectID": "principles-and-methods/methods-quantitative.html#expressing-uncertainty-using-probability-and-alternatives-to-probability",
    "href": "principles-and-methods/methods-quantitative.html#expressing-uncertainty-using-probability-and-alternatives-to-probability",
    "title": "Methods to quantify uncertainty",
    "section": "",
    "text": "Expressing uncertainty using probability and alternatives to probability\n\n\n\n\n\n\nSummaryAddressing variabilityDeterministic alternatives to probabilityExpressing uncertainty using possibility\n\n\nFor yes/no questions or binary quantities, uncertainty can be quantitatively expressed by assigning probabilities to the two possible answers. The probabilities must sum to 100%, determining the probability for the other answer. Uncertainty for a non-variable quantity can be fully quantified by specifying a probability distribution, indicating the probability of the true value falling within any given range. Partial quantification of uncertainty can be achieved by specifying a credible interval, a range of values of interest along with the probability of the true value lying within that range. Additional ranges and probabilities provide a more complete quantification. Probabilities and distributions can be derived from expert judgment, statistical analysis of data, or calculations involving other probabilities. Approximate probabilities, expressed as ranges, can be used to simplify the specification process. Probability bounds are special cases of credible intervals, allowing for approximate probabilities. Probability bounds are useful for combining uncertainties of multiple quantities in a deterministic model. Expert judgment and statistical analysis are both valid approaches for obtaining probabilities, with statistical analysis preferred when applicable. Combining statistical results with expert judgment is often recommended. Calculations based on models are discussed for combining uncertainties expressed using probabilities.\n\n\nQuantifying uncertainty about a variable quantity is more challenging than quantifying uncertainty about a quantity with a single uncertain true value. The first step is to define the variable and specify its context or scope. Fully quantifying uncertainty about a variable involves modeling its variability, typically using a statistical model. This model can be a family of probability distributions or a more complex model of variable relationships. Uncertainty about the variability is expressed by using probability distributions to represent uncertainty about parameters in the statistical model. The choice of statistical model also introduces uncertainty, which should be considered in the analysis. For example, in a linear regression model, uncertainty about the parameters affects the uncertainty of percentiles or individual response values. By expressing uncertainty about the parameters using a joint probability distribution, the result is a probability distribution that represents uncertainty about percentiles or individual responses based on the covariate. Partial expression of uncertainty about a variable is possible but may require specialized knowledge and is less commonly used. Full quantification is necessary when the entire distribution of variability is of interest, while partial quantification can be used for specific aspects, such as specified percentiles. The approach taken to address uncertainty about variables has significant implications for calculating uncertainty about the output of a model. This is discussed further in Section 11.4.\n\n\nWhen dealing with a categorical question, if probability is not used to quantify uncertainty, the alternative options are qualitative expression or including the uncertainty in a later expression that combines multiple sources. For an uncertain quantity, the minimum quantitative expression of uncertainty is specifying a range of values, which can be an upper or lower bound. However, a range by itself does not indicate the probability of including the true value or the relative likelihood of different values within the range. To provide a complete expression of uncertainty, a probability or approximate probability for the range must also be provided. If uncertainty is quantified with a range alone, the missing probability information should be provided later in the process, such as when quantifying overall uncertainty. In cases where absolute upper or lower limits are derived from theoretical considerations, such as a concentration not exceeding 100%, a range with absolute limits implies a probability content of 100%. If using such a range, the probability judgment should be explicitly stated, making it a probabilistic expression. Deterministic methods for working with bounds and ranges are discussed in section 11.6.\n\n\nPossibility theory, along with fuzzy logic and fuzzy sets, has been proposed as an alternative approach to quantify uncertainty. It has been used in conjunction with probabilistic methods, such as Monte Carlo, in risk assessment applications. While fuzzy methods have been applied in various contexts, their benefits compared to probability-based methods are still uncertain. The IPCS (2014) Guidance Document briefly discusses fuzzy methods, acknowledging their ability to handle uncertainties arising from vagueness or incomplete information but noting their inability to provide precise estimates of uncertainty or handle random sampling error. Furthermore, the fuzzy/possibility measure lacks an operational definition comparable to subjective probability, as defined by de Finetti (1937) and Savage (1954). Consequently, these methods are not included in our comprehensive assessment of methods."
  },
  {
    "objectID": "principles-and-methods/methods-quantitative.html#obtaining-probabilities-by-statistical-analysis-of-data",
    "href": "principles-and-methods/methods-quantitative.html#obtaining-probabilities-by-statistical-analysis-of-data",
    "title": "Methods to quantify uncertainty",
    "section": "Obtaining probabilities by statistical analysis of data",
    "text": "Obtaining probabilities by statistical analysis of data\n\n\n\n\n\n\nConfidence intervals\n\n\n\n\n\n\nConfidence intervals are suitable for application across EFSA in situations where standard statistical models are used in order to quantify uncertainty separately about individual statistical model parameters using intervals.\nThe quantification provided is not directly suitable for combining with other uncertainties in probabilistic calculations although expert judgement may be applied in order to support such uses.\n\nRead more in SO Annex B.10\n\n\n\n\n\n\n\n\n\nThe Bootstrap\n\n\n\n\n\n\nThe bootstrap is suitable for application across EFSA in situations where data are randomly sampled and it is difficult to apply other methods of statistical inference.\nIt provides an approximate quantification of uncertainty in such situations and is often easy to apply using Monte Carlo.\nThe results of the bootstrap need to be evaluated carefully, especially when the data sample size is not large or when using an estimator for which the performance of the bootstrap has not been previously considered in detail.\n\nRead more in SO Annex B.11\n\n\n\n\n\n\n\n\n\nBayesian inference\n\n\n\n\n\n\nThe method is suitable for application across EFSA, subject only to availability of the necessary statistical expertise.\nIt can be used for quantification of parameter uncertainty in all parametric statistical models.\nFor all except the simplest models, incorporating expert judgements in prior distributions is likely to require the development of further guidance on EKE.\n\nRead more in SO Annex B.12"
  },
  {
    "objectID": "principles-and-methods/methods-quantitative.html#obtaining-probabilities-by-expert-judgement",
    "href": "principles-and-methods/methods-quantitative.html#obtaining-probabilities-by-expert-judgement",
    "title": "Methods to quantify uncertainty",
    "section": "Obtaining probabilities by expert judgement",
    "text": "Obtaining probabilities by expert judgement\n\n\n\n\n\n\nSemi-formal EKE\n\n\n\n\n\n\nThe method has a high applicability in Working Groups and boards of EFSA and should be applied to quantify uncertainties in all situations:\n\nwhere empirical data from experiments/surveys, literature are limited;\nwhere the purpose of the risk assessment does not require the performance of a full formal EKE;\nor where restrictions in the resources (e.g. in urgent situations) forces EFSA to apply a simplified procedure.\n\nThe method is applicable in all steps of the risk assessment, esp. to summarise the overall uncertainty of the conclusion. Decisions on the risk assessment methods (e.g. risk models, factors, sources of uncertainties) could be judged qualitatively with quantitative elements (e.g. subjective probabilities on appropriateness, what-if scenarios).\nThe method should not substitute the use of empirical data, experiments, surveys or literature, when these are already available or could be retrieved with corresponding resources.\nIn order to enable an EFSA Working Group to perform expert elicitations, all experts should have basic knowledge in probabilistic judgements and some experts of the Working Group should be trained in steering expert elicitations according to the EFSA Guidance.\nDetailed guidance for semi-formal EKE should be developed to complement the existing guidance for formal EKE (EFSA, 2014a,b), applicable to a range of judgement types (quantitative and categorical questions, approximate probabilities, probability bounds, etc.).\n\nRead more in SO Annex B.8\n\n\n\n\n\n\n\n\n\nFormal EKE\n\n\n\n\n\n\nThe method has a high applicability in Working Groups and boards of EFSA and should be applied to quantify uncertainties in situations where empirical data from experiments/surveys, literature are limited and the purpose of the risk assessment is sensitive and need the performance of a full formal EKE.\nThe method is applicable in steps of the risk assessment, where quantitative parameters have to be obtained.\nThe method should not substitute the use of empirical data, experiments, surveys or literature, when these are already available or could be retrieved with corresponding resources.\nIn order to initiate a formal EKE, some experts of the Working Group should be trained in steering expert elicitations according to the EFSA Guidance. In case of complex or sensitive questions, the elicitation should be performed by professional elicitation groups.\nFurther guidance is needed on formal methods for types of expert elicitation not covered by Guidance on expert knowledge elicitation and Guidance on statistical reporting (e.g. for variables, dependencies, qualitative questions, approximate probabilities and probability bounds), as well as on semi-formal methods.\n\nRead more in SO Annex B.9"
  },
  {
    "objectID": "principles-and-methods/methods-quantitative.html#combining-uncertainties-for-model-inputs-by-probability-calculations",
    "href": "principles-and-methods/methods-quantitative.html#combining-uncertainties-for-model-inputs-by-probability-calculations",
    "title": "Methods to quantify uncertainty",
    "section": "Combining uncertainties for model inputs by probability calculations",
    "text": "Combining uncertainties for model inputs by probability calculations\n\n\n\n\n\n\nDeterministic models (calculations)\n\n\n\n\n\nThe methods described in SO Sections 9, 11.2 and 11.3 can be used to quantify uncertainty about inputs to the model in the form of probability distributions or probability bounds. The mathematics of probability then leads in principle to an expression of uncertainty about the output using probability. Calculating that expression is easier in some situations than others.\n\n\n\n\n\n\n\n\n\nProbabilistic models\n\n\n\n\n\nSome probabilistic models are really just deterministic models with variable inputs. They can be handled as described in SO Section 11.4.2. Other models are more innately probabilistic and Monte Carlo simulation has a fundamental role in representing the processes involved, as well as quantifying variable inputs. Examples of this might include models of disease transmission, infection and recovery in a mixed population of susceptible and resistant individuals, or probabilistic modelling of cumulative exposures in a population of individuals to multiple contaminants via multiple routes. While there may be possibilities for some special form of Probability Bounds Analysis in such cases, it is likely to be easier to embed the model in a 2D Monte Carlo analysis or a Bayesian graphical model (see Section 11.5.2) in order to calculate uncertainty for the output of the model from uncertainties expressed about inputs using probability distributions.\n\n\n\n\n\n\n\n\n\nProbability calculations for logic models\n\n\n\n\n\n\nThis is potentially an important tool for EFSA as it provides a way to structure logical arguments involving yes/no conclusions and to calculate the combined uncertainty about a conclusion based on uncertainty about underlying yes/no questions expressed using probability.\n\nRead more in SO Annex B.18\n\n\n\n\n\n\n\n\n\nStructured tools for evidence appraisal\n\n\n\n\n\nStructured approaches for appraising the evidence are valuable methods that should be used in EFSA when assessments include evidence retrieved from the literature and when evaluating studies submitted for regulated products. Several critical appraisal tools are available and there is a need to choose the one that is more appropriate to the study design and adapt it where needed to the specific topic and domain. These approaches enhance consistency and transparency in the evaluation of the risk of bias and other types of uncertainties across a body of evidence. However, they need to be used in conjunction with other methods in the guidance to express the impact of the identified uncertainties on assessment conclusions.\nRead more in SO Annex B.19\n\n\n\n\n\n\n\n\n\nSensitivity analysis\n\n\n\n\n\n\nSensitivity analysis can represent a valuable complement of uncertainty analysis in EFSA. It helps assessors in providing risk managers with information about most influential factors on which to focus actions and further research.\nIt has potential for applicability in any area of work in EFSA.\nObstacles to application of the method could be technical complexity and the need to involve an experienced statistician in the computation and interpretation of some specific methods. Training should be provided to staff and experts in order to facilitate the performance of sensitivity analysis.\nIt is necessary to clarify prior to start the sensitivity analysis which question it is intended to reply, otherwise its value could be limited and not addressing the informative needs.\n\nRead more in SO Annex B.17\n\n\n\n\n\n\n\n\n\nProbability Bounds Analysis\n\n\n\n\n\n\nThis is potentially an important tool for EFSA as it provides a way to incorporate probabilistic judgements without requiring the specification of full probability distributions and without making assumptions about dependence. In so doing, it provides a bridge between interval analysis and Monte Carlo. It allows the consideration of less extreme cases than interval analysis and involves less work than full EKE for distributions followed by Monte Carlo.\nJudgements and concept are rather similar to what EFSA experts do already when using assessment factors and conservative assumptions. Probability bounds analysis provides a transparent and mathematically rigorous calculation which results in an unambiguous quantitative probability statement for the output.\n\nRead more in SO Annex B.13\n\n\n\n\n\n\n\n\n\nMonte Carlo simulation for uncertainty analysis (1D-MC and 2D-MC)\n\n\n\n\n\n\nMC is the most practical way to carry fully probabilistic assessments of uncertainty and uncertainty about variability and is therefore a very important tool.\nApplication of MC is demanding because it requires full probability distributions. Twodimensional MC is particularly demanding because it requires modelling choices (distribution families) and quantification of uncertainty about distribution parameters using statistical inference from data and/or expert knowledge elicitation.\nIt is likely that MC will be used to quantify key uncertainties in some assessments, especially in assessments where variability is modelled, with other methods being used to address other uncertainties.\nMC output can be used to make partial probability statements concerning selected parameters which can then be combined with other partial probability statements using probability bounds analysis.\n\nRead more in SO Annex B.14\n\n\n\n\n\n\n\n\n\nApproximate probability calculations\n\n\n\n\n\nThe method is potentially useful, especially as a quick way to approximately combine uncertainties. However, the fact that the accuracy of the method is generally unknown may limit its usefulness.\nRead more in SO Annex B.15\n\n\n\n\n\n\n\n\n\nAdvanced statistical and probabilistic modelling methodologies\n\n\n\n\n\nStatistical model averaging provides a partial solution to the problem of addressing model uncertainty. Both Bayesian and non-Bayesian versions exist which have, respectively, many of the same strengths and weaknesses identified above for Bayesian inference (Section 11.2.3) and confidence intervals (Section 11.2.1). See details in Sections 11.5.2."
  },
  {
    "objectID": "principles-and-methods/methods-quantitative.html#deterministic-methods-for-quantifying-uncertainty",
    "href": "principles-and-methods/methods-quantitative.html#deterministic-methods-for-quantifying-uncertainty",
    "title": "Methods to quantify uncertainty",
    "section": "Deterministic methods for quantifying uncertainty",
    "text": "Deterministic methods for quantifying uncertainty\n\n\n\n\n\n\nUncertainty tables for quantitative questions\n\n\n\n\n\nA template for listing sources of uncertainty affecting a quantitative question and assessing their individual and combined impacts on the uncertainty of the assessment conclusion.\n\nThis method is applicable to all types of uncertainty affecting quantities of interest, in all areas of scientific assessment. It is flexible and can be adapted to fit within the time available, including urgent situations.\nThe method is a framework for documenting expert judgements and making them transparent. It is generally used for semi-formal expert judgements, but formal techniques (see SO Annex B.9) could be incorporated where appropriate, e.g. when the uncertainties considered are critical to decision-making.\nThe method uses expert judgement to combine multiple uncertainties. The results of this will be less reliable than calculation, it would be better to use uncertainty tables as a technique for facilitating and documenting expert judgement of quantitative ranges for combination by interval analysis. However, uncertainty tables using +/- symbols are a useful option for two important purposes: the need for an initial prioritisation of uncertainties, and to inform probability judgements in the characterisation of overall uncertainty (see SO Section 14).\n\nRead more in SO Annex B.5\n\n\n\n\n\n\n\n\n\nInterval analysis\n\n\n\n\n\nA method to compute a range of values for the output of an assessment calculation based on specified ranges for the individual inputs.\n\nInterval analysis provides a simple and rigorous calculation of bounds for the output. However, it provides only extreme upper and lower values for the output resulting from combinations of inputs and gives no information on probability of values within the output range.\nIt has the potential to be very useful because it can be used to check quickly whether the output range includes both acceptable and unacceptable consequences. If it does, a more sophisticated analysis of uncertainty is needed.\n\nSO Annex B.7\n\n\n\n\n\n\n\n\n\nCalculations with conservative assumptions\n\n\n\n\n\nAssessment factors, conservative assumptions and decision criteria are widely used to account for uncertainty, variability and extrapolation in many areas of EFSA assessment. Some are defaults that can be used in many assessments, while others are specific to particular assessments. They are simple to use and communicate. When well specified and justified they are a valuable tool, providing an appropriate degree of conservatism for the issues they address. They are more reliable when it is possible to calibrate them by statistical analysis of relevant data. Most assessments involve a combination of multiple factors and assumptions, some default and some specific. Conservatism needs to be evaluated for the assessment as a whole, taking account of all the elements involved. Assessing the combined effect of multiple factors and assumptions is much more reliable when done by probabilistic analysis than by expert judgement. In order to be transparent and avoid implying risk management judgements, the degree of conservatism needs to be quantified and agreed with decision-makers. This can be done by providing a probability or approximate probability that the result of the calculation is conservative relative to the quantity of interest. For deterministic calculations that are part of a standardised procedure, this should be done when calibrating the procedure (SO Section 7.1.3). Where deterministic calculations are used in case-specific or urgent assessments, their conservatism could be quantified by expert judgement when characterising overall uncertainty, or the deterministic calculation could be replaced by a probability bounds analysis.\nRead more in SO Annex B.16"
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-main-elements.html",
    "href": "principles-and-methods/uncertainty-analysis-main-elements.html",
    "title": "Main elements of uncertainty analysis",
    "section": "",
    "text": "Some assessments require only some elements, and each element can be implemented in various ways with various methods.\n\nWhat is needed depends in part on the general type of the assessment\n\nstandardised assessment\ncase-specific assessment\ndevelopment, review of a standardised procedure or\nurgent assessment,\n\nand partly on the specific needs of the individual assessment, which need to be decided by assessors.\n\n\n\n\n\n\n\nIdentifying uncertainties affecting the assessment\n\n\n\n\n\nThis is necessary in every assessment, and should be done in a structured way to minimise the chance of overlooking relevant uncertainties. In assessments that follow standardised procedures, it is only necessary to identify non-standard uncertainties.\n\nSource of uncertainty\nAn individual contribution to uncertainty\n\n\nAlways necessary\nThis is necessary in every assessment, and should be done in a structured way to minimise the chance of overlooking relevant uncertainties.\n\n\nGeneric lists\nIt is useful to develop generic lists of standard and non-standard uncertainties.\n\n\nStandardised procedures\nIn assessments that follow standardised procedures, it is only necessary to identify non-standard uncertainties.\n\n\n\n\n\n\n\n\n\n\nPrioritising uncertainties within the assessment\n\n\n\n\n\nPlays an important role in the planning the uncertainty analysis, enabling the assessor to focus detailed analysis on the most important uncertainties and address others collectively when evaluating overall uncertainty. Often prioritisation will be done by expert judgement during the planning process, but in more complex assessments it may be done explicitly, using influence or sensitivity analysis.\n\n\n\n\n\n\n\n\n\nDividing the uncertainty analysis into parts (when appropriate)\n\n\n\n\n\nIn some assessments, it may be sufficient to characterise overall uncertainty for the whole assessment directly, by expert judgement. In other cases, it may be preferable to evaluate uncertainty for some or all parts of the assessment separately and then combine them, either by calculation or expert judgement.\n\n\n\n\n\n\n\n\n\nEnsuring the questions or quantities of interest are well-defined\n\n\n\n\n\nThis is necessary in every assessment. Some assessments follow standardised procedures, within which the questions and/or quantities of interest should be predefined. In other assessments, the assessors will need to identify and define the questions and/or quantities of interest case by case.\n\n\n\n\n\n\n\n\n\nCharacterising uncertainty for parts of the uncertainty analysis\n\n\n\n\n\nThis is needed for assessments where the assessors choose to divide the uncertainty analysis into parts, but may only be done for some of the parts, with the other parts being considered when characterising overall uncertainty.\n\n\n\n\n\n\n\n\n\nCombining uncertainty from different parts of the uncertainty analysis\n\n\n\n\n\nThis is needed for assessments where the assessors quantify uncertainty separately for two or more parts of the uncertainty analysis.\n\n\n\n\n\n\n\n\n\nCharacterising overall uncertainty\n\n\n\n\n\nExpressing quantitatively the overall impact of as many as possible of the identified uncertainties, and describing qualitatively any that remain unquantified. This is necessary in all assessments except standardised assessments where no non-standard uncertainties are identified.\n\n\n\n\n\n\n\n\n\nPrioritising uncertainties for future investigation\n\n\n\n\n\nThis is implicit or explicit in any assessment where recommendations are made for future data collection or research, and may be informed by influence or sensitivity analysis.\n\n\n\n\n\n\n\n\n\nReporting uncertainty analysis\n\n\n\n\n\nRequired for all assessments, but extremely brief in standardised assessments where no non-standard uncertainties are identified."
  },
  {
    "objectID": "principles-and-methods/methods-sources-inputs.html",
    "href": "principles-and-methods/methods-sources-inputs.html",
    "title": "Questions to identify sources of uncertainty affecting inputs",
    "section": "",
    "text": "General types of uncertainty affecting inputs to scientific assessment, together with questions that may help to identify them in specific assessments\n\n\n\n\n\n\n1. Ambiguity\n\n\n\n\n\nAre all necessary aspects of any data, evidence, assumptions or scenarios used in the assessment (including the quantities measured, the subjects or objects on which measurements are made, and the time and location of measurements) adequately described, or are multiple interpretations possible?\n\n\n\n\n\n\n\n\n\n2. Accuracy and precision of the measures\n\n\n\n\n\nHow accurate and precise are methods/tools used to measure data (e.g. analytical methods, questionnaire). How adequate are any data quality assurance procedures and data validation that were followed?\n\n\n\n\n\n\n\n\n\n3. Sampling uncertainty\n\n\n\n\n\nIs the input based on measurements or observations on a sample from a larger population? If yes: How was the sample collected? Was stratification needed or applied? Was the sampling biased in any way, e.g. by intentional or unintentional targeting of sampling? How large was the sample? How does this affect the uncertainty of the estimates used in the assessment?\n\n\n\n\n\n\n\n\n\n4. Missing data within studies\n\n\n\n\n\nWhat is the frequency of missing data within the studies that are available? Is the mechanism causing the missing data random, or may it have introduced bias or imbalance among experimental groups (if any)? Was imputation of missing data performed, and did it use sound methodologies?\n\n\n\n\n\n\n\n\n\n5. Missing studies\n\n\n\n\n\nIs all the evidence needed to answer the assessment question available? Are the published studies reflecting all the available evidence? Where required studies are specified in guidance or legislation, are they all provided?\n\n\n\n\n\n\n\n\n\n6. Assumptions about inputs\n\n\n\n\n\nIs the input partly or wholly based on assumptions, such standard scenarios or default values? If so, what is the nature, quantity, relevance, reliability and quality of data or evidence available to support those assumptions?\n\n\n\n\n\n\n\n\n\n7. Statistical estimates\n\n\n\n\n\nDoes the input include a statistical measure of uncertainty (e.g. confidence interval)? If so, what uncertainties does this quantify, and what other uncertainties need to be considered? Is the statistical analysis used to produce the evidence appropriate and adequate? Are the implicit and explicit assumptions done in the statistical analysis expected to influence the results.\n\n\n\n\n\n\n\n\n\n8. Extrapolation uncertainty (e.g. limitations in external validity)\n\n\n\n\n\nAre any data, evidence, assumptions and scenarios used in the assessment (including the quantities they address, and the subjects or objects, time and location to which that quantity refers) directly relevant to what is needed for the assessment, or is some extrapolation or read across required? If the input is based on measurements or observations on a sample from a population, how closely relevant is the sampled population to the population or subpopulation of interest for the assessment? Is some extrapolation implied?\n\n\n\n\n\n\n\n\n\n9. Other uncertainties\n\n\n\n\n\nIs the input affected by any other sources of uncertainty that you can identify, or other reasons why the input might differ from the real quantity or effect it represents?"
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-principles.html",
    "href": "principles-and-methods/uncertainty-analysis-key-principles.html",
    "title": "Key principles",
    "section": "",
    "text": "Read more about different types of assessment.\n\nKey principles when conducting case-specific assessments\n\n\n\n\n\n\n\nWhat is a case-specific assessment?\n\n\n\nA case-specific assessment is needed when there is no standardised procedure for the type of assessment in hand, and when parts of the assessment use standardised procedure but other parts are case-specific or deviate from the standardised procedure (e.g. for refinement or urgency), and for calibrating standardised procedures when they are first established or revised.\n\n\n\nThe uncertainty analysis should start at a level that is appropriate to the assessment in hand. For assessments where data to quantify uncertainty is available and/or where suitable quantitative methods are already established, this may be included in the initial assessment. In other assessments, it may be best to start with a simple approach, unless it is evident at the outset that more complex approaches are needed.\nUncertainty analysis should be refined as far as is needed to inform decision-making. – This point is reached either when there is sufficient certainty about the question or quantity of interest for the decision-makers to make a decision with the level of certainty they require, or – if it becomes apparent that achieving the desired level of uncertainty is unfeasible or too costly and the decision-makers decide instead to manage the uncertainty without further refinement of the analysis.\nRefinements of the uncertainty analysis using more complex or resource-intensive methods and options should be targeted on those sources of uncertainty where they will contribute most efficiently to improving the characterisation of uncertainty, taking account of their influence on the assessment conclusion and the cost and feasibility of the refinement.\nThe characterisation of overall uncertainty must integrate the contributions of identified sources of uncertainties that have been expressed in different ways.\n\n\n\nKey principles when using standardised procedures\n\n\n\n\n\n\n\nWhat is a standardised assessment procedure?\n\n\n\nStandardised assessment procedures with accepted provision for uncertainty are common in many areas of EFSA’s work and are subject to periodic review.\nMost standardised procedures involve deterministic calculations using a combination of standard study data, default assessment factors and default values.\n\n\nUsing a standardised procedure can greatly simplify uncertainty analysis in routine assessments.\nThe documentation or guidance for a standardised procedure should specify\n\nthe question or quantity of interest,\nthe standardised elements of the procedure,\nthe type and quality of case-specific data to be provided and\nthe generic sources of uncertainty considered when calibrating the level of conservatism.\n\nIt is the responsibility of assessors to check the applicability of all these elements to each new assessment and check for any non-standard sources of uncertainty relevant to the question under assessment.\nAny deviations that would increase the uncertainties considered in the calibration or introduce additional sources of uncertainty, will mean that it cannot be assumed that the calibrated level of conservatism and certainty will be achieved for that assessment.\nTherefore, assessors should check for non-standard uncertainties in every assessment using a standardised procedure.\n\nIn assessments where none are identified, it is sufficient to record that a check was made and none were found. When non-standard uncertainties are present, a simple evaluation of their impact may be sufficient for decision-making, depending on how much scope was left for non-standard uncertainties when calibrating the standardised procedure.\nWhere the non-standard uncertainties are substantial or the standardised assessment procedure is not applicable, the assessors may need to carry out a case-specific assessment and uncertainty analysis.\n\n\n\nKey principles for development or review of a standardised procedure\n\nThe level of conservatism provided by each standardised procedure should be assessed by an appropriate uncertainty analysis.\n\nThis is to ensure they provide an appropriate degree of coverage for the sources of uncertainty that are generally associated with the class of assessments to which they apply.\n\nThe uncertainty analysis for the development or review of a standardised procedure\n\nis conducted as for a case-specific assessment to evaluate the probability of meeting the defined requirements, but where the procedure is adjusted if necessary to achieve an appropriate level of probability.\nThis requires defining the management objective for the procedure, and how often and/or to what extent that objective should be achieved in the future standardised assessments where the procedure will be used.\nThe uncertainty analysis should consider all relevant uncertainties, including uncertainties about how the standard study designs used to generate data, and any default factors, assumptions, scenarios and calculations used in the assessment, relate to conditions and processes in the real world.\nConsultation with decision-makers will be required to confirm that the level of conservatism is appropriate.\n\n\n\nKey principles for urgent assessments\n\n\n\n\n\n\n\nWhat is an urgent assessment?\n\n\n\nA scientific assessment requested to be completed within an unusually short period of time.\n\n\nIn some situations, e.g. emergencies, EFSA may be required to provide an urgent assessment in very limited time and the approach taken must be adapted accordingly.\n\nUncertainty is generally increased in such situations, and may be a major driver for decision-making.\nCharacterisation of uncertainty is therefore still necessary, despite the urgency of the assessment.\nThe approach to providing it must be scaled to fit within the time and resources available.\n\nEvery uncertainty analysis should express in quantitative terms the combined effect of as many as possible of the identified sources of uncertainty affecting each assessment.\n\nWhen time is severely limited, this may have to be done by a streamlined expert judgement procedure in which the contributions of all identified sources of uncertainty are evaluated and combined collectively, without dividing the uncertainty analysis into parts.\nThis initial assessment may need to be followed by more refined assessment and uncertainty analysis, including more detailed consideration of the most important sources of uncertainty, after the initial assessment has been delivered to decision-makers."
  },
  {
    "objectID": "principles-and-methods/uncertainty-communication-principles.html",
    "href": "principles-and-methods/uncertainty-communication-principles.html",
    "title": "Principles of Uncertainty Communication",
    "section": "",
    "text": "Communicating uncertainty aims to increase the transparency of the scientific assessment process and to provide the risk managers with a more informed evidence base by reporting on the strengths and weaknesses of the evidence.\nTransparency is one of EFSA’s basic principles and has important implications for risk communication. A transparent approach to explaining how an organisation works, its governance and how it makes its decisions, is intended to build trust. Communications should clearly convey the most important areas of uncertainty in the scientific assessment, whether and how these can be addressed by the assessors and decision-makers, and the implications of these remaining uncertainties for public health. Messages about uncertainty need to be based on information that is provided in EFSA’s scientific assessments. EFSA’s communications should be designed to ensure the results of its work (including its assessment of risk and uncertainty) are correctly understood by its audience, which is the objective of the Communication of Uncertainty GD.\n\n\nTypes of assessment\nAssessments must clearly identify sources of uncertainty and characterise their impact on the conclusion. The assessors should try to express the expert’s uncertainty in the conclusion, the overall uncertainty, quantitatively using %probability or a probability distribution. In reporting an uncertainty analysis, the Uncertainty Analysis GD describes some differences depending on the type of assessment. For standardised assessments it should mention the standard procedure was followed and a confirmation of the absence of non-standard uncertainties. In standardised assessments with non-standard uncertainties, the reporting should include how these were considered in the assessment. For other assessments, the uncertainty analysis should be reported with consideration for time constraints in urgent assessments. Depending on the type of assessment, decision-makers or legislation may require specific reporting formats, such as qualitative descriptors or unqualified conclusions. In cases where practical certainty cannot be achieved, assessors should report inconclusive findings and comply with documentation requirements.\n\n\nOverall uncertainty\nA concise summary of the overall uncertainty, suitable for the executive summary, should provide a simplified quantitative expression of the combined effect of identified sources of uncertainty. Assessors should ensure compatibility between the reporting of uncertainty analysis and the assessment conclusions, revising both if necessary.\nIn some assessments, information on the main contributors to uncertainty may be useful for decision-makers, informing further actions such as data gathering and refinement of the assessment.\n\n\nUnquantified uncertainties\nThe assessment conclusion should include a clear statement of quantified uncertainty and, if applicable, a description of any unquantified uncertainties. The latter should be described in terms of their nature, cause, effects, difficulty in quantification, assumptions made, and suggestions for reduction or better characterisation. When there are unquantified uncertainties, words implying magnitude or likelihood should be avoided."
  },
  {
    "objectID": "principles-and-methods.html",
    "href": "principles-and-methods.html",
    "title": "Principles and methods behind the guidances",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nWelcome to the Principles and Methods page\nHere, you can explore the principles behind EFSAs approach to uncertainty analysis and communication of uncertainty.\nYou can find the reasons why analysing and communicating uncertainty is important and learn about the main elements of uncertainty analysis.\nThe principles described on this page are primarily coming from the scientific opinion supporting the Uncertainty analysis Guidance, as well as the Section 3 in the Communication Guidance.\nOn the left hand side you can see the side menu with links to sections where you can learn more about underlying concepts associated with uncertainty and methods for uncertainty analysis. Look around and make yourself at home on the principles and methods page, because you might come back here after exploring the rest of the tutorial for some in-depth insight!\nThe section on methods might be useful, when you want to find an appropriate method to support an uncertainty analysis.\nThe sections on concepts and principles might be useful as a reminder on why and how to perform a quality uncertainty analysis.\nNote, that the text in this tutorial is a condensed extract from the two EFSA guidances and the supporting opinion. If something is unclear or if you want to learn more, we recommend you to consult the original Guidances. Always use the original documents as your primary source.\nEnjoy the exploration!\n\n\n\n\n\n\n\n\n\nLearn more about\n\n\n\nPrinciples of Uncertainty Analysis:\n\nGeneral background, introducing key definitions, roles, information required for decision making, constraints and expressions of uncertainty.\nUncertainty expressions, both quantitative and qualitative.\nCommon concerns and objections to quantitative expression of uncertainty and how they are addressed by the approach developed in this document and the accompanying Guidance.\nKey concepts, including uncertainty/variability, model uncertainty, overall uncertainty and expert judgement.\nMain elements of uncertainty analysis.\nKey principles for conducting different types of assessments\n\nMethods for uncertainty analysis\n\nOverview of methods for Uncertainty Analysis\nQualitative methods\nMethods to quantify uncertainty\nQuestions to identify sources of uncertainty affecting inputs\nQuestions to identify sources of uncertainty affecting assessment methodology\n\nPrinciples of Uncertainty Communication:\n\nPrinciples of Uncertainty Communication.\nCommunicating Scientific Uncertainties, covering risk communication mandate, risk perception and the uncertainty communicaiton challenges."
  },
  {
    "objectID": "uncertainty-communication/expressions-precise.html",
    "href": "uncertainty-communication/expressions-precise.html",
    "title": "Precise probability",
    "section": "",
    "text": "A single number (in EFSA outputs: a percentage between 0% and 100%) quantifying the likelihood of either:\n\nA specified answer to a question (e.g. a ‘yes’ answer to a ‘yes/no’ question)\nA specified quantity lying in a specified range of values, or above or below a specified value (e.g. 90% probability that between 10 and 100 infected organisms will enter the EU in 2019; 5% probability that more than 100 infected organisms will enter).\n\n\n\n\n\n\n\nNote\n\n\n\nThe term ‘precise’ is used here to refer to how the probability is expressed, as a single number, and does not imply that it is actually known with absolute precision, which is not possible."
  },
  {
    "objectID": "uncertainty-communication/expressions-precise.html#what-is-a-probability",
    "href": "uncertainty-communication/expressions-precise.html#what-is-a-probability",
    "title": "Precise probability",
    "section": "",
    "text": "A single number (in EFSA outputs: a percentage between 0% and 100%) quantifying the likelihood of either:\n\nA specified answer to a question (e.g. a ‘yes’ answer to a ‘yes/no’ question)\nA specified quantity lying in a specified range of values, or above or below a specified value (e.g. 90% probability that between 10 and 100 infected organisms will enter the EU in 2019; 5% probability that more than 100 infected organisms will enter).\n\n\n\n\n\n\n\nNote\n\n\n\nThe term ‘precise’ is used here to refer to how the probability is expressed, as a single number, and does not imply that it is actually known with absolute precision, which is not possible."
  },
  {
    "objectID": "uncertainty-communication/expressions-precise.html#frequentist-vs-bayesian-view",
    "href": "uncertainty-communication/expressions-precise.html#frequentist-vs-bayesian-view",
    "title": "Precise probability",
    "section": "Frequentist vs Bayesian view",
    "text": "Frequentist vs Bayesian view\nThere are two major perspectives on the scope of probability as a measure for quantifying uncertainty.\nThe first perspective, known as the frequentist view, argues that probability should be limited to addressing uncertainties caused by variability and should not be extended to uncertainties arising from limitations in knowledge. In other words, frequentists do not consider probability as a relevant concept for measuring uncertainty associated with imperfect knowledge. Consequently, this viewpoint does not offer a solution for characterizing the many other types of uncertainty, such as data quality and extrapolation, which are frequently significant in EFSA assessments.\nThe second perspective, known as the subjectivist (Bayesian) view, asserts that probability serves as a direct personal expression of uncertainty. According to this view, any well-defined question or quantity can have its uncertainty quantified using probability. Therefore, it can encompass uncertainties caused by limitations in knowledge, as well as those caused by variability.\n\nAdvantages of subjective probability\nSubjective probability enhances comparability among individuals when expressing uncertainties. It allows for intuitive comparisons based on shared understanding, such as coin tosses or dice rolls. An operational definition enables subjective probabilities for well-defined quantities or categorical questions. It also permits the legitimate application of mathematical tools to subjective probabilities, facilitating judgments on combining sources of uncertainty. The Guidance encourages the use of subjective probability to express uncertainty, except when quantification is particularly challenging (see Section 5.12 of the SO).\n\n\nCombining uncertainties\nThe subjectivist interpretation of probability does not exclude the frequentist interpretation, but it requires reinterpreting frequentist probabilities as subjective probabilities to combine them with other sources of uncertainty. This is particularly relevant when integrating probabilities from statistical data analysis with probabilities derived from expert judgment. Bayesian analysis already yields subjective probabilities, but confidence intervals from frequentist analysis would need reinterpretation. The process and appropriateness of such reinterpretation are discussed in Section 11.2.1, considering that probabilities from statistical analysis may still entail additional uncertainties requiring expert judgment.\nLearn more about probability distributions in the respective section."
  },
  {
    "objectID": "uncertainty-communication/expressions-precise.html#approximate-probability",
    "href": "uncertainty-communication/expressions-precise.html#approximate-probability",
    "title": "Precise probability",
    "section": "Approximate probability",
    "text": "Approximate probability\nProbabilities need not be expressed precisely, and in practice, they are always approximate to some extent. Assessors typically provide approximate probabilities within a specified range instead of specifying them to infinite decimal places. Learn more about approximate probabilities in the respective section of the tutorial."
  },
  {
    "objectID": "uncertainty-communication/assessor.html",
    "href": "uncertainty-communication/assessor.html",
    "title": "for Assessors",
    "section": "",
    "text": "Checklist for identifying messages with associated uncertainty expressions, and specific guidance for their communication:\n\n\n\n\n\n\n\n1. Did the scientific assessment for this message identify any non-standard uncertainties?\n\n\n\n\n\n\nYesNo\n\n\nIf an unqualified conclusion is required, follow the guidance for unqualified conclusions (see Question 5 here). If an unqualified conclusion is not required, state the result of the standardised procedure in the form expressed by the assessors. Also communicate the uncertainty expressions for this message, consulting the respective guidance boxes.\n\n\nReport the conclusion as expressed by assessors and state that a standardised assessment procedure was followed that takes account of standard uncertainties, and no non-standard uncertainties were identified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Does the scientific output list or describe the sources or causes of any uncertainties affecting this message?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nState that uncertainties exist, using the wording in the scientific output.\nInclude in the message a brief description of the sources of uncertainty that have the biggest impact on the respective key messages. (If necessary, consult the assessors to identify these.)\n\n\nExample: “The experts identified limitations in the data on exposure and toxic effects of ZEN and its modified forms, for example (…)” Based on EFSA 2017;15(7):4851\n\n\n\n\nWhen documenting sources of uncertainty in the assessment report, assessors should include brief text descriptions suitable for subsequent use in communications to informed audiences without using specialist technical terms. - Assessors should try to identify which sources of uncertainty have most influence on their conclusions, either by qualitative assessment or by influence or sensitivity analysis UA.\nWhere there is conflicting evidence on an issue, this is a source of uncertainty which must be documented and taken into account in uncertainty analysis, and may be assessed using a weight of evidence approach WE.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Is the direction and/or degree of uncertainty expressed verbally (i.e. with terms like ‘low’, ‘medium’, ‘high’) or with symbols (e.g. ‘+’ or ‘-’)?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nAvoid altering the wordings used by assessors to describe the direction and/or degree of uncertainty, or factors contributing to uncertainty (Box 2). Always check the rewording with the assessors if you do.\nState clearly what outcomes and conditions this expression of uncertainty refers to.\nMake clear that any uncertainty referred to in the communication has been taken into account in the assessment conclusion.\nBefore communicating the uncertainty expression, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment.\n\nOptionally, mention specific methods that were used in evaluating the uncertainty.\n\nOptionally, mention factors contributing to the overall uncertainty, including the relative importance of individual sources of uncertainty and things like the relevance and reliability of evidence (e.g. in weight of evidence assessments, see WE).\n\nClearly distinguish individual sources of uncertainty from overall uncertainty about the assessment conclusions.\n\n\nExample: “The Panel noted that a high proportion of measurements of ZEN and its modified forms in feed were below the limit of detection, leading to very high uncertainty when estimating exposure” Based on EFSA 2017;15(7):4851\n\n\n\n\nIf using “+” and “−” or other symbols to indicate the direction and magnitude of uncertainty, accompany these with quantitative definitions of their meaning, as discussed in SO, Annex B5.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. Does this message report an inconclusive assessment outcome?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nDescribe the main sources of uncertainty in more detail, but concisely, following the guidance in Question 2 above.\nInconclusive assessments are especially likely to include options or requirements for obtaining further data. Communicate these as instructed in UC Section 3.1.6.\n\n\nExample: “EFSA’s experts could not reach a conclusion on the risk for cattle, ducks, goats, horses, rabbits, mink and cats due to limitations in available data on exposure and toxic effects of ZEN and itsmodified forms, for example (…)” Based on EFSA 2017;15(7):4851\n\n\n\n\nWhen explaining why the assessment is inconclusive, include a description of the key sources of uncertainty that are responsible for this.\n\nIf the assessment is not totally uncertain, try to express what the science can say and quantify the uncertainty unless the risk manager/legislation requires that only unqualified conclusions be given.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. Do risk managers expect or does legislation require that this message should be expressed as an unqualified conclusion, without any expression of uncertainty?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nReport the unqualified conclusion for this message using the same wording as the assessors.\nOptionally, describe briefly how the assessment was made (i.e. what evidence and methods were used to arrive at the conclusions).\nBriefly describe some examples of uncertainties affecting the assessment for this message, as identified in your completed template, consulting Box 4 for guidance on how to communicate this.\nIf the assessment contains any verbal or numerical expression of the impact of the uncertainties as identified in your template, follow the respective guidance in Boxes 6–9 below.\nSay that the assessors took the uncertainties into account when reaching their conclusion(s) for this message.\n\n\nExample: “Following the standard assessment procedure (or ‘Using the evaluation system agreed for contaminants in feed’), experts estimated that high exposure to feed containing ZEN is below the reference value for a health risk for sheep, dog, pig and fish, and well below the reference value for chicken and turkeys. They therefore concluded that the exposure to feed containing ZEN ‘in farm situations’ is a low health risk for sheep,dog,pig and fish,and an extremely low health risk for poultry. In reaching this conclusion, the experts took account of limitations in the data on exposure and toxic effects of ZEN and its modified forms, for example (…)” Based on EFSA 2017;15(7):4851\n\n\n\n\nProvide the information needed for the FAQ required at the entry level communications (see above).\nSpecify what level of certainty is associated with each unqualified conclusion. Risk managers can explain why that level of certainty is appropriate for decisionmaking, if considered necessary. Make this information available to interested parties in suitable ways, e.g. in an FAQ and/or in documentation or guidance on the assessment methodology.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6. Is the uncertainty described with numbers as a precise probability?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nState clearly what the probability refers to, including whether it refers to a numerical estimate or a qualitative conclusion. When the probability refers to a numerical estimate, also state the range of the quantity that the probability refers to (see example below).\nBefore giving the probability, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account when assessing their level of certainty.\nOptionally, mention specific methods that were used in quantifying the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\n\n\nExample: “The Panel performed its assessment using a mathematical model of the entry of nematodes into the EU and their establishment and spread in greenhouse tulips. Uncertainty on the factors represented in the model was quantified by expert judgement, taking into account the limitations of the available data. The Panel estimates…[continue as for entry level]” Based on EFSA 2017;15(8):4879\n\n\n\n\nNo specific guidance for assessors other than the general guidance for assessors in UC Section 3.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7. Is the uncertainty described with numbers as an approximate probability?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nState clearly what the probability refers to, including whether it refers to a numerical estimate or a qualitative conclusion. When the probability refers to a numerical estimate, also state the range of the quantity that the probability refers to.\nAn approximate probability may comprise a range of probabilities chosen by the assessors from the approximate probability scale, or a different range of probabilities specified by the assessors.\nAlways communicate the quantitative range of probabilities because this expresses the assessors’ conclusion without ambiguity. If a verbal expression is also used, present the quantitative probability first (e.g. ‘66–90% certain (likely)’) because it has been shown that this order leads to more consistent understanding than if the verbal expression is presented first (see UC Section 3.1)\nTo avoid inconsistency and misunderstanding, do not use the verbal terms in Table 1 to refer to any probabilities or ranges of probabilities other than those shown in this table.\nBefore giving the probability, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account when assessing their level of certainty.\nOptionally, mention specific methods that were used in quantifying the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\n\n\nExample: “Experts began work on the Scientific Opinion after the 2015 EU summary report on foodborne zoonotic diseases identified an increasing trend of listeriosis over the period 2009–2013. The Panel performed a statistical analysis, which confirmed the increasing trend, and developed a mathematical model of the factors influencing the incidence of infections. Considering the modelling results and the degree of support from indicator data, the experts…’ [continue as for entry level]” Based on EFSA 2018;16(1):5134\n\n\n\n\nUse different probabilities or ranges from those shown in Table 1 if they better express your judgement UA. In such cases, avoid accompanying it with any verbal probability expression because a harmonised interpretation exists only for the terms in Table 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8. Is the uncertainty described with numbers as a one-dimensional probability distribution?\n\n\n\n\n\n\nInformedTechnical\n\n\nAs for the entry level with the following differences:\n\nBefore giving the results, describe examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account in their assessment.\n\nOptionally, mention specific methods that were used to quantify the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\n\nConsider providing a visual representation of the uncertainty if this is expected to be useful for understanding, e.g. to aid understanding of the distribution of probability around the central estimate. Use a box plot for this at the informed level. (Do not use graphical representations of full distributions, such as PDF or CDF, in communications for non-technical audiences because these are easily misunderstood.) When using a box plot, explain clearly that it represents the uncertainty, as they are commonly used to represent variability and people may misinterpret them in that way. When including a graphic, provide this in addition to the entry level representation (see above) and not instead of it.\n\nLabel the graphs (including axes, legends, and units) appropriately so that, as far as possible, people will understand them on their own. Accompany every visual with sufficient textual explanation that the informed-level audience will understand it.\n\n\nExample: “Ten experts from different EU countries each surveyed a sample of slaughterhouses in their country to gather information on the prevalence of animals being pregnant at slaughter in 2015. Six of those experts used the survey results and other available evidence to estimate the average prevalence in Europe for different species in 2015, taking account of the uncertainties involved. The Panel’s conclusions are based on the results. Experts estimated… [continue as in entry-level communication]” Based on EFSA 2017;15(5):4782\n\nWhen communicating using box or box-and-whisker plots:\n\nAlways accompany box plots with explanations of each element they contain (including central estimate, box and whiskers).\nIf there is a value or quantity that is of particular interest to risk managers or the public (e.g. a regulatory/reference value), communicate the probability of the true value being above or below this (depending which is of interest) alongside the box plot, following the specific guidance in Question 6.\n\n\n\n\n\n\nFigure 1: Example of a box-and-whiskers plot\n\n\n\n\n\nExample: “Figure 1 is a box plot summarising the combined judgement of six experts about the average number of pregnant dairy cows per 100 cows slaughtered in the EU in 2015. The horizontal line inside the box is the median estimate: the true average value is considered equally likely to be above or below this estimate. The box and whiskers represent the experts’ collective uncertainty about the EU average in 2015 (not variation between samples of animals). There is 50% certainty that the true average is in the box and 98% certainty it is between the whiskers. There is still a 2% chance that the true average is outside the whiskers” Based on EFSA 2017;15(5):4782\n\n\n\n\nInclude a table of the distribution and a box plot.\nProvide P5, P25, P50, P75 and P95 ranges. In addition, if there are values of specific interest to the public/risk managers (e.g. a reference dose/value), then provide the probability for the true value being above or below this (depending which is of interest, e.g. a health concern).\nState which sources of uncertainty are considered in the distribution and provide a qualitative or quantitative description of uncertainties not considered in the distribution (e.g. uncertainty about the quality and representativeness of entry data, assumptions in modelling exposure or assumptions about the distribution of different parameters of a model).\nState clearly how each distribution was obtained, and in particular whether it was derived by statistical analysis, mechanistic modelling, expert judgement or a combination of these.\n\nOptional:\n\nProvide a PDF graph of the distribution if communicating only the quantiles would fail to indicate something important about the distribution, e.g. bimodal, skewed.\nAccompany the PDF with a CDF graph of the distribution if this is useful for technical readers of the assessment (e.g. to enable them to read off approximate estimates for quantiles other than those reported explicitly). Using the same horizontal scale, plot the CDF above the PDF and clearly mark the location of the central estimate (and optionally the P5, P25, P75 and P95) on both curves.\nAccompany a PDF or CDF with an explanatory text expressed in the simplest terms possible. If there is a value or quantity that is of particular interest to risk managers or the public (e.g. a regulatory/reference value), explicitly mark it on both curves. Explain clearly that the distribution represents uncertainty about the quantity of interest, for which there is a single true value, as distributions are more commonly used to represent variability and people may misinterpret them in that way.\n\n\n\n\n\n\nFigure 2: Example of a PDF graph\n\n\n\n\n\nExample: “The red line in Figure 2 shows a probability distribution quantifying uncertainty about how many dairy cows out of a hundred on average are pregnant when slaughtered in the EU in 2015, i.e. the prevalence of being pregnant when slaughtered. The height of the red curve shows the relative likelihood of the prevalence values in each part of the horizontal axis. The central (median) estimate is 16 out of 100, with 50% certainty that the European average for 2015 is between 9 and 27, and 98% certainty it is between 2 and 60 (as shown on the graph)”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9. Is the uncertainty described with numbers as a two-dimensional probability distribution?\n\n\n\n\n\n\nInformedTechnical\n\n\n\nBefore giving the results, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account in their assessment.\nOptionally, mention specific methods that were used in quantifying the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\nProvide a visual representation of the uncertainty if possible, especially if the uncertainty information is part of the key messages. Use a box plot for this showing median, P5, P25, P75 and P95 for the specific result(s) selected from the 2D distribution. Ask the scientific officer to provide the box plot for the selected result, and communicate it as indicated in Quesion 8.\nDo not use graphical representations of full 2D distributions in entry or informed level communications as many people misunderstand them.\n\n\nExample: “The Panel developed a mathematical model of the exposure of toddlers to carbendazim in apples and apple products. They used UK data on the occurrence of carbendazim in apples and on consumption of apples and apple products by toddlers. The model computed different eating patterns among toddlers by simulating apple consumption and carbendazim intake for 10,000 ‘toddler-days’ (10,000 random samples from a survey of daily apple consumption by UK toddlers, including records where no apples or apple products were consumed). The simulation also calculated five types of uncertainty affecting the model, e.g. limited measurements of occurrence and consumption, limitations in the precision of the occurrence data. Using this model, the Panel estimated that the number of ‘toddler-days’ in which more than the safe level for carbendazim is ingested from apples and apple products is 10 per 10,000 toddler-days. However, this takes account of only the five sources of uncertainty that were quantified: other uncertainties were taken into account separately by expert judgement” Based on EFSA 2007;5(8):538\nNote: after the above text provide information on the experts’ judgement about other uncertainties which the model did not quantify, then go back to to this Questionnaire to identify the type of expression used and locate the corresponding specific guidance for communication.\n\n\n\n\nProvide a box plot and/or relevant table containing quantiles for selected results from the distribution.\nProvide a description of the relative magnitude of variability and uncertainty.\nConsider including a graphical representation of the 2D distribution in the technical-level communication (EFSA scientific output). When this is carried out, proceed as follows:\n\nUse a 2D CDF rather than a 2D CCDF unless there are strong reasons for using the latter, as there is some evidence that CCDFs are less well understood.\nEnsure the graph is well formatted, with clear labelling of axes and values, and markers to show the specific results that were selected for use in the entry-level communication (see above). If helpful for communication, also request a box plot for the selected result and display it alongside the 2D graph.\nProvide sufficient textual explanation for the 2D graph so the intended audience will understand it.\nExplain that the 2D graph represents both variability and uncertainty for the quantity of interest. Consider whether it is helpful to include accompanying text on the relative magnitudes of variability and uncertainty and, if so, request this from the scientific officer.\n\n\n\n\n\n\n\nFigure 3: Example of graphical representation of 2D distribution\n\n\n\n\n\nExample: “A graph showing the results of the model for the 2D probability assessment is presented in Figure 3. Thehorizontalaxisis the amount of carbendazim from apples and/or apple products ingested per toddler-day : the vertical solid red line shows the safe level for this. The vertical axis is the number of ‘toddler-days’ per 10,000 on which carbendazim intake exceeded any given level on the horizontal axis. The median estimate for this is shown by the solid curve: each point on the curve shows the number of toddlerdays (on the vertical axis) on which ingestion of carbendazim exceeded the level shown below that point on the horizontal axis. The dashed curves show the upper and lower 95% confidence interval for the estimates: i.e. there is 95% certainty that any selected estimate lies between the dashed curves. However, this takes into account only the five sources of uncertainty that were quantified: other uncertainties were considered separately by expert judgement. From the results shown in the graph, the Panel estimated that the number of toddler-daysonwhichmorethanthesafelevelforcarbendazimisingested from apples and apple products is 10 per 10,000, and is 95% certain the number is between 1 and 40 per 10,000. These results are indicated by the horizontal dotted lines shown in the graph’.” Based on EFSA 2007;5(8):538\nNote: after the second paragraph above, provide information on the experts’ judgement about the other uncertainties, go back to this Questionnaire to identify the type of expression used and locate the corresponding specific guidance for communication. Note: EFSA outputs rarely use 2D graphs. The graphical example in Figure 3 is unusual because the vertical axis is inverted. Also, the example contains a second distribution (shown in grey) that is not referred to in the text above. Take care to use a format that facilitates understanding of the key results."
  },
  {
    "objectID": "uncertainty-communication/expressions-sources.html",
    "href": "uncertainty-communication/expressions-sources.html",
    "title": "Sources of uncertainty",
    "section": "",
    "text": "Verbal description of a source or cause of uncertainty. In some areas of EFSA’s work, there are standard terminologies for describing some types of uncertainties, but often descriptions are specific to the assessment in hand (see SO)\n\nVideoText\n\n\n\n\n\n\nUncertainty analysis is part of scientific assessment, so in all cases, it should be reported in a manner consistent with EFSA’s general principles regarding transparency and reporting.\nThe simplest uncertainty analysis consists of two steps. The first is to identify sources of uncertainty. The second step is to evaluate their combined impact on the answer to the assessment question (SO 3.1).\nIn particular, it is important to list the sources of uncertainty that have been identified.\nSources of uncertainty (or uncertainties) are associated with inputs to an assessment (such as data or estimates) as well as the methodology used to process this input to reach a conclusion (such as statistical models, calculations or expert judgment).\nCheck with your EFSA Panel for a list of sources of uncertainty which are encountered regularly in their work.\nEnsure that the reporting of the assessment includes all sources of uncertainty that you have identified.\nHere it is important to distinguish between “additional uncertainties”, i.e. sources of uncertainty that are only taken into account at the last step for the assessment of overall uncertainty\nand\n“unquantified uncertainties”, i.e. identified sources of uncertainty that the assessors are unable to include, or choose not to include, in the assessment of overall uncertainty.\nIf you are doing a standard assessment, uncertainties are divided into standard and non-standard uncertainties. The standard uncertainties are reported in the guidance for the standard assessment. Pay extra attention to identifying and reporting any non-standard uncertainties, because they are required to be listed in the standardised assessments.\nOn this page you can find some suggestions for the sources of uncertainty, but the list is by no means exhaustive. The guidance document suggests that you might want to organise the sources of uncertainty in the table to make it easier for you and your readers to have an overview of them.\nSources of uncertainty can be integrated with tables for evidence, data or model outputs to support the characterisation of overall uncertainty."
  },
  {
    "objectID": "uncertainty-communication/expressions-sources.html#communicating-a-list-of-sources-of-uncertainty",
    "href": "uncertainty-communication/expressions-sources.html#communicating-a-list-of-sources-of-uncertainty",
    "title": "Sources of uncertainty",
    "section": "",
    "text": "Verbal description of a source or cause of uncertainty. In some areas of EFSA’s work, there are standard terminologies for describing some types of uncertainties, but often descriptions are specific to the assessment in hand (see SO)\n\nVideoText\n\n\n\n\n\n\nUncertainty analysis is part of scientific assessment, so in all cases, it should be reported in a manner consistent with EFSA’s general principles regarding transparency and reporting.\nThe simplest uncertainty analysis consists of two steps. The first is to identify sources of uncertainty. The second step is to evaluate their combined impact on the answer to the assessment question (SO 3.1).\nIn particular, it is important to list the sources of uncertainty that have been identified.\nSources of uncertainty (or uncertainties) are associated with inputs to an assessment (such as data or estimates) as well as the methodology used to process this input to reach a conclusion (such as statistical models, calculations or expert judgment).\nCheck with your EFSA Panel for a list of sources of uncertainty which are encountered regularly in their work.\nEnsure that the reporting of the assessment includes all sources of uncertainty that you have identified.\nHere it is important to distinguish between “additional uncertainties”, i.e. sources of uncertainty that are only taken into account at the last step for the assessment of overall uncertainty\nand\n“unquantified uncertainties”, i.e. identified sources of uncertainty that the assessors are unable to include, or choose not to include, in the assessment of overall uncertainty.\nIf you are doing a standard assessment, uncertainties are divided into standard and non-standard uncertainties. The standard uncertainties are reported in the guidance for the standard assessment. Pay extra attention to identifying and reporting any non-standard uncertainties, because they are required to be listed in the standardised assessments.\nOn this page you can find some suggestions for the sources of uncertainty, but the list is by no means exhaustive. The guidance document suggests that you might want to organise the sources of uncertainty in the table to make it easier for you and your readers to have an overview of them.\nSources of uncertainty can be integrated with tables for evidence, data or model outputs to support the characterisation of overall uncertainty."
  },
  {
    "objectID": "uncertainty-communication/expressions-sources.html#generic-list-of-common-types-of-uncertainty-affecting-scientific-assessments",
    "href": "uncertainty-communication/expressions-sources.html#generic-list-of-common-types-of-uncertainty-affecting-scientific-assessments",
    "title": "Sources of uncertainty",
    "section": "Generic list of common types of uncertainty affecting scientific assessments",
    "text": "Generic list of common types of uncertainty affecting scientific assessments\nUncertainties associated with:\n\nAssessment inputsAssessment methodology\n\n\n\nAmbiguity\nAccuracy and precision of the measures\nSampling uncertainty\nMissing data within studies\nMissing studies\nAssumptions about inputs\nStatistical estimates\nExtrapolation uncertainty (i.e. limitations in external validity)\nOther uncertainties\n\n\n\n\nAmbiguity\nExcluded factors\nDistributional assumptions\nUse of fixed values\nRelationship between parts of the assessment\nEvidence for the structure of the assessment\nUncertainties relating to the process for dealing with evidence from the literature\nExpert judgement\nCalibration or validation with independent data\nDependency between sources of uncertainty\nOther uncertainties"
  },
  {
    "objectID": "uncertainty-communication/expressions-distribution.html",
    "href": "uncertainty-communication/expressions-distribution.html",
    "title": "Probability distribution",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nBefore we go into a probability distribution. Let us revisit what is a probability.\nWhen you work with scientific assessment, you will encounter the two dominating ways to interpret a probability (SO 5.10.). The first is to interpret probability as “the frequency with which sampled values arise within a specified range or for a specified category”. An example of this interpretation is the average proportion of times you would get heads up when flipping a coin many times. This is also known as the frequency interpretation.\nThe second interpretation is to interpret probability as “a quantification of a judgement on the likelihood of a particular range or category”. An example of this interpretation is your uncertainty about getting a head up the next time you flip the coin. This is known as the subjective (or personal) probability interpretation.\nThe subjective (or personal) probability interpretation is used by EFSA to quantify the experts’ uncertainty.\nNote, that the mathematical theory for probability is the same for both interpretations.\nThe interpretation of probability matters when specifying models for scientific assessment and in particular when communicating conclusions.\nEFSA’s guidances uses subjective probability as the preferred measure for expressing uncertainty, because it can quantify the relative likelihood of alternative outcomes taking account of all sources of uncertainty, which is what decision-makers need to know."
  },
  {
    "objectID": "uncertainty-communication/expressions-distribution.html#the-interpretation-of-probability",
    "href": "uncertainty-communication/expressions-distribution.html#the-interpretation-of-probability",
    "title": "Probability distribution",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nBefore we go into a probability distribution. Let us revisit what is a probability.\nWhen you work with scientific assessment, you will encounter the two dominating ways to interpret a probability (SO 5.10.). The first is to interpret probability as “the frequency with which sampled values arise within a specified range or for a specified category”. An example of this interpretation is the average proportion of times you would get heads up when flipping a coin many times. This is also known as the frequency interpretation.\nThe second interpretation is to interpret probability as “a quantification of a judgement on the likelihood of a particular range or category”. An example of this interpretation is your uncertainty about getting a head up the next time you flip the coin. This is known as the subjective (or personal) probability interpretation.\nThe subjective (or personal) probability interpretation is used by EFSA to quantify the experts’ uncertainty.\nNote, that the mathematical theory for probability is the same for both interpretations.\nThe interpretation of probability matters when specifying models for scientific assessment and in particular when communicating conclusions.\nEFSA’s guidances uses subjective probability as the preferred measure for expressing uncertainty, because it can quantify the relative likelihood of alternative outcomes taking account of all sources of uncertainty, which is what decision-makers need to know."
  },
  {
    "objectID": "uncertainty-communication/expressions-distribution.html#probability-distribution",
    "href": "uncertainty-communication/expressions-distribution.html#probability-distribution",
    "title": "Probability distribution",
    "section": "Probability distribution",
    "text": "Probability distribution\n\nVideoText\n\n\n\n\n\n\nSo far, we have been talking about probability for a binary event, which is an event having only two outcomes. This is the simplest probability model and it can be applied to express uncertainty about a yes/no question.\nProbability is also used for quantities having “more than two possible values”.\nThis can be quantities that are categorical - with distinct classes that do not have to come in a particular order Alternatively, a quantity can be discrete - taking numerical integer values, usually obtained by counts A quantity can also be continuous - taking numerical continuous values for which probability is expressed over ranges instead of specific numbers\nThe probabilities for quantities taking different values are summarised by a probability distribution.\nThe probabilities in a probability distribution can be assigned a frequency or subjective probability interpretation.\nWhen used to quantify uncertainty, a subjective probability distribution quantifies the experts’ uncertainty regarding a quantity that has a single true value. Let me give you an example. It can be the number of steps in the staircase outside my room. The number is fixed, but I have not counted them recently, so I am uncertain about it.\nProbability distributions are useful for characterising uncertainty about non-variable quantities, for which there is a single true value, such as parameters within scientific models.\nA quantity can alternatively be a variable. The supporting opinion of the uncertainty analysis guidance defines a variable as “a quantity that has multiple true values”. This could for example be body weight measured in different individuals in a population, or in the same individual at different points in time.\nProbability distributions can also be used to model variable quantities in probabilistic models of variability.\nA scientific assessment can consist of probabilistic models for both uncertainty and variability."
  },
  {
    "objectID": "uncertainty-communication/expressions-distribution.html#uncertainty-and-variability",
    "href": "uncertainty-communication/expressions-distribution.html#uncertainty-and-variability",
    "title": "Probability distribution",
    "section": "Uncertainty and variability",
    "text": "Uncertainty and variability\n\nVideoText\n\n\n\n\n\n\nI think we are ready to say something about the distinction between uncertainty and variability.\nUncertainty refers to the state of knowledge, whereas variability refers to actual variation or heterogeneity in the real world. Both uncertainty and variability can be represented by probability distributions, as illustrated in the left and central graphs in Figure 1.\n\n\n\n\n\nFigure 1: Uncertainty and variability\n\n\n\n\nThe left you see a probability distribution for a non-variable quantity - Let us call it parameter P. It is a single true value which is uncertain, and the distribution represents uncertainty about P.\nIn the center, you see a probability distribution for a variable quantity. This variable V has multiple true values. The probability distribution represents variability of V.\nTo the right you see a probabilistic model for both variability and uncertainty. We use a probability distribution to represent variability in this variable, but the true distribution for variability is unknown and we use probability distributions to represent our uncertainty about it’s variability.\nUncertainty may be altered (either reduced or increased) by further research, because it results from limitations in knowledge.\nVariability cannot be altered by obtaining more knowledge, because it refers to real differences in the world or how the assessors choose to model the world.\nIt is important that assessors distinguish uncertainty and variability because they have different implications for decision-making: informing decisions about whether to invest resources in research aimed at reducing uncertainty or in management options aimed at influencing variability (e.g. to change exposures to subgroups of the population)."
  },
  {
    "objectID": "uncertainty-communication/expressions-distribution.html#communicating-uncertainty-about-a-quantity-of-interest-by-a-probability-distribution",
    "href": "uncertainty-communication/expressions-distribution.html#communicating-uncertainty-about-a-quantity-of-interest-by-a-probability-distribution",
    "title": "Probability distribution",
    "section": "Communicating uncertainty about a quantity of interest by a probability distribution",
    "text": "Communicating uncertainty about a quantity of interest by a probability distribution\nA graph showing probabilities for different values of an uncertain quantity that has a single true value (e.g. the average exposure for a population). The graph (Figure 2) can be plotted in various formats, most commonly a probability density function (PDF), cumulative distribution function (CDF) or complementary cumulative distribution function (CCDF) (see Section 4.1.4.2 in UC)\n\n\n\n\n\nFigure 2: CDF, PDF and boxplot"
  },
  {
    "objectID": "uncertainty-communication/expressions-distribution.html#common-terminology-for-a-probability-distribution",
    "href": "uncertainty-communication/expressions-distribution.html#common-terminology-for-a-probability-distribution",
    "title": "Probability distribution",
    "section": "Common terminology for a probability distribution",
    "text": "Common terminology for a probability distribution\n\nVideoText\n\n\n\n\n\n\nA probability distribution for a discrete or continuous quantity gives the probability that the quantity lies in any specified range of values.\nA probability distribution can be represented by its Cumulative Distribution Function (often abbreviated as CDF). The CDF gives the probability that the quantity ls less than or equal to any specified value (Figure 2).\nThe CDF contains all the information about probabilities for the quantity: for example, it can be used to calculate the probability that the quantity lies in any specified range of values.\nThis is a curve over the possible range of the quantity (on the x-axis) increasing from 0 to 100% probability (on the y-axis).\nTo summarise uncertainty, an assessor might want to find values that divide the range of the quantity into parts containing specified amounts of probability. Such values are known as quantiles (or percentiles).\nThe guidance denotes quantiles with the letter P, together with the associated probability that the quantity would take a value below the quantile. For example, the value that divides a probability distribution into two parts with equal probabilities is the P50 quantile.\nThis value is also known as the median.\nMost continuous quantities have a Probability Density Function.\nThe Probability Density Function express probabilities as area under its curve. The total area under the curve is 1, corresponding to 100% probability.\nThe area under the PDF curve to the left of the median is 50%.\nThe Probability Density Function can be thought of as a smooth histogram for a continuous quantity. Note that when used for a probability distribution, the area for the histogram should be 1."
  },
  {
    "objectID": "uncertainty-communication/expressions-distribution.html#guidance-on-communicating-a-probability-distribution",
    "href": "uncertainty-communication/expressions-distribution.html#guidance-on-communicating-a-probability-distribution",
    "title": "Probability distribution",
    "section": "Guidance on communicating a probability distribution",
    "text": "Guidance on communicating a probability distribution\n\nVideoText\n\n\n\n\n\n\nThe communication guidance offers specific recommendations on how to communicate a probability distribution representing uncertainty.\nAlways state clearly what the probability distribution refers to.\nGraphical representations of distributions are difficult to interpret for non-technical audiences. Therefore, for entry and informed levels, only communicate selected results extracted from the distribution. Provide the central estimate (mean or median), the P5–P95 range (within which it is 90% certain that the true value lies) and/or the P25–P75 range (within which it is 50% certain the true value lies) If the assessors have provided different quantiles (e.g. P1–P99), use these instead. If it is critical for understanding the message, also give an idea about the form of the distribution behind the range.\nIn addition, or alternatively: if a regulatory (reference) value exists, or a value of particular interest for other reasons (e.g. more than zero occurrence of an adverse outcome), provide the probability of exceeding that value.\nFor an informed audience, it is recommended to in addition provide information on how the probability distributions have been derived, underlying data or evidence and methods, and use boxplots to visualise the summaries from the distribution. Add labels and explanatory text.\nWhen using a box plot, explain clearly that it represents uncertainty and not variability.\nMore information should be provided for a technical audience: The assessors should include a table of the distribution (including P5, P25, P50, P75 and P95 ranges) and a box plot. In addition, if there are values of specific interest to the public/risk managers (e.g. a reference dose/value), then provide the probability for the true value being above or below this. State which sources of uncertainty are considered in the distribution and provide a qualitative or quantitative description of uncertainties not considered in the distribution State clearly how each distribution was obtained, and in particular whether it was derived by statistical analysis, mechanistic modelling, expert judgement or a combination of these.\nIf quantiles of the distribution are not enough to communicate something important about the distribution, the assessors can also provide a PDF graph of the distribution.\nThey should accompany the PDF with a CDF graph of the distribution if this is useful for technical readers of the assessment.\nUse the same horizontal scale, and plot (as shown here to the left) the CDF above the PDF and clearly mark the location of the central estimate.\nExplain clearly that the distribution represents uncertainty about the quantity of interest, for which there is a single true value, as distributions are more commonly used to represent variability and people may misinterpret them in that way.\nNote that there is specific guidance on how to communicate a distribution that quantifies both variability and uncertainty for the same quantity. The term for this is a “2-dimensional probability distribution’."
  },
  {
    "objectID": "uncertainty-communication/expressions-distribution.html#additional-guidance-when-communicating-uncertainty-with-numbers",
    "href": "uncertainty-communication/expressions-distribution.html#additional-guidance-when-communicating-uncertainty-with-numbers",
    "title": "Probability distribution",
    "section": "Additional guidance when communicating uncertainty with numbers",
    "text": "Additional guidance when communicating uncertainty with numbers\n\nVideoText\n\n\n\n\n\n\nGeneral guidance is provided for communicating uncertainty with numbers.\nAn example of general guidance is about communicating uncertainty about proportions or frequencies: Avoid using percentages for the outcome of interest (e.g. a proportion of something, or the incidence or risk of an outcome or effect), to avoid confusion with percentages quantifying uncertainty. Where possible, use frequencies (e.g. 1 in 20) to express incidence. Example: ‘The Panel was 90% certain that at most 3 in 10,000 sheep would be affected by the virus.’\nAnother example of general guidance is about framing:\nAt the entry level, frame the message positively as % certainty for the outcome, conclusion or range of values that EFSA considers more likely.\nThis is important because expressing, e.g., 5% probability as 5% certainty is misleading, since there is then a 95% probability that the outcome will not occur. This is better expressed as 95% certainty of non-occurrence.\nAt the informed level, repeat the certainty statement from the entry level and explain clearly the main reasons why the outcome might occur. Then, give the main reasons why the outcome might not occur.\nMore tips and recommendations are provided in the guidance of communication of uncertainty."
  },
  {
    "objectID": "uncertainty-communication/communicator.html",
    "href": "uncertainty-communication/communicator.html",
    "title": "for Communicators",
    "section": "",
    "text": "Checklist for identifying messages with associated uncertainty expressions, and specific guidance for their communication:\n\n\n\n\n\n\n\n1. Did the scientific assessment for this message identify any non-standard uncertainties?\n\n\n\n\n\n\nYesNo\n\n\nIf an unqualified conclusion is required, follow the guidance for unqualified conclusions (see Question 5 here). If an unqualified conclusion is not required, state the result of the standardised procedure in the form expressed by the assessors. Also communicate the uncertainty expressions for this message, consulting the respective guidance boxes.\n\n\nReport the conclusion as expressed by assessors and state that a standardised assessment procedure was followed that takes account of standard uncertainties, and no non-standard uncertainties were identified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Does the scientific output list or describe the sources or causes of any uncertainties affecting this message?\n\n\n\n\n\n\nEntryInformed\n\n\n\nState that uncertainties exist, using the wording in the scientific output.\n\n\nExample: “The experts identified limitations in the data on exposure and toxic effects of ZEN and its modified forms” Based on EFSA 2017;15(7):4851\n\n\n\n\nState that uncertainties exist, using the wording in the scientific output.\nInclude in the message a brief description of the sources of uncertainty that have the biggest impact on the respective key messages. (If necessary, consult the assessors to identify these.)\n\n\nExample: “The experts identified limitations in the data on exposure and toxic effects of ZEN and its modified forms, for example (…)” Based on EFSA 2017;15(7):4851\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Is the direction and/or degree of uncertainty expressed verbally (i.e. with terms like ‘low’, ‘medium’, ‘high’) or with symbols (e.g. ‘+’ or ‘-’)?\n\n\n\n\n\n\nEntryInformed\n\n\n\nAvoid altering the wordings used by assessors to describe the direction and/or degree of uncertainty, or factors contributing to uncertainty (see Question 2 above). Always check the rewording with the assessors if you do.\nState clearly what outcomes and conditions this expression of uncertainty refers to (see Question 1 above).\nMake clear that any uncertainty referred to in the communication has been taken into account in the assessment conclusion.\n\n\nExample: “The Panel noted that there was very high uncertainty about the exposure estimates and took this into account in its conclusion that there is no health concern” Based on EFSA 2017;15(7):4851\n\n\n\nAs for entry level, with the following differences: - Before communicating the uncertainty expression, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment. – Optionally, mention specific methods that were used in evaluating the uncertainty. – Optionally, mention factors contributing to the overall uncertainty, including the relative importance of individual sources of uncertainty and things like the relevance and reliability of evidence (e.g. in weight of evidence assessments). – Clearly distinguish individual sources of uncertainty from overall uncertainty about the assessment conclusions\n\nExample: “The Panel noted that a high proportion of measurements of ZEN and its modified forms in feed were below the limit of detection, leading to very high uncertainty when estimating exposure” Based on EFSA 2017;15(7):4851\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. Does this message report an inconclusive assessment outcome?\n\n\n\n\n\n\nEntryInformed\n\n\n\nCommunicate clearly that EFSA is unable to give any conclusion on the quantity or question of interest to which this message refers. If the assessment is inconclusive, this implies that nothing further can be said and therefore the communication should avoid using language that might suggest otherwise.\nIndicate very briefly the sources of uncertainty that contribute most to this outcome (e.g. lack of data, poor quality or limited relevance of data).\n\n\nExample: “EFSA’s experts could not reach a conclusion on the risk for cattle, ducks, goats, horses, rabbits, mink and cats because of a lack of data” Based on EFSA 2017;15(7):4851\n\n\n\n\nDescribe the main sources of uncertainty in more detail, but concisely (following the guidance in Question 2 above).\nInconclusive assessments are especially likely to include options or requirements for obtaining further data. Communicate these as instructed in UC Section 3.1.6.\n\n\nExample: “EFSA’s experts could not reach a conclusion on the risk for cattle, ducks, goats, horses, rabbits, mink and cats due to limitations in available data on exposure and toxic effects of ZEN and itsmodified forms, for example (…)” Based on EFSA 2017;15(7):4851\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. Do risk managers expect or does legislation require that this message should be expressed as an unqualified conclusion, without any expression of uncertainty?\n\n\n\n\n\n\nEntryInformed\n\n\n\nReport the unqualified conclusion for this message using the same wording as the assessors.\n\n\nExample: “EFSA’s experts concluded that the exposure to feed containing ZEN ‘in farm situations’ is a low health risk for sheep, dog, pig and fish, and an extremely low health risk for poultry” Based on EFSA 2017;15(7):4851\nIn this example, the word ‘low’ refers to the conclusion on the level of health risk. There is no expression of uncertainty about this – no indication that the risk might be other than ‘low’, i.e. an unqualified conclusion about the level of risk.\n\n\n\n\nAs for the entry level.\nOptionally, describe briefly how the assessment was made (i.e. what evidence and methods were used to arrive at the conclusions).\nBriefly describe some examples of uncertainties affecting the assessment for this message, as identified in your completed template, consulting Box 4 for guidance on how to communicate this.\nIf the assessment contains any verbal or numerical expression of the impact of the uncertainties as identified in your template, follow the respective guidance in Boxes 6–9 below.\nSay that the assessors took the uncertainties into account when reaching their conclusion(s) for this message.\n\n\nExample: “Following the standard assessment procedure (or ‘Using the evaluation system agreed for contaminants in feed’), experts estimated that high exposure to feed containing ZEN is below the reference value for a health risk for sheep, dog, pig and fish, and well below the reference value for chicken and turkeys. They therefore concluded that the exposure to feed containing ZEN ‘in farm situations’ is a low health risk for sheep,dog,pig and fish,and an extremely low health risk for poultry. In reaching this conclusion, the experts took account of limitations in the data on exposure and toxic effects of ZEN and its modified forms, for example (…)” Based on EFSA 2017;15(7):4851\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6. Is the uncertainty described with numbers as a precise probability?\n\n\n\n\n\n\nEntryInformed\n\n\n\nState clearly what the probability refers to, including whether it refers to a numerical estimate or a qualitative conclusion. When the probability refers to a numerical estimate, also state the range of the quantity that the probability refers to (see example below).\n\n\nExample: “The Panel estimates that, under current regulations, the total number of infested tulips in in greenhouses in the EU is 60,000. Based on what is known, the Panel is 50% certain that the number is between 10,000 and 200,000 infested plants.” Based on EFSA 2017;15(8):4879\n\n\n\nAs for the entry level, with the following differences: - Before giving the probability, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account when assessing their level of certainty. – Optionally, mention specific methods that were used in quantifying the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\n\nExample: “The Panel performed its assessment using a mathematical model of the entry of nematodes into the EU and their establishment and spread in greenhouse tulips. Uncertainty on the factors represented in the model was quantified by expert judgement, taking into account the limitations of the available data. The Panel estimates…[continue as for entry level]” Based on EFSA 2017;15(8):4879\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7. Is the uncertainty described with numbers as an approximate probability?\n\n\n\n\n\n\nEntryInformed\n\n\n\nState clearly what the probability refers to, including whether it refers to a numerical estimate or a qualitative conclusion. When the probability refers to a numerical estimate, also state the range of the quantity that the probability refers to.\nAn approximate probability may comprise a range of probabilities chosen by the assessors from the approximate probability scale, or a different range of probabilities specified by the assessors.\nAlways communicate the quantitative range of probabilities because this expresses the assessors’ conclusion without ambiguity. If a verbal expression is also used, present the quantitative probability first (e.g. ‘66–90% certain (likely)’) because it has been shown that this order leads to more consistent understanding than if the verbal expression is presented first (see UC Section 3.1)\nTo avoid inconsistency and misunderstanding, do not use the verbal terms in Table 1 to refer to any probabilities or ranges of probabilities other than those shown in this table.\n\n\nExample: “The experts considered it 66–90% certain (likely) that the increasing proportion of elderly and susceptible people has contributed to the rise in Listeria cases” Based on EFSA 2018;16(1):5134\n\n\n\nAs for entry level, with the following differences: - Before giving the probability, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account when assessing their level of certainty. - Optionally, mention specific methods that were used in quantifying the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\n\nExample: “Experts began work on the Scientific Opinion after the 2015 EU summary report on foodborne zoonotic diseases identified an increasing trend of listeriosis over the period 2009–2013. The Panel performed a statistical analysis, which confirmed the increasing trend, and developed a mathematical model of the factors influencing the incidence of infections. Considering the modelling results and the degree of support from indicator data, the experts…’ [continue as for entry level]” Based on EFSA 2018;16(1):5134\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8. Is the uncertainty described with numbers as a one-dimensional probability distribution?\n\n\n\n\n\n\nEntryInformed\n\n\n\nA probability distribution quantifies uncertainty regarding a quantity that has a single true value (not a variable). Graphical representations of distributions are difficult to interpret for non-technical audiences. Therefore, for entry and informed levels, only communicate selected results extracted from the distribution as recommended below.\nState clearly what the distribution refers to (see Section 3.1 General guidance, point 2).\nProvide the central estimate (mean or median), the P5–P95 range (within which it is 90% certain that the true value lies) and/or the P25–P75 range (within which it is 50% certain the true value lies), expressed in such a way that the meaning of the ranges is clear. If it is critical to understanding the message, also give an idea about the form of the distribution behind the range (likelihood associated with particular values/outcomes). If the assessors have provided different quantiles (e.g. P1–P99, see example below), use these instead.\n\n\nExample: “Experts estimated that, on average in Europe in 2015, 16 out of 100 slaughtered dairy cows were pregnant. Their assessment is based on limited data, but the experts are 50% certain that the European average for 2015 is between 9 and 27, and 98% certain it is between 2 and 60” Based on EFSA 2017;15(5):4782\nNote that the example refers to a quantity that is uncertain but not variable: although the average will vary between countries and over time, ‘the European average for 2015’ has a single true value, which is uncertain. For guidance on communicating quantities that are both variable and uncertain, see Question 9 below.\n\n\nIn addition, or alternatively: if a regulatory (reference) value exists, or a value of particular interest for other reasons (e.g. more than zero occurrence of an adverse outcome), provide the probability of exceeding that value. (See Question 9 for an example of this.)\n\n\n\nAs for the entry level with the following differences:\n\nBefore giving the results, describe examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account in their assessment.\n\nOptionally, mention specific methods that were used to quantify the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\n\nConsider providing a visual representation of the uncertainty if this is expected to be useful for understanding, e.g. to aid understanding of the distribution of probability around the central estimate. Use a box plot for this at the informed level. (Do not use graphical representations of full distributions, such as PDF or CDF, in communications for non-technical audiences because these are easily misunderstood.) When using a box plot, explain clearly that it represents the uncertainty, as they are commonly used to represent variability and people may misinterpret them in that way. When including a graphic, provide this in addition to the entry level representation (see above) and not instead of it.\n\nLabel the graphs (including axes, legends, and units) appropriately so that, as far as possible, people will understand them on their own. Accompany every visual with sufficient textual explanation that the informed-level audience will understand it.\n\n\nExample: “Ten experts from different EU countries each surveyed a sample of slaughterhouses in their country to gather information on the prevalence of animals being pregnant at slaughter in 2015. Six of those experts used the survey results and other available evidence to estimate the average prevalence in Europe for different species in 2015, taking account of the uncertainties involved. The Panel’s conclusions are based on the results. Experts estimated… [continue as in entry-level communication]” Based on EFSA 2017;15(5):4782\n\nWhen communicating using box or box-and-whisker plots:\n\nAlways accompany box plots with explanations of each element they contain (including central estimate, box and whiskers).\nIf there is a value or quantity that is of particular interest to risk managers or the public (e.g. a regulatory/reference value), communicate the probability of the true value being above or below this (depending which is of interest) alongside the box plot, following the specific guidance in Question 6.\n\n\n\n\n\n\nFigure 1: Example of a box-and-whiskers plot\n\n\n\n\n\nExample: “Figure 1 is a box plot summarising the combined judgement of six experts about the average number of pregnant dairy cows per 100 cows slaughtered in the EU in 2015. The horizontal line inside the box is the median estimate: the true average value is considered equally likely to be above or below this estimate. The box and whiskers represent the experts’ collective uncertainty about the EU average in 2015 (not variation between samples of animals). There is 50% certainty that the true average is in the box and 98% certainty it is between the whiskers. There is still a 2% chance that the true average is outside the whiskers” Based on EFSA 2017;15(5):4782\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9. Is the uncertainty described with numbers as a two-dimensional probability distribution?\n\n\n\n\n\n\nEntryInformed\n\n\n\nThe term “2D probability distribution” refers here to a distribution that quantifies both variability and uncertainty for the same quantity (e.g. uncertainty about the variability of exposure in a population). Such distributions are difficult to interpret even for technical audiences. Therefore, for entry and informed levels only communicate selected results extracted from the 2D distribution.\nCheck with the assessors which results from the 2D assessment are likely to interest the audience, normally one or both of the following:\n\nThe median estimate for a specified quantile of variability, e.g. the 95th percentile, together with a P2.5 and P97.5 (or other quantiles) to represent its uncertainty.\nThe median estimate for the frequency of exceeding (or being below) a specified value of the quantity, e.g. the proportion of a population that exceeds a regulatory reference value, together with a P2.5 and P97.5 (or other quantiles) to represent its uncertainty.\n\nObtain the selected results from the assessors. The range of values between the two quantiles has a specified probability (95% for P2.5 and P97.5 in the example below), so communicate this following the approach in Question 6.\n\n\nExample: “The Panel estimated that,if intake of carbendazim from apples and apple products including apple juice was measured for 10,000 UK toddlers on single day schosen at random, including instances where no apples or apple products were consumed, 10 of those intakes would exceed the safe level.The experts are 95% certain that the number of intakes exceeding the safe level would be between 1 and 40 per 10,000” Based on EFSA 2007;5(8):538\nThe scientific opinion expressed the results in ‘toddlerdays’, which is not readily understandable to non-technical audiences, so it has been reformulated. Other expressions, e.g. number of toddlers per year, might be preferable but are not derivable without additional assumptions. In real communications, the meaning of ‘safe level’ also needs explaining.)\n\n\n\n\nBefore giving the results, describe a few examples of the evidence/data that were considered and the uncertainties affecting the assessment, and state that the experts took these into account in their assessment.\nOptionally, mention specific methods that were used in quantifying the uncertainty, e.g. modelling, statistical analysis, expert knowledge elicitation (EKE), or a combination of these.\nProvide a visual representation of the uncertainty if possible, especially if the uncertainty information is part of the key messages. Use a box plot for this showing median, P5, P25, P75 and P95 for the specific result(s) selected from the 2D distribution. Ask the scientific officer to provide the box plot for the selected result, and communicate it as indicated in Quesion 8.\nDo not use graphical representations of full 2D distributions in entry or informed level communications as many people misunderstand them.\n\n\nExample: “The Panel developed a mathematical model of the exposure of toddlers to carbendazim in apples and apple products. They used UK data on the occurrence of carbendazim in apples and on consumption of apples and apple products by toddlers. The model computed different eating patterns among toddlers by simulating apple consumption and carbendazim intake for 10,000 ‘toddler-days’ (10,000 random samples from a survey of daily apple consumption by UK toddlers, including records where no apples or apple products were consumed). The simulation also calculated five types of uncertainty affecting the model, e.g. limited measurements of occurrence and consumption, limitations in the precision of the occurrence data. Using this model, the Panel estimated that the number of ‘toddler-days’ in which more than the safe level for carbendazim is ingested from apples and apple products is 10 per 10,000 toddler-days. However, this takes account of only the five sources of uncertainty that were quantified: other uncertainties were taken into account separately by expert judgement” Based on EFSA 2007;5(8):538\nNote: after the above text provide information on the experts’ judgement about other uncertainties which the model did not quantify, then go back to to this Questionnaire to identify the type of expression used and locate the corresponding specific guidance for communication."
  },
  {
    "objectID": "implementation.html",
    "href": "implementation.html",
    "title": "Implementation of uncertainty analysis",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nWelcome to the implementation page!\nHere we have collected some examples of scientific opinions and report which demonstrate how the guidances can be put into practice.\nYou should not think of this list as recommendations or endorsements, but rather as a list of references or real-life examples of assessments, which contain certain aspects of the implementation of the guidance for uncertainty analysis or the guidance of communication of uncertainty.\nThe examples are reported in an interactive table which contains a list of scientific opinions organised by the Panel, and the type of assessment.\nThis list is not exhaustive.\nYou can use the search bar to quickly subset this table by the uncertainty expression or the method of uncertainty analysis used in the assessment.\nLastly, we provide the motivation for including the specific assessment in the list of examples. You can get to the full document text by clicking the link in the last column of the table. The link will take you to the location of the example in the document.\nWe hope you find these examples useful in your work!\nThank you!\n\n\n\n\nExamples of scientific reports and opinions implementing the Guidance on Uncertainty Analysis.\nPlease use the ⊕ sign to unfold the comments section. Each line in the comment section corresponds to an expression or a method indicated for the selected scientific opinion. If you click on the method or expression you will be taken to a place in the opinion where this is used or described.\nMethods for uncertainty analysis are described in Principles and Methods section. Expressions resulting from applying uncertainty analysis are described in the Communication of Uncertainty section.\n\n\nLegend:\nAssessment types: CS - Case-specific assessment; SA - Standardised assessment; UA - Urgent assessment; DRGD - Development or revision of guidance documents\nPanels: AHAW - Animal Health and Welfare; PLH - Plant Health; BIOHAZ - Biological Hazards; CONTAM - Contaminants in the Food Chain; SC - Scientific Committee; PPR - Panel on Plant Protection Products and their Residues; NDA - Nutrition, Novel Foods and Food Allergens; CEP - Food Contact Materials, Enzymes and Processing Aids`"
  },
  {
    "objectID": "uncertainty-communication.html",
    "href": "uncertainty-communication.html",
    "title": "Guidance on Communication of Uncertainty",
    "section": "",
    "text": "Introduction videoIntroduction transcript\n\n\n\n\n\n\nWelcome to the page for the EFSA guidance on communication of uncertainty\nThis page takes you through the suggestions provided for communicators and assessors.\nThe guidance consists of a series of questions helping you to tailor the uncertainty communication message targeting the audience at the “entry”, “informed” and “technical” levels.\nYou can access these checklists by clicking one of the buttons below.\nYou can also learn about the principles behind the EFSA guidance on communication of uncertainty.\nNote that the text in this tutorial is a condensed extract from the EFSA guidance on communication of uncertainty. If something is unclear or feels incomplete, or if you want to learn more, we recommend you to consult the original document. Always refer to the guidance documents as your primary source!\nWe hope you enjoy the material!\n\n\n\nUse one of the buttons below to proceed to communication checklists!\n\nI AM A COMMUNICATOR I AM AN ASSESSOR\n\n\n\n\n\n\n\nPrinciples of uncertainty communication\n\n\n\n\nPrinciples of Uncertainty Communication.\nCommunicating Scientific Uncertainties, covering risk communication mandate, risk perception and the uncertainty communicaiton challenges.\n\n\n\nThe Uncertainty Analysis GD contains several options for carrying out an uncertainty analysis. The Communication of Uncertainty GD provides recommendations for communicating the types of expressions of uncertainty that are normally produced when uncertainty analysis is conducted by following those options. It does not include expressions that are discouraged by the Uncertainty Analysis GD, or other expressions that may be generated by some more specialised methods and the outputs of some methods for sensitivity analysis.\nIt is currently expected that, in many assessments, the conclusion, summary and abstract of the assessment will not contain expressions of uncertainty. This may arise in two types of situation, as indicated in the description for ‘unqualified conclusions’.\n\n\n\n\n\n\nExpressions of uncertainty\n\n\n\n\nUnqualified conclusion, with no expression of uncertainty\nDescription of sources of uncertainty\nQualitative description of the direction and/or magnitude of uncertainty using words or symbols\nInconclusive assessment\nA precise probability\nAn approximate probability\nA probability distribution\nA two-dimensional probability distribution"
  },
  {
    "objectID": "uncertainty-analysis/figure4.html",
    "href": "uncertainty-analysis/figure4.html",
    "title": "Standardized Assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Take account&lt;/br&gt;of the contribution&lt;/br&gt;of any additional uncertainties] --&gt; E[Check for and&lt;/br&gt;describe any &lt;/br&gt;unquantified uncertainties]\n  end\n  A([&lt; BACK]) --&gt; B[Obtain a probability&lt;/br&gt; or a distribution&lt;/br&gt; for each part of the&lt;/br&gt; uncertainty analysis]\n  B --&gt; C[Combine the parts&lt;/br&gt; using the calculation&lt;/br&gt; chosen earlier]\n  C --&gt; ov\n  ov --&gt; G[[Report conclusion&lt;/br&gt;in form needed by&lt;/br&gt;decision-makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion or annex]]\n  \n  click A \"figure3.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C \"figure3.html\" \"See Section 17 regarding reporting.\"\n  click Z \"figure4.html\"\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure4.html#assessing-non-standard-uncertainties-for-separate-parts-of-the-uncertainty-analysis-using-probabilities-or-distributions",
    "href": "uncertainty-analysis/figure4.html#assessing-non-standard-uncertainties-for-separate-parts-of-the-uncertainty-analysis-using-probabilities-or-distributions",
    "title": "Standardized Assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Take account&lt;/br&gt;of the contribution&lt;/br&gt;of any additional uncertainties] --&gt; E[Check for and&lt;/br&gt;describe any &lt;/br&gt;unquantified uncertainties]\n  end\n  A([&lt; BACK]) --&gt; B[Obtain a probability&lt;/br&gt; or a distribution&lt;/br&gt; for each part of the&lt;/br&gt; uncertainty analysis]\n  B --&gt; C[Combine the parts&lt;/br&gt; using the calculation&lt;/br&gt; chosen earlier]\n  C --&gt; ov\n  ov --&gt; G[[Report conclusion&lt;/br&gt;in form needed by&lt;/br&gt;decision-makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion or annex]]\n  \n  click A \"figure3.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C \"figure3.html\" \"See Section 17 regarding reporting.\"\n  click Z \"figure4.html\"\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure4.html#options-for-combining-non-standard-uncertainties-in-a-standard-assessment",
    "href": "uncertainty-analysis/figure4.html#options-for-combining-non-standard-uncertainties-in-a-standard-assessment",
    "title": "Standardized Assessments",
    "section": "Options for combining non-standard uncertainties in a standard assessment",
    "text": "Options for combining non-standard uncertainties in a standard assessment\n\nVideoText\n\n\n\n\n\n\nThe simplest, but also most approximate, option is to evaluate the combined impact of the non-standard uncertainties collectively. This is feasible when there are few uncertainties to consider in the assessment of overall uncertainty.\nIn this case, the procedure is to Define the question or quantity of interest. Decide how to express the uncertainty. Elicit a probability judgement for the overall uncertainty. Check for and describe any unquantified uncertainties. Report conclusion in form needed by decision-makers and provide a detailed analysis in opinion or annex.\nWhen there are several non-standard uncertainties related to different parts of an assessment, the guidance recommends to assess non-standard uncertainties for different parts of the uncertainty analysis separately using probability bounds. Specifically, the approach is to Divide the uncertainty analysis into convenient parts define the question or quantity of interest for each part and an appropriate calculation for combining the parts. Elicit probability bounds for each part and combine probability bounds by calculation. Take account of the contribution of any additional uncertainties Check for and describe any unquantified uncertainties\nThen the assessors should ask if the result is expected to be sufficient for decision-making.\nIf yes, no further uncertainty analysis is required.\nIf the result is not expected to be sufficient for decision making, make a more refined characterisation of uncertainty. Go back to some or all parts of the assessment, and either elicit refined probability bounds or assess non-standard uncertainties for separate parts of the uncertainty analysis using probabilities or distributions instead. The latter option requires fully specified probabilities, distributions and dependencies, and more complex calculations, but usually decreases uncertainty about the answer to the question of interest."
  },
  {
    "objectID": "uncertainty-analysis/figure4.html#communication-of-a-standard-assessment",
    "href": "uncertainty-analysis/figure4.html#communication-of-a-standard-assessment",
    "title": "Standardized Assessments",
    "section": "Communication of a standard assessment",
    "text": "Communication of a standard assessment\n\nVideoText\n\n\n\n\n\n\nWhen the uncertainty analysis is complete, assessors should report the conclusion in the form needed by decision-makers and provide a detailed analysis in the opinion or as an appendix or annex, including a list of all the non-standard uncertainties that were identified.\nThere is explicit guidance for communicators on how to communicate information about uncertainty in assessments using standardised procedures, for example in EFSA press releases or social media outputs.\nIf the scientific assessment did not identify any non-standard uncertainties - report the conclusion as expressed by assessors and state that a standardised assessment procedure was followed that takes account of standard uncertainties, and no non-standard uncertainties were identified.\nIf the scientific assessment identified any non-standard uncertainties - report as an unqualified conclusion (if that is required) or state the result of the standardised procedure in the form expressed by the assessors.\nMention the identified non-standard uncertainties in the report.\nThis was all for now about the steps of a standard assessment.\nNote that there is more detailed information in the guidance for uncertainty analysis in the text and footnotes accompanying each flowchart."
  },
  {
    "objectID": "uncertainty-analysis/figure6.html",
    "href": "uncertainty-analysis/figure6.html",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B[Conduct the assessment&lt;/br&gt;as planned, noting&lt;/br&gt;any further&lt;/br&gt;uncertainties you identify] \n  B --&gt; C[Ensure the question&lt;/br&gt;or quantity of interest&lt;/br&gt;is well-defined]\n  C --&gt; ov\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Decide&lt;/br&gt;how to express&lt;/br&gt;the uncertainty] --&gt; E[Elicit&lt;/br&gt;a probability judgement&lt;/br&gt;for the overall&lt;/br&gt;uncertainty]\n    E --&gt; F[Check for&lt;/br&gt;and describe&lt;/br&gt;any unquantified&lt;/br&gt;uncertainties]\n  end\n    ov --&gt; G[[Report conclusion&lt;/br&gt;in form needed&lt;/br&gt;by decision makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion&lt;/br&gt;or annex]]\n  \n  click A \"figure5.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure3.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure6.html#assessing-uncertainties-collectively-in-a-case-specific-assessment",
    "href": "uncertainty-analysis/figure6.html#assessing-uncertainties-collectively-in-a-case-specific-assessment",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B[Conduct the assessment&lt;/br&gt;as planned, noting&lt;/br&gt;any further&lt;/br&gt;uncertainties you identify] \n  B --&gt; C[Ensure the question&lt;/br&gt;or quantity of interest&lt;/br&gt;is well-defined]\n  C --&gt; ov\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Decide&lt;/br&gt;how to express&lt;/br&gt;the uncertainty] --&gt; E[Elicit&lt;/br&gt;a probability judgement&lt;/br&gt;for the overall&lt;/br&gt;uncertainty]\n    E --&gt; F[Check for&lt;/br&gt;and describe&lt;/br&gt;any unquantified&lt;/br&gt;uncertainties]\n  end\n    ov --&gt; G[[Report conclusion&lt;/br&gt;in form needed&lt;/br&gt;by decision makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion&lt;/br&gt;or annex]]\n  \n  click A \"figure5.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure3.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure12.html",
    "href": "uncertainty-analysis/figure12.html",
    "title": "Uncertainty analysis for urgent assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B[Ensure the inputs,&lt;/br&gt;methods and outputs&lt;/br&gt;of the standard procedure&lt;/br&gt;are well-defined] \n  B --&gt; C[Define the class&lt;/br&gt;of problems or applications&lt;/br&gt;this procedure&lt;/br&gt;will be used for]\n  C --&gt; D[Agree the management&lt;/br&gt;objective for the&lt;/br&gt;standard procedure]\n  D --&gt; E[Design and perform&lt;/br&gt;a scientific assessment&lt;/br&gt;and uncertainty analysis&lt;/br&gt;of the extent to which&lt;/br&gt;the standard procedure&lt;/br&gt;will achieve the management objective]\n  E --&gt; F{Is there&lt;/br&gt;sufficient probability&lt;/br&gt;of achieving&lt;/br&gt;the management objective&lt;/br&gt;to an acceptable extent?}\n  F -- Yes --&gt; G[Document the assessment&lt;/br&gt;and uncertainty analysis&lt;/br&gt;of the standard procedure]\n  F -- No --&gt; H[Modify the standard&lt;/br&gt;procedure in ways that&lt;/br&gt;you expect to achieve&lt;/br&gt;the management objective]\n  H --&gt; I[Redo the assessment&lt;/br&gt;and uncertainty analysis&lt;/br&gt;for the modified procedure]\n  I -.-&gt; E\n  G --&gt; J[[Make the standard&lt;/br&gt;procedure available&lt;/br&gt;for use]]\n  \n  \n  click A \"../uncertaintyanalysis.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure12.html#urgent-assessments",
    "href": "uncertainty-analysis/figure12.html#urgent-assessments",
    "title": "Uncertainty analysis for urgent assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B[Ensure the inputs,&lt;/br&gt;methods and outputs&lt;/br&gt;of the standard procedure&lt;/br&gt;are well-defined] \n  B --&gt; C[Define the class&lt;/br&gt;of problems or applications&lt;/br&gt;this procedure&lt;/br&gt;will be used for]\n  C --&gt; D[Agree the management&lt;/br&gt;objective for the&lt;/br&gt;standard procedure]\n  D --&gt; E[Design and perform&lt;/br&gt;a scientific assessment&lt;/br&gt;and uncertainty analysis&lt;/br&gt;of the extent to which&lt;/br&gt;the standard procedure&lt;/br&gt;will achieve the management objective]\n  E --&gt; F{Is there&lt;/br&gt;sufficient probability&lt;/br&gt;of achieving&lt;/br&gt;the management objective&lt;/br&gt;to an acceptable extent?}\n  F -- Yes --&gt; G[Document the assessment&lt;/br&gt;and uncertainty analysis&lt;/br&gt;of the standard procedure]\n  F -- No --&gt; H[Modify the standard&lt;/br&gt;procedure in ways that&lt;/br&gt;you expect to achieve&lt;/br&gt;the management objective]\n  H --&gt; I[Redo the assessment&lt;/br&gt;and uncertainty analysis&lt;/br&gt;for the modified procedure]\n  I -.-&gt; E\n  G --&gt; J[[Make the standard&lt;/br&gt;procedure available&lt;/br&gt;for use]]\n  \n  \n  click A \"../uncertaintyanalysis.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/future-investigation.html",
    "href": "uncertainty-analysis/future-investigation.html",
    "title": "Prioritising uncertainties for future investigation",
    "section": "",
    "text": "Prioritising sources of uncertainty may be useful at different stages of the assessment and uncertainty analysis.\nIn the early stages, it can be used to select more important uncertainties to be analysed by more refined methods.\nPrioritisation can also be used during the course of an assessment, to identify parts of the assessment where it might be beneficial to search for more data, use more complex models, or invite additional experts.\nAt the end of the assessment, it may be useful to prioritise uncertainties to identify potential areas for further research.\nPrioritisation, at any stage of the assessment, should be based on the contribution of individual sources of uncertainty to the uncertainty of the assessment as a whole.\nThe relative influence of different uncertainties can be assessed in a simple and approximate way using qualitative methods based on expert judgement. An ordinal scale can be used to express expert judgements of the magnitude and/or direction of impact of each uncertainty on the question or quantity of interest.\nWhen the assessment involves a calculation or quantitative model, the contributions of uncertainties about the model inputs can be assessed rigorously by sensitivity analysis. These range from simple ‘what if’ calculations to sophisticated global sensitivity analyses.\nThe influence of uncertainties relating to choices regarding the structure of the model or assessment may need to be addressed by repeating the assessment with alternative choices.\nPrioritisation at the early stages of an assessment must necessarily be done by expert judgement or by sensitivity analysis using a preliminary model, as the assessment model is still under development."
  },
  {
    "objectID": "uncertainty-analysis/figure2.html",
    "href": "uncertainty-analysis/figure2.html",
    "title": "Standardized Assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B{Are assessors able&lt;/br&gt;to evaluate&lt;/br&gt;the non-standard&lt;/br&gt; uncertainties&lt;/br&gt; collectively?}\n      B -- Yes --&gt; C[Define the question&lt;/br&gt;or quantity of interest]\n      C --&gt; ov\n  B -- No --&gt; Z([NEXT &gt;])\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Decide&lt;/br&gt;how to express&lt;/br&gt;the uncertainty] --&gt; E[Elicit&lt;/br&gt;a probability judgement&lt;/br&gt;for the overall&lt;/br&gt;uncertainty]\n    E --&gt; F[Check for&lt;/br&gt;and describe&lt;/br&gt;any unquantified&lt;/br&gt;uncertainties]\n  end\n    ov --&gt; G[[Report conclusion&lt;/br&gt;in form needed&lt;/br&gt;by decision makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion&lt;/br&gt;or annex]]\n  \n  click A \"figure1.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure3.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure2.html#assessing-non-standard-uncertainties-collectively",
    "href": "uncertainty-analysis/figure2.html#assessing-non-standard-uncertainties-collectively",
    "title": "Standardized Assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B{Are assessors able&lt;/br&gt;to evaluate&lt;/br&gt;the non-standard&lt;/br&gt; uncertainties&lt;/br&gt; collectively?}\n      B -- Yes --&gt; C[Define the question&lt;/br&gt;or quantity of interest]\n      C --&gt; ov\n  B -- No --&gt; Z([NEXT &gt;])\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Decide&lt;/br&gt;how to express&lt;/br&gt;the uncertainty] --&gt; E[Elicit&lt;/br&gt;a probability judgement&lt;/br&gt;for the overall&lt;/br&gt;uncertainty]\n    E --&gt; F[Check for&lt;/br&gt;and describe&lt;/br&gt;any unquantified&lt;/br&gt;uncertainties]\n  end\n    ov --&gt; G[[Report conclusion&lt;/br&gt;in form needed&lt;/br&gt;by decision makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion&lt;/br&gt;or annex]]\n  \n  click A \"figure1.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure3.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure11.html",
    "href": "uncertainty-analysis/figure11.html",
    "title": "Uncertainty analysis for urgent assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B[Conduct the assessment,&lt;/br&gt;listing uncertainties&lt;/br&gt;you identify] \n  B --&gt; C[Ensure the question&lt;/br&gt;or quantity of interest&lt;/br&gt;is well-defined]\n  C --&gt; D[Decide how to&lt;/br&gt;express the uncertainty]\n  D --&gt; E[Quickly check&lt;/br&gt;the assessment for&lt;/br&gt;additional uncertainties]\n  E --&gt; ov\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n F[Elicit&lt;/br&gt;a probability judgement&lt;/br&gt;for the overall&lt;/br&gt;uncertainty] --&gt; G[Check for&lt;/br&gt;and describe&lt;/br&gt;any unquantified&lt;/br&gt;uncertainties]\n  end\n    ov --&gt; H[[Report conclusion&lt;/br&gt;in form needed&lt;/br&gt;by decision makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion&lt;/br&gt;or annex]]\n  \n  click A \"../uncertaintyanalysis.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure11.html#urgent-assessments",
    "href": "uncertainty-analysis/figure11.html#urgent-assessments",
    "title": "Uncertainty analysis for urgent assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B[Conduct the assessment,&lt;/br&gt;listing uncertainties&lt;/br&gt;you identify] \n  B --&gt; C[Ensure the question&lt;/br&gt;or quantity of interest&lt;/br&gt;is well-defined]\n  C --&gt; D[Decide how to&lt;/br&gt;express the uncertainty]\n  D --&gt; E[Quickly check&lt;/br&gt;the assessment for&lt;/br&gt;additional uncertainties]\n  E --&gt; ov\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n F[Elicit&lt;/br&gt;a probability judgement&lt;/br&gt;for the overall&lt;/br&gt;uncertainty] --&gt; G[Check for&lt;/br&gt;and describe&lt;/br&gt;any unquantified&lt;/br&gt;uncertainties]\n  end\n    ov --&gt; H[[Report conclusion&lt;/br&gt;in form needed&lt;/br&gt;by decision makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion&lt;/br&gt;or annex]]\n  \n  click A \"../uncertaintyanalysis.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure1.html",
    "href": "uncertainty-analysis/figure1.html",
    "title": "Uncertainty analysis for standardised assessments",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nThe Guidance on uncertainty analysis is designed for different types of assessment.\nA standard assessment follows a procedure that specifies every step of assessment for a specified class or products or problems, and is accepted by assessors and decision-makers as providing an appropriate basis for decision-making.\nA standard assessment is judged (implicitly or explicitly) to be sufficiently conservative, providing adequate cover for the standard source of uncertainty affecting the assessment.\nFor example, uncertainties due to within and between species differences in toxicity are often addressed by a default factor of 100 in chemical risk assessment.\nMost standardised procedures involve deterministic calculations using a combination of standard study data, default assessment factors and default values.\nStandard sources of uncertainty should in principle have been identified when developing a standard procedure, together with criteria to evaluate when uncertainties are standard.\nAn uncertainty analysis for a standard procedure starts with Checking for non-standard uncertainties.\nThis is done by checking every part of assessment for non-standard uncertainties and listing those identified.\nIf no non-standard uncertainties are identified - the assessors should document in the opinion that non-standard uncertainties were checked for and none were identified. In this case, no further analysis of the uncertainties is needed.\nIf one or more non-standard uncertainties are identified, their impact on the final conclusions will have to be assessed.\n\n\n\n\n\n\n\n\n%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TD\n  A([&lt; BACK]) --&gt; B{Check every part&lt;/br&gt;of assessment&lt;/br&gt;for nonstandard&lt;/br&gt;uncertainties}\n  B -- None identified --&gt; C[[Document in the opinion that&lt;/br&gt;non-standard uncertainties&lt;/br&gt;were checked for&lt;/br&gt;and none were identified]]\n  B --One or more identified --&gt; Z([NEXT &gt;])\n  click A \"../uncertaintyanalysis.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure2.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure1.html#introduction-to-standardised-assessments",
    "href": "uncertainty-analysis/figure1.html#introduction-to-standardised-assessments",
    "title": "Uncertainty analysis for standardised assessments",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nThe Guidance on uncertainty analysis is designed for different types of assessment.\nA standard assessment follows a procedure that specifies every step of assessment for a specified class or products or problems, and is accepted by assessors and decision-makers as providing an appropriate basis for decision-making.\nA standard assessment is judged (implicitly or explicitly) to be sufficiently conservative, providing adequate cover for the standard source of uncertainty affecting the assessment.\nFor example, uncertainties due to within and between species differences in toxicity are often addressed by a default factor of 100 in chemical risk assessment.\nMost standardised procedures involve deterministic calculations using a combination of standard study data, default assessment factors and default values.\nStandard sources of uncertainty should in principle have been identified when developing a standard procedure, together with criteria to evaluate when uncertainties are standard.\nAn uncertainty analysis for a standard procedure starts with Checking for non-standard uncertainties.\nThis is done by checking every part of assessment for non-standard uncertainties and listing those identified.\nIf no non-standard uncertainties are identified - the assessors should document in the opinion that non-standard uncertainties were checked for and none were identified. In this case, no further analysis of the uncertainties is needed.\nIf one or more non-standard uncertainties are identified, their impact on the final conclusions will have to be assessed.\n\n\n\n\n\n\n\n\n%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TD\n  A([&lt; BACK]) --&gt; B{Check every part&lt;/br&gt;of assessment&lt;/br&gt;for nonstandard&lt;/br&gt;uncertainties}\n  B -- None identified --&gt; C[[Document in the opinion that&lt;/br&gt;non-standard uncertainties&lt;/br&gt;were checked for&lt;/br&gt;and none were identified]]\n  B --One or more identified --&gt; Z([NEXT &gt;])\n  click A \"../uncertaintyanalysis.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure2.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure10.html",
    "href": "uncertainty-analysis/figure10.html",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B[Perform the&lt;/br&gt;scientific assessment&lt;/br&gt;and uncertainty analysis&lt;/br&gt;together] \n  B --&gt; C[Do you want&lt;/br&gt;to try the simpler&lt;/br&gt;option of&lt;/br&gt;using only bounded&lt;/br&gt;probabilities?]\n  C --Yes--&gt;D[Quantify uncertainty&lt;/br&gt;for each part of&lt;/br&gt;the uncertainty analysis,&lt;/br&gt;using probability bounds for&lt;/br&gt;both uncertainty and variability]\n  D--&gt; F[Combine the parts&lt;/br&gt;by probability bounds&lt;/br&gt; calculation]\n  C --No --&gt;E[Quantify uncertainty&lt;/br&gt;for each part of&lt;/br&gt;the uncertainty analysis,&lt;/br&gt;using 2D distributions&lt;/br&gt;for variable quantities]\n  E --&gt; G[Combine the parts&lt;/br&gt;by 2D Monte Carlo&lt;/br&gt;simulation]\n    subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    H[Take account&lt;/br&gt;of the contribution&lt;/br&gt;of any additional&lt;/br&gt;uncertainties] --&gt; I[Check for&lt;/br&gt;and describe any&lt;/br&gt;unquantified uncertainties]\n  end\n  F & G --&gt; ov\n  ov--&gt; J[[Report conclusion&lt;/br&gt;in form needed&lt;/br&gt;by decision makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion&lt;/br&gt;or annex]]\n  click A \"figure5.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure10.html#evaluating-uncertainties-separately-for-different-parts-of-an-assessment-involving-nonvariable-quantities",
    "href": "uncertainty-analysis/figure10.html#evaluating-uncertainties-separately-for-different-parts-of-an-assessment-involving-nonvariable-quantities",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B[Perform the&lt;/br&gt;scientific assessment&lt;/br&gt;and uncertainty analysis&lt;/br&gt;together] \n  B --&gt; C[Do you want&lt;/br&gt;to try the simpler&lt;/br&gt;option of&lt;/br&gt;using only bounded&lt;/br&gt;probabilities?]\n  C --Yes--&gt;D[Quantify uncertainty&lt;/br&gt;for each part of&lt;/br&gt;the uncertainty analysis,&lt;/br&gt;using probability bounds for&lt;/br&gt;both uncertainty and variability]\n  D--&gt; F[Combine the parts&lt;/br&gt;by probability bounds&lt;/br&gt; calculation]\n  C --No --&gt;E[Quantify uncertainty&lt;/br&gt;for each part of&lt;/br&gt;the uncertainty analysis,&lt;/br&gt;using 2D distributions&lt;/br&gt;for variable quantities]\n  E --&gt; G[Combine the parts&lt;/br&gt;by 2D Monte Carlo&lt;/br&gt;simulation]\n    subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    H[Take account&lt;/br&gt;of the contribution&lt;/br&gt;of any additional&lt;/br&gt;uncertainties] --&gt; I[Check for&lt;/br&gt;and describe any&lt;/br&gt;unquantified uncertainties]\n  end\n  F & G --&gt; ov\n  ov--&gt; J[[Report conclusion&lt;/br&gt;in form needed&lt;/br&gt;by decision makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion&lt;/br&gt;or annex]]\n  click A \"figure5.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/reporting.html",
    "href": "uncertainty-analysis/reporting.html",
    "title": "Reporting uncertainty analysis",
    "section": "",
    "text": "Requirements on reporting\n\n\n\n\n\nUncertainty analysis is part of scientific assessment; so in all cases, it should be reported in a manner consistent with EFSA’s general principles regarding transparency and reporting. In particular, it is important to list the sources of uncertainty that have been identified and document how they were identified, how each source of uncertainty has been evaluated and how they have been combined, where and how data and expert judgement have been used, what methodological approaches have been used (including models of any type) and the rationale for choosing them, and what the results were. Where the assessment used methods that are already described in other documents, it is sufficient to refer to those.\nThe uncertainty analysis should be reported in all assessments with two exceptions:\nFor standard assessments where no case-specific sources of uncertainty have been identified, the EFSA output must at minimum state what standardised procedure was followed and report that non-standard uncertainties were checked for and none were found.\nIf non-standard uncertainties are found, the assessors should report that standard uncertainties in the assessment are accepted to be covered by the standardised procedure and the uncertainty analysis is therefore restricted to non-standard uncertainties that are particular to this assessment, the analysis of which should then be reported as described below.\nThe level of detail can be reduced due to time constraints in urgent assessments.\n\n\n\n\n\n\n\n\n\nWhere to report the uncertainty analysis\n\n\n\n\n\nThe location of information on the uncertainty analysis within the assessment report should be chosen to maximise transparency and accessibility for readers. This may be facilitated by including one or more separate sections on uncertainty analysis, which are identifiable in the table of contents.\n\n\n\n\n\n\n\n\n\nHow to report the uncertainty analysis\n\n\n\n\n\nThe Scientific Committee has stated that EFSA’s scientific assessments must report clearly and unambiguously what sources of uncertainty have been identified and characterise their overall impact on the assessment conclusion, in a form compatible with the requirements of decision-makers and any legislation applicable to the assessment in hand. See Ch 15 in the Uncertainty Analysis Guidance for examples.\nIn other cases, where the form for reporting conclusions is not specified by decision-makers or legislation, the assessment conclusion should include (a) a clear statement of the overall result for those uncertainties that have been quantified and (b) a clear description of unquantified sources of uncertainty, i.e. those that could not be included in the quantitative analysis.\nThe former will generally express the overall quantified uncertainty about the assessment conclusion using probabilities, probability distributions, probability bounds, or ranges from the approximate probability scale.\nFor each unquantified source of uncertainty, the assessors should describe (either in the conclusion or another section, as appropriate) which part(s) of the assessment it arises in, the cause or reason for it, how it affects the assessment (but not how much), why it is difficult to quantify, what assumptions have been made about it in the assessment and what could be done to reduce or better characterise it.\nAssessors must avoid using any words that imply a judgement about the magnitude or likelihood of the unquantified sources of uncertainty.\n\n\n\n\n\n\n\n\n\nPrepare reporting for the summary section\n\n\n\n\n\nIn addition to the detailed reporting of the methods and results of the uncertainty analysis, the assessors should prepare a concise summary of the overall characterisation of uncertainty in format and style suitable for inclusion in the executive summary of the assessment report. This should present, in the simplest terms possible, a quantitative expression of the combined effect on the assessment conclusion of those uncertainties that have been quantified, and a brief description of any unquantified sources of uncertainty.\n\n\n\n\n\n\n\n\n\nBe consistent\n\n\n\n\n\nAssessors must check that there is no incompatibility between the reporting of the uncertainty analysis and the assessment conclusions."
  },
  {
    "objectID": "uncertainty-analysis/dividing.html",
    "href": "uncertainty-analysis/dividing.html",
    "title": "Dividing the assessment into parts",
    "section": "",
    "text": "Often an assessment will comprise a number of main parts (e.g. exposure and hazard in a chemical risk assessment) and smaller, subsidiary parts (e.g. individual parameters, studies, or lines of evidence within the exposure or hazard assessment).\nThe uncertainty analysis may also be divided into parts. Assessors should choose at what level to conduct it:\n\nEvaluate all uncertainties collectively, for the assessment as a whole.\nDivide the uncertainty analysis into parts, which evaluate uncertainties separately in major parts of the scientific assessment (e.g. exposure and hazard in a risk assessment). Then, combine the parts of the uncertainty analysis and include also any other identified uncertainties that relate to other parts of the scientific assessment as a whole, so as to characterise the overall uncertainty.\nDivide the uncertainty analysis into still smaller parts, corresponding to still smaller parts of the scientific assessment (e.g. every input of a calculation or model). Evaluate uncertainty collectively within each of the smaller parts, combine them into the main parts, and combine those to characterise overall uncertainty for the whole assessment.\n\nIf the uncertainty analysis will be divided into parts, assessors will need to combine them to characterise overall uncertainty. Assessors should define in advance how the parts will be combined, as this will increase transparency and rigour. It is recommended to use a conceptual model diagram (see glossary for explanation) to show how the parts will be combined. The parts may be combined by expert judgement (Section 12.6), or by calculation (Sections 13, 14 or 15) if assessors quantify the uncertainty for each part and can specify an appropriate quantitative or logical model to combine them. Calculation is likely to give more reliable results, but should be weighed against the additional work involved.\nAssessors should judge what is best suited to the needs of each assessment. For example, it may be more efficient to evaluate uncertainty for different parts separately if they require different expertise (e.g. toxicity and exposure). Evaluating all uncertainties collectively (first option in point (2) above) is generally quicker and superficially simpler but requires integrating them all subjectively by expert judgement, which may be less reliable than evaluating different parts of the uncertainty analysis separately, if they are then combined by calculation. For this reason, it is recommended to treat separately those parts of the assessment that are affected by larger uncertainties (identified by a simple initial prioritisation, see Section 8).\nWhen a part of the scientific assessment is treated separately in the uncertainty analysis, it is not necessary to evaluate immediately all of the uncertainties affecting it; some of them can be set to one side and considered later as part of the overall characterisation of uncertainty, if this is more convenient for the assessor. However, it is recommended that only the lesser uncertainties are deferred to the overall characterisation, since it will be more reliable to combine the larger uncertainties by calculation.\nWhen the scientific assessment includes a quantitative or logical model, assessors may find it convenient to quantify uncertainty separately for every parameter of the model. In such cases, it will still be necessary to identify additional uncertainties that are not quantified within the model, e.g. uncertainties relating to the structure of the model (see Section 7.2) and take them into account in the characterisation of overall uncertainty (Section 16). In other cases, assessors might find it sufficient to analyse all the uncertainties affecting a model collectively (simplest option in point (2) above), or for major parts of the model without separating the individual parameters (intermediate option in point (2))."
  },
  {
    "objectID": "uncertainty-analysis/figure7.html",
    "href": "uncertainty-analysis/figure7.html",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B[Perform the&lt;/br&gt;scientific assessment&lt;/br&gt;and uncertainty analysis&lt;/br&gt;together] \n  B --&gt; K[Quantify uncertainty&lt;/br&gt;for each part of&lt;/br&gt;the uncertainty analysis]\n  B --&gt; C[Evaluate uncertainty&lt;/br&gt;for each part,&lt;/br&gt;qualitatively or quantitatively]\n  K --&gt; L[Combine the parts&lt;/br&gt;using a suitable&lt;/br&gt;logic model]\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Decide how&lt;/br&gt;to express&lt;/br&gt;the uncertainty] --&gt; E[Elicit&lt;/br&gt;a probability judgement&lt;/br&gt;for the overall&lt;/br&gt;uncertainty]\n    E --&gt; F[Check for&lt;/br&gt;and describe&lt;/br&gt;any unquantified&lt;/br&gt;uncertainties]\n    M[Take account&lt;/br&gt;of the contribution&lt;/br&gt;of any additional&lt;/br&gt;uncertainties] --&gt; N[Check for&lt;/br&gt;and describe any&lt;/br&gt;unquantified uncertainties]\n  end\n  L--&gt;M\n    C --&gt; D\n    F --&gt; G[[Report conclusion&lt;/br&gt;in form needed&lt;/br&gt;by decision makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion&lt;/br&gt;or annex]]\n    N --&gt; G  \n  click A \"figure5.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure3.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure7.html#combining-multiple-yesno-questions-by-expert-judgement-or-calculation",
    "href": "uncertainty-analysis/figure7.html#combining-multiple-yesno-questions-by-expert-judgement-or-calculation",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B[Perform the&lt;/br&gt;scientific assessment&lt;/br&gt;and uncertainty analysis&lt;/br&gt;together] \n  B --&gt; K[Quantify uncertainty&lt;/br&gt;for each part of&lt;/br&gt;the uncertainty analysis]\n  B --&gt; C[Evaluate uncertainty&lt;/br&gt;for each part,&lt;/br&gt;qualitatively or quantitatively]\n  K --&gt; L[Combine the parts&lt;/br&gt;using a suitable&lt;/br&gt;logic model]\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Decide how&lt;/br&gt;to express&lt;/br&gt;the uncertainty] --&gt; E[Elicit&lt;/br&gt;a probability judgement&lt;/br&gt;for the overall&lt;/br&gt;uncertainty]\n    E --&gt; F[Check for&lt;/br&gt;and describe&lt;/br&gt;any unquantified&lt;/br&gt;uncertainties]\n    M[Take account&lt;/br&gt;of the contribution&lt;/br&gt;of any additional&lt;/br&gt;uncertainties] --&gt; N[Check for&lt;/br&gt;and describe any&lt;/br&gt;unquantified uncertainties]\n  end\n  L--&gt;M\n    C --&gt; D\n    F --&gt; G[[Report conclusion&lt;/br&gt;in form needed&lt;/br&gt;by decision makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion&lt;/br&gt;or annex]]\n    N --&gt; G  \n  click A \"figure5.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure3.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/prioritizing.html",
    "href": "uncertainty-analysis/prioritizing.html",
    "title": "Prioritizing uncertainties",
    "section": "",
    "text": "Prioritising sources of uncertainty may be useful at different stages of the assessment and uncertainty analysis. In the early stages, it can be used to select more important uncertainties to be analysed by more refined methods, e.g. to be evaluated individually rather than collectively, to be expressed with probabilities or distributions rather than bounds, to be elicited by more rather than less formal methods, etc. Prioritisation can also be used during the course of an assessment, to identify parts of the assessment where it might be beneficial to search for more data, use more complex models, or invite additional experts. At the end of the assessment, it may be useful to prioritise uncertainties to identify potential areas for further research.\nPrioritisation, at any stage of the assessment, should be based on the contribution of individual sources of uncertainty to the uncertainty of the assessment as a whole. This is determined by a combination of the magnitude of each uncertainty and how much it affects the result of the assessment, both of which need to be taken into account [SO5.7].\nThe relative influence of different uncertainties can be assessed in a simple and approximate way using qualitative methods based on expert judgement. An ordinal scale can be used to express expert judgements of the magnitude and/or direction of impact of each uncertainty on the question or quantity of interest, as in ‘uncertainty tables’ [SO10.5 and 10.6]. Or separate ordinal scales could be used to express judgements of the magnitude of each uncertainty and its influence, as in the Numeral, Unit, Spread, Assessment and Pedigree (NUSAP) approach [SO10.4].\nWhen the assessment involves a calculation or quantitative model, the contributions of uncertainties about the model inputs can be assessed rigorously by sensitivity analysis. These range from simple ‘what if’ calculations and ‘minimal assessment’ (EFSA, 2014a) to sophisticated sensitivity analyses [see SO12] for which specialist help might be required (Section 1.7). The influence of uncertainties relating to choices regarding the structure of the model or assessment may need to be addressed by repeating the assessment with alternative choices. Prioritisation at the early stages of an assessment must necessarily be done by expert judgement or by sensitivity analysis using a preliminary model, as the assessment model is still under development."
  },
  {
    "objectID": "uncertainty-analysis/overall.html",
    "href": "uncertainty-analysis/overall.html",
    "title": "Characterising overall uncertainty",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nUncertainty refers to all types of limitations in available knowledge that affect the answer to an assessment question.\nImplementing uncertainty analysis implies that the scientific assessment must say what sources of uncertainty have been identified and characterise their overall impact on the assessment conclusion.\nOverall uncertainty is in the EFSA guidance for uncertainty analysis referred to as the assessors’ uncertainty about the question, or quantity of interest at the time of reporting, taking account of the combined effect of all sources of uncertainty identified by the assessors as being relevant to the assessment.\nThe overall uncertainty must be reported clearly and unambiguously, in a form compatible with the requirements of decision-makers and any legislation applicable to the assessment at hand.\nAll EFSA scientific assessments require at least a basic analysis of uncertainty since uncertainty is an inherent component of scientific assessment and it is important to reflect it in the conclusions to allow risk managers to make informed decisions\nAssessors, not risk managers, are responsible for assessing the impact of uncertainties on EFSA conclusions. This view is adopted from the basic principles for addressing uncertainty in risk analysis as stated in the Codex Working Principles for Risk Analysis:\n\nConstraints, uncertainties and assumptions having an impact on the risk assessment should be explicitly considered at each step in the risk assessment and documented in a transparent manner by the risk assessors\nResponsibility for resolving the impact of uncertainty on the risk management decision lies with the risk manager, not the risk assessors.\n\nThis division is rational: assessing scientific uncertainty requires scientific expertise, while resolving the impact of uncertainty on decision-making involves weighing the scientific assessment against other considerations.\nWhen producing the Guidance for Uncertainty Analysis, the Scientific Committee considered arguments that uncertainty analysis is not relevant for some types of assessment but then concluded that it applies to all EFSA scientific assessments. You can read more about this in section 1.4 in the supporting opinion."
  },
  {
    "objectID": "uncertainty-analysis/overall.html#uncertainty-and-roles-when-characterising-it",
    "href": "uncertainty-analysis/overall.html#uncertainty-and-roles-when-characterising-it",
    "title": "Characterising overall uncertainty",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nUncertainty refers to all types of limitations in available knowledge that affect the answer to an assessment question.\nImplementing uncertainty analysis implies that the scientific assessment must say what sources of uncertainty have been identified and characterise their overall impact on the assessment conclusion.\nOverall uncertainty is in the EFSA guidance for uncertainty analysis referred to as the assessors’ uncertainty about the question, or quantity of interest at the time of reporting, taking account of the combined effect of all sources of uncertainty identified by the assessors as being relevant to the assessment.\nThe overall uncertainty must be reported clearly and unambiguously, in a form compatible with the requirements of decision-makers and any legislation applicable to the assessment at hand.\nAll EFSA scientific assessments require at least a basic analysis of uncertainty since uncertainty is an inherent component of scientific assessment and it is important to reflect it in the conclusions to allow risk managers to make informed decisions\nAssessors, not risk managers, are responsible for assessing the impact of uncertainties on EFSA conclusions. This view is adopted from the basic principles for addressing uncertainty in risk analysis as stated in the Codex Working Principles for Risk Analysis:\n\nConstraints, uncertainties and assumptions having an impact on the risk assessment should be explicitly considered at each step in the risk assessment and documented in a transparent manner by the risk assessors\nResponsibility for resolving the impact of uncertainty on the risk management decision lies with the risk manager, not the risk assessors.\n\nThis division is rational: assessing scientific uncertainty requires scientific expertise, while resolving the impact of uncertainty on decision-making involves weighing the scientific assessment against other considerations.\nWhen producing the Guidance for Uncertainty Analysis, the Scientific Committee considered arguments that uncertainty analysis is not relevant for some types of assessment but then concluded that it applies to all EFSA scientific assessments. You can read more about this in section 1.4 in the supporting opinion."
  },
  {
    "objectID": "uncertainty-analysis/overall.html#assessment-of-overall-uncertainty",
    "href": "uncertainty-analysis/overall.html#assessment-of-overall-uncertainty",
    "title": "Characterising overall uncertainty",
    "section": "Assessment of Overall Uncertainty",
    "text": "Assessment of Overall Uncertainty\n\nVideoText\n\n\n\n\n\n\nSo how is overall uncertainty assessed?\nThe guidance includes tiered options for quantifying overall uncertainty, from simple to refined.\nIn some assessments, it may be sufficient to characterise overall uncertainty for the whole assessment directly. This is done by expert judgement.\nIn other cases, it may be preferable to evaluate uncertainty for some or all parts of the assessment separately and then combine them. This can be done by calculation or by expert judgement.\nAssessing overall uncertainty directly is sometimes the quickest, least complex, and most approximate option. The other alternative, to assess overall uncertainty by breaking the assessment into parts and then combine them is sometimes more complex and take more time, but is on the other hand more reliable.\nIn practice, even when statistical and mathematical models are used, there will nearly always be some additional uncertainties that need to be taken into account when assessing overall uncertainty. This is done by expert judgement.\nNote that it might happen, that assessors judge all additional sources of uncertainty would make no difference. In that case, the basis for that judgement must be documented and justified.\nWhen assessors are unable to quantify some of the identified uncertainties affecting an assessment, it is essential that they describe them qualitatively and report this together with their quantitative expression of overall uncertainty, as the latter will then be conditional on assumptions made in the assessment regarding the sources of uncertainty that were not quantified.\nAll the flow charts in the Guidance include an assessment of overall uncertainty as a necessary step in all types of assessment, except for standard assessments with no non-standard uncertainties.\nAssessors should always try to express overall uncertainty quantitatively. The reason is that it avoids the ambiguity of qualitative expression and therefore provides better information for decision-making. (SO 4.2, SO 5.11))\nThe Guidance recognises that firm conclusions (without probabilities) are required in some types of assessment and describes how to assess overall uncertainty in this case.\nThe GD includes practical advice for quantifying overall uncertainty (UA GD 16.1])\nThe conclusion is conditional on any sources of uncertainty that have and have not been included in the quantification of overall uncertainty.\nConditionality has important implications for decision-making, because it means the assessment conclusion is valid only if the assumptions on which it is conditional are valid. It is therefore important to report clearly what sources of uncertainty that have - and have not - been taken into account, and what assumptions are made about those that have not been quantified.\nNote that, non-identified sources of uncertainty, or so-called ‘unknown unknowns’, are outside the scope of EFSA uncertainty analysis.\nWe hope this introduction was helpful and that you get the chance to characterise overall uncertainty in a near future!"
  },
  {
    "objectID": "uncertainty-analysis/identifying.html",
    "href": "uncertainty-analysis/identifying.html",
    "title": "Identifying uncertainties",
    "section": "",
    "text": "Standard uncertainties are those that are considered (implicitly or explicitly) to be addressed by the provisions of a standardised procedure or standardised assessment element. For example, uncertainties due to within and between species differences in toxicity are often addressed by a default factor of 100 in chemical risk assessment.\nAll other uncertainties are non-standard uncertainties. These include any deviations from a standardised procedure or standardised assessment element that lead to uncertainty regarding the result of the procedure.\n\nBoth standard and non-standard uncertainties may be found in any type of assessment, but the proportions vary. It is recommended that EFSA’s Panels include lists of standard uncertainties within the documentation for standard procedures, as this will help assessors to distinguish standard and non-standard uncertainties."
  },
  {
    "objectID": "uncertainty-analysis/identifying.html#standard-and-non-standard-uncertainties",
    "href": "uncertainty-analysis/identifying.html#standard-and-non-standard-uncertainties",
    "title": "Identifying uncertainties",
    "section": "",
    "text": "Standard uncertainties are those that are considered (implicitly or explicitly) to be addressed by the provisions of a standardised procedure or standardised assessment element. For example, uncertainties due to within and between species differences in toxicity are often addressed by a default factor of 100 in chemical risk assessment.\nAll other uncertainties are non-standard uncertainties. These include any deviations from a standardised procedure or standardised assessment element that lead to uncertainty regarding the result of the procedure.\n\nBoth standard and non-standard uncertainties may be found in any type of assessment, but the proportions vary. It is recommended that EFSA’s Panels include lists of standard uncertainties within the documentation for standard procedures, as this will help assessors to distinguish standard and non-standard uncertainties."
  },
  {
    "objectID": "uncertainty-analysis/identifying.html#procedure-for-identifying-uncertainties",
    "href": "uncertainty-analysis/identifying.html#procedure-for-identifying-uncertainties",
    "title": "Identifying uncertainties",
    "section": "Procedure for identifying uncertainties",
    "text": "Procedure for identifying uncertainties\n\nEvery assessment must say what sources of uncertainty have been identified.\nAssessors should systematically examine every part of their assessment for uncertainties, including both the inputs to the assessment (e.g. data, estimates, other evidence) and the methods used in the assessment (e.g. statistical methods, calculations or models, reasoning, expert judgement), to minimise the risk that important uncertainties are overlooked.\nUncertainties affecting assessment inputs are identified when appraising the evidence retrieved from literature or from existing databases.\nUncertainties affecting the methods used in the assessment are generally not addressed by existing frameworks for evidence appraisal. It is therefore recommended that assessors use the right column of Table 1 (referring to [SO8.1] for details and explanation) as a guide to what types of uncertainty to look for in the methods of their assessment.\nAssessors are advised to avoid spending excessive time trying to match uncertainties to the types listed in Table 1 or other frameworks: the purpose of the lists is to facilitate identification of uncertainties, not to classify them.\nAssessors should determine which of the uncertainties they identify in an assessment are standard and which are non-standard (Section 7.1), as this will affect their treatment in subsequent stages of the uncertainty analysis."
  },
  {
    "objectID": "uncertainty-analysis/figure3.html",
    "href": "uncertainty-analysis/figure3.html",
    "title": "Standardized Assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Take account&lt;/br&gt;of the contribution&lt;/br&gt;of any additional uncertainties] --&gt; E[Check for and&lt;/br&gt;describe any &lt;/br&gt;unquantified uncertainties]\n  end\n  A([&lt; BACK]) --&gt; B[Divide the uncertainty analysis&lt;/br&gt;into convenient parts,&lt;/br&gt;define the question or&lt;/br&gt;quantity of interest &lt;/br&gt;for each part, and&lt;/br&gt;an appropriate calculation&lt;/br&gt;for combining the parts]\n  B --&gt; C[Elicit probability&lt;/br&gt; bounds for each part,&lt;/br&gt; and combine probability&lt;/br&gt;bounds by calculation]\n  C --&gt; ov\n  ov --&gt; F{Is the result&lt;/br&gt;expected to be sufficient&lt;/br&gt;for decision-making?}\n  F -- Yes--&gt; G[[Report conclusion&lt;/br&gt;in form needed by&lt;/br&gt;decision-makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion or annex]]\n  F -- No --&gt; Z([NEXT &gt;])\n  click A \"figure2.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure4.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure3.html#assessing-non-standard-uncertainties-for-separate-parts-of-the-uncertainty-analysis-using-probability-bounds",
    "href": "uncertainty-analysis/figure3.html#assessing-non-standard-uncertainties-for-separate-parts-of-the-uncertainty-analysis-using-probability-bounds",
    "title": "Standardized Assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    D[Take account&lt;/br&gt;of the contribution&lt;/br&gt;of any additional uncertainties] --&gt; E[Check for and&lt;/br&gt;describe any &lt;/br&gt;unquantified uncertainties]\n  end\n  A([&lt; BACK]) --&gt; B[Divide the uncertainty analysis&lt;/br&gt;into convenient parts,&lt;/br&gt;define the question or&lt;/br&gt;quantity of interest &lt;/br&gt;for each part, and&lt;/br&gt;an appropriate calculation&lt;/br&gt;for combining the parts]\n  B --&gt; C[Elicit probability&lt;/br&gt; bounds for each part,&lt;/br&gt; and combine probability&lt;/br&gt;bounds by calculation]\n  C --&gt; ov\n  ov --&gt; F{Is the result&lt;/br&gt;expected to be sufficient&lt;/br&gt;for decision-making?}\n  F -- Yes--&gt; G[[Report conclusion&lt;/br&gt;in form needed by&lt;/br&gt;decision-makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion or annex]]\n  F -- No --&gt; Z([NEXT &gt;])\n  click A \"figure2.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure4.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure5.html",
    "href": "uncertainty-analysis/figure5.html",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nThe Guidance on uncertainty analysis covers different types of assessment.\nA case-specific assessment is needed when there is no standardised procedure for the type of assessment at hand, so the assessors have to develop a customized assessment plan.\nCertain standardised elements such as, for example, default values, may be used for some parts of the assessment. But for other parts, case-specific approaches may be required.\nIn case-specific assessments, the approach to uncertainty analysis should be considered from the very start, as part of assessment planning phase.\nThe uncertainty analysis should start at the level that is appropriate to the assessment in hand. For assessments where data for quantifying uncertainty is available or where suitable quantitative methods are established, they may be included in the initial assessment. Otherwise, it may be best to start with a simple approach, unless it is evident at the outset that more complex approaches are needed.\nThe first step is uncertainty identification. Try to search for uncertainties systematically in all parts of your assessment.\nWhen this is done, you can proceed to assessing the identified uncertainties, preferably step by step, by dividing the uncertainty analysis into convenient parts, or collectively in a single step.\nIf you decide to divide the uncertainty analysis into parts, you should think about the question or quantity of interest for each part, and an appropriate conceptual model for combining the parts. If you decide to handle all uncertainties in a single step, you will be doing the characterisation of overall uncertainty.\nThese options allow uncertainty analysis to be implemented at different level of detail or for refinement suitable for different timescales and levels of resource."
  },
  {
    "objectID": "uncertainty-analysis/figure5.html#introduction-to-case-specific-assessments",
    "href": "uncertainty-analysis/figure5.html#introduction-to-case-specific-assessments",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nThe Guidance on uncertainty analysis covers different types of assessment.\nA case-specific assessment is needed when there is no standardised procedure for the type of assessment at hand, so the assessors have to develop a customized assessment plan.\nCertain standardised elements such as, for example, default values, may be used for some parts of the assessment. But for other parts, case-specific approaches may be required.\nIn case-specific assessments, the approach to uncertainty analysis should be considered from the very start, as part of assessment planning phase.\nThe uncertainty analysis should start at the level that is appropriate to the assessment in hand. For assessments where data for quantifying uncertainty is available or where suitable quantitative methods are established, they may be included in the initial assessment. Otherwise, it may be best to start with a simple approach, unless it is evident at the outset that more complex approaches are needed.\nThe first step is uncertainty identification. Try to search for uncertainties systematically in all parts of your assessment.\nWhen this is done, you can proceed to assessing the identified uncertainties, preferably step by step, by dividing the uncertainty analysis into convenient parts, or collectively in a single step.\nIf you decide to divide the uncertainty analysis into parts, you should think about the question or quantity of interest for each part, and an appropriate conceptual model for combining the parts. If you decide to handle all uncertainties in a single step, you will be doing the characterisation of overall uncertainty.\nThese options allow uncertainty analysis to be implemented at different level of detail or for refinement suitable for different timescales and levels of resource."
  },
  {
    "objectID": "uncertainty-analysis/figure5.html#options-for-case-specific-assessment",
    "href": "uncertainty-analysis/figure5.html#options-for-case-specific-assessment",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "Options for case-specific assessment",
    "text": "Options for case-specific assessment\n\nVideoText\n\n\n\n\n\n\nGuidance for handling the overall uncertainty in one step or by breaking the assessment into parts is provided by the series of flowcharts. When following the flowcharts, see if all parts of an assessment relate to yes/no questions if there are parts relating to variable quantities and what expression of uncertainty that is used.\nIn all cases, the characterisation of overall uncertainty must integrate the contributions of identified sources of uncertainties that have been expressed in different ways.\nA case-specific uncertainty analysis should be refined as far as is needed to inform decision-making. This point is reached either when there is sufficient certainty about the question or quantity of interest for the decision-makers to make a decision with the level of certainty they require, or if it becomes apparent that achieving the desired level of uncertainty is unfeasible or too costly and the decision-makers decide instead to manage the uncertainty without further refinement of the analysis.\nWe recommend familiarising yourself with the flowcharts by clicking on them in the tutorial. Note that there is more detailed information in the guidance for uncertainty analysis in the text and footnotes accompanying each flowchart.\n\n\n\n\n\n\n\n\n%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TD\n  A([&lt; BACK]) --&gt;  B[Plan your&lt;/br&gt;scientific assessment&lt;/br&gt;in your usual manner]\n  B --&gt; C[Identify uncertainties&lt;/br&gt;systematically in all&lt;/br&gt;parts of your assessment]\n  C --&gt; D{Do you want&lt;/br&gt;to evaluate&lt;/br&gt;all uncertainties&lt;/br&gt;collectively in&lt;/br&gt;a single step?}\n  D--Yes--&gt; E([NEXT &gt;])\n  D--No--&gt; F[Divide the uncertainty analysis&lt;/br&gt;into convenient parts,&lt;/br&gt;define the question&lt;/br&gt;or quantity of interest&lt;/br&gt;for each part,&lt;/br&gt;and an appropriate conceptual&lt;/br&gt;model for combining the parts]\n  F--&gt;G{Do all parts&lt;/br&gt;relate to&lt;/br&gt;yes/no questions?}\n  G--Yes--&gt;H([NEXT &gt;])\n  G--No--&gt; I{Do any part&lt;/br&gt;relate to&lt;/br&gt;variable quantities}\n  I --Yes--&gt; K([NEXT &gt;])\n  I --No--&gt; L([NEXT &gt;])\n  click A \"../uncertaintyanalysis.html\" \"Go Back\" _self\n  click E \"figure6.html\" \"Go Forward\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click H \"figure7.html\" \"Go Forward\" _self\n  click L \"figure9.html\" \"Go Forward\" _self\n  click K \"figure10.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,E,H,L,K dark"
  },
  {
    "objectID": "uncertainty-analysis/well-defined.html",
    "href": "uncertainty-analysis/well-defined.html",
    "title": "Ensuring that the questions or quantities of interest are well-defined",
    "section": "",
    "text": "In order to evaluate uncertainty, the questions and/or quantities of interest for the assessment must be well-defined. This applies both to the assessment as a whole and to different parts of the uncertainty analysis, if it is separated into parts. Any ambiguity in the definition of questions or quantities of interest will add extra uncertainty and make the evaluation more difficult. When a question or quantity of interest is not already well-defined for the purpose of scientific assessment, assessors should define it well for the purpose of uncertainty analysis.\nA quantity or question of interest is well-defined if, at least in principle, it could be determined in such a way that assessors would be sure to agree on the answer. A practical way to achieve this is by specifying an experiment, study or procedure that could be undertaken, at least in principle, and which would determine the true answer for the question or quantity with certainty [see SO5.1 for more discussion]. For example:\n\na well-defined measure for a quantity of interest, and the time, population or location, and conditions (e.g. status quo or with specified management actions) for which the measure will be considered;\nfor a question of interest, the presence or absence of one or more clearly-defined states, conditions, mechanisms, etc., of interest for the assessment, and the time, population or location, and conditions (e.g. status quo or with specified management actions) for which this will be considered;\nthe result of a clearly-defined scientific study, procedure or calculation, which is established (e.g. in legislation or guidance) as being relevant for the assessment.\n\nWhen drafting the definition of each question or quantity of interest, check each word in turn. Identify words that are ambiguous (e.g. high), or imply a risk management judgement (e.g. negligible, safe). Replace or define them with words that are, as far as possible, unambiguous and free of risk management connotations or, where appropriate, with numbers.\nSometimes the Terms of Reference for an assessment are very open, e.g. requesting a review of the literature on an area of science. In such cases, assessors should seek to ensure the conclusions they produce either refer to well-defined quantities, or contain well-defined statements that can be considered as answers to well-defined questions, in one of the three forms listed above (point 2, options a–c). This is necessary both for transparency and so that assessors can evaluate and express the uncertainty associated with their conclusions."
  },
  {
    "objectID": "uncertainty-analysis/figure9.html",
    "href": "uncertainty-analysis/figure9.html",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B[Perform the&lt;/br&gt;scientific assessment&lt;/br&gt;and uncertainty analysis&lt;/br&gt;together] \n  B --&gt; C[Do you want&lt;/br&gt;to try the simpler&lt;/br&gt;option of&lt;/br&gt;using only bounded&lt;/br&gt;probabilities?]\n  C --Yes--&gt;D[Obtain probability&lt;/br&gt;bounds for each&lt;/br&gt;part of the&lt;/br&gt;uncertainty analysis]\n  D--&gt; F[Combine the parts&lt;/br&gt;by probability bounds&lt;/br&gt; calculation]\n  C --No --&gt;E[Obtain a probability&lt;/br&gt;or distribution for&lt;/br&gt;each part of the&lt;/br&gt;uncertainty analysis]\n  E --&gt; G[Combine the parts&lt;/br&gt;by 1D Monte Carlo&lt;/br&gt;simulation]\n    subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    H[Take account&lt;/br&gt;of the contribution&lt;/br&gt;of any additional&lt;/br&gt;uncertainties] --&gt; I[Check for&lt;/br&gt;and describe any&lt;/br&gt;unquantified uncertainties]\n  end\n  F & G --&gt; ov\n  ov--&gt; J[[Report conclusion&lt;/br&gt;in form needed&lt;/br&gt;by decision makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion&lt;/br&gt;or annex]]\n  click A \"figure5.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure3.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/figure9.html#evaluating-uncertainties-separately-for-different-parts-of-an-assessment-involving-nonvariable-quantities",
    "href": "uncertainty-analysis/figure9.html#evaluating-uncertainties-separately-for-different-parts-of-an-assessment-involving-nonvariable-quantities",
    "title": "Uncertainty analysis for case-specific assessments",
    "section": "",
    "text": "%%{init:{\"theme\":\"base\", \"themeVariables\": {\"primaryColor\":\"white\", \"secondaryColor\":\"white\", \"tertiaryColor\":\"white\", \"mainBkg\":\"#f2f2f2\", \"nodeBorder\":\"#7F7F7F\", \"clusterBkg\":\"#FFF9FB\"}}}%%\nflowchart TB\n  A([&lt; BACK]) --&gt; B[Perform the&lt;/br&gt;scientific assessment&lt;/br&gt;and uncertainty analysis&lt;/br&gt;together] \n  B --&gt; C[Do you want&lt;/br&gt;to try the simpler&lt;/br&gt;option of&lt;/br&gt;using only bounded&lt;/br&gt;probabilities?]\n  C --Yes--&gt;D[Obtain probability&lt;/br&gt;bounds for each&lt;/br&gt;part of the&lt;/br&gt;uncertainty analysis]\n  D--&gt; F[Combine the parts&lt;/br&gt;by probability bounds&lt;/br&gt; calculation]\n  C --No --&gt;E[Obtain a probability&lt;/br&gt;or distribution for&lt;/br&gt;each part of the&lt;/br&gt;uncertainty analysis]\n  E --&gt; G[Combine the parts&lt;/br&gt;by 1D Monte Carlo&lt;/br&gt;simulation]\n    subgraph ov [Characterisation of overall uncertainty]\n    direction LR\n    H[Take account&lt;/br&gt;of the contribution&lt;/br&gt;of any additional&lt;/br&gt;uncertainties] --&gt; I[Check for&lt;/br&gt;and describe any&lt;/br&gt;unquantified uncertainties]\n  end\n  F & G --&gt; ov\n  ov--&gt; J[[Report conclusion&lt;/br&gt;in form needed&lt;/br&gt;by decision makers,&lt;/br&gt;and detailed analysis&lt;/br&gt;in opinion&lt;/br&gt;or annex]]\n  click A \"figure5.html\" \"Go Back\" _self\n  click B callback \"See Section 7.1.\"\n  click C callback \"See Section 17 regarding reporting.\"\n  click Z \"figure3.html\" \"Go Forward\" _self\n  classDef dark fill:#0d5caa,stroke:#0d5caa,stroke-width:0px,color:#fff\n  class A,Z dark"
  },
  {
    "objectID": "uncertainty-analysis/characterising-and-combining.html",
    "href": "uncertainty-analysis/characterising-and-combining.html",
    "title": "Characterising uncertainty for parts of the uncertainty analysis and combining them",
    "section": "",
    "text": "Characterising uncertainty for parts of the uncertainty analysis is needed for assessments where the assessors choose to divide the uncertainty analysis into parts, but may only be done for some of the parts, with the other parts being considered when characterising overall uncertainty.\nWhen characterising overall uncertainty assessors should initially include all sources of uncertainty that might be relevant, not only those they are sure are relevant. This is necessary to minimise the risk of overlooking sources of uncertainty which, while initially of doubtful significance, may prove on further analysis to be important.It is recommended to use a systematic approach for identifying uncertainties, to minimise the risk of overlooking important ones.\nIn many assessments, the number of potentially relevant sources of uncertainty identified may be large. All the sources of uncertainty that are identified must be recorded in a list. This is necessary to inform the assessors’ judgement of the overall uncertainty and ensure a transparent record of the assessment.\nWith the exception of standardised assessments, assessors should try to quantify the combined impact of as many as possible of the uncertainties on the question or quantity of interest.\nThere are three options for quantifying the overall uncertainty, depending on the context:\n\n\n\n\n\n\nOption 1\n\n\n\n\n\nMake a single judgement of the overall impact of all the identified uncertainties. This is applicable in assessments where the uncertainty analysis has not been divided into parts. Assessors should quantify the collective impact of as many as possible of the identified uncertainties directly by expert judgement, using formal or semi-formal EKE. If assessors find it too challenging to express their judgement of the overall uncertainty as a probability distribution (if continuous quantity of interest) or precise probability (if categorical quantity of interest), it may be sufficient to give an approximate probability.\n\n\n\n\n\n\n\n\n\nOption 2\n\n\n\n\n\nQuantify uncertainty separately in some parts of the assessment, combine them by calculation, and then adjust the result of the calculation by expert judgement to account for the additional uncertainties that are not yet included.\nThis is suitable in assessments where the uncertainty analysis has been divided into parts, and the assessors have quantified and combined at least some of the uncertainties in at least some parts of the assessment earlier in the uncertainty analysis.\nThe task that remains is to characterise the overall uncertainty, including those already quantified and the additional uncertainties that are not yet quantified. Some of the additional uncertainties may be uncertainties that were not included in the parts that were previously quantified, while others may relate to the model used for combining the parts. In this option, the contribution of the additional uncertainties is combined with the previously quantified uncertainties by expert judgement. Expert judgement is simpler, because it does not require explicit specification of a model for combining the uncertainties by calculation, but is more approximate because the combination must be done by subjective judgement.\n\n\n\n\n\n\n\n\n\nOption 3\n\n\n\n\n\nQuantify uncertainty separately in some parts of the assessment and combine them by calculation. Then quantify the contribution of the additional uncertainties separately, by expert judgement, and combine it with the previously quantified uncertainty by calculation.\nThis option involves judging the impact of the additional uncertainties as an additive or multiplicative factor on the scale of the quantity being assessed, expressed as a distribution or probability bound, and then combining this by calculation with the quantitative expression for uncertainties that were quantified and combined earlier in the assessment.\nThis is analogous to the well-established practice of using additional assessment factors to allow for additional sources of uncertainty. For example, EFSA endorses the use of case-by-case expert judgement to assign additional assessment factors to address uncertainties due to deficiencies in available data, extrapolation for duration of exposure, extrapolation from lowest observed adverse effect level (LOAEL) to no observed adverse effect level (NOAEL) and extrapolation from severe to less severe effects. However, the approach proposed here is more rigorous and transparent because it makes explicit the probability judgements that are implied when using such assessment factors.\nThis option seems less useful when the subject of assessment is a yes/no question. This is because it seems likely that experts would find it easiest to judge the necessary adjustment by thinking first about what the calculated probability needs to be adjusted to and then back-calculating, so there is no advantage over eliciting the adjusted probability directly.\n\n\n\nIn some assessments, there may be some identified sources of uncertainty that the assessors are unable to include in their quantitative expression of overall uncertainty. When this happens, the unquantified uncertainties must be characterised qualitatively and reported alongside the quantitative expression of uncertainty. The quantitative expression will then be conditional on the assumptions that have been made about these unquantified uncertainties. This has major implications for decision-making, so assessors should try to include as many uncertainties as possible in their quantitative expression.\n\n\n\n\n\nFigure 1: Options for characterising overall uncertainty"
  },
  {
    "objectID": "introduction.html#about",
    "href": "introduction.html#about",
    "title": "Introduction",
    "section": "About",
    "text": "About\nThis tutorial is provided within OC/EFSA/KNOW/2022/01 Lot 2 Trainings on horizontal scientific assessment methodologies EFSA training.\nThis page was created by Ullrika Sahlin and Dmytro Perepolkin, CEC, Lund University, Sweden."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EFSA Tutorial on Uncertainty",
    "section": "",
    "text": "Welcome to the\nEFSA tutorial on Uncertainty!\n\n\n\n\n\n\n\nWarning\n\n\n\nTHIS PAGE IS CURRENTLY UNDER CONSTRUCTION\n\n\n   \n\nSTART HERE\n\n \n\nThis tutorial is provided within OC/EFSA/KNOW/2022/01 Lot 2 Trainings on horizontal scientific assessment methodologies EFSA training."
  },
  {
    "objectID": "uncertainty-communication/expressions-unqualified.html",
    "href": "uncertainty-communication/expressions-unqualified.html",
    "title": "Unqualified conclusion",
    "section": "",
    "text": "Unqualified conclusion, with no expression of uncertainty\nThis occurs in two situations: - When a standardised assessment procedure only takes into account standard uncertainties, its outcome may be reported as an unqualified conclusion, without any expression of uncertainty (see UA, SO for more explanation) - When uncertainty is present in an assessment, but decision-makers or legislation requires an unqualified conclusion (e.g. safe, not safe or ‘cannot conclude’), without any expression of uncertainty. In some of these cases, uncertainty expressions may be included elsewhere in the assessment report, e.g. in the detailed results or in an annex"
  },
  {
    "objectID": "uncertainty-communication/expressions-2d.html",
    "href": "uncertainty-communication/expressions-2d.html",
    "title": "Two-dimensional probability distribution",
    "section": "",
    "text": "In this guidance, the term ‘two-dimensional (or 2D) probability distribution’ refers to a distribution that quantifies the uncertainty of a quantity that is variable, i.e. takes multiple true values (e.g. the exposure of different individuals in a population). This is most often plotted as a CDF or CCDF representing the median estimate of the variability, with uncertainty intervals quantifying uncertainty around the CDF or CCDF (Figure 1).\n\n\n\n\n\nFigure 1: Estimated cumulative distribution of ratio of exposure to the TDI for melamine, for 1-year-olds consuming contaminated chocolate from China"
  },
  {
    "objectID": "uncertainty-communication/expressions-approximate.html",
    "href": "uncertainty-communication/expressions-approximate.html",
    "title": "Approximate probability",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nAn approximate probability is one of the possible expressions of uncertainty that might result from an implementation of EFSAs guidance on uncertainty analysis.\nIt is a quantitative expression of uncertainty with partial information on the probability.\nHere, you can think of probability as your uncertainty about a binary outcome, such as your uncertainty about whether the answer to a yes/no question is “yes”.\nAn approximate probability is the result of specifying a range for your probability instead of a precise value.\nYou could for example say that you are at least 70% certain that the answer is yes.\nIt is not necessary to express probabilities fully or precisely, and in practice they will always be approximate to some degree.\nThere are also other reasons EFSA use approximate probabilities.\nOne is that uncertainty quantified by an approximate probability is sometimes sufficient for decision-making.\nAnother reason is that experts might find it easier to express their probability judgment as a range rather than a single value. This range act as an upper or lower bound on their probability judgment. For example, an assessment might say “there is less than 10% probability that the mean exposure exceeds 10 mg per kg of body weight a day”. Here the range is 0 to 10% probability.\nThe meaning of such a range is that it is judged that the probability would lie in the range if more time was taken to assess uncertainty and specify the probability precisely."
  },
  {
    "objectID": "uncertainty-communication/expressions-approximate.html#the-meaning-of-approximate-probability",
    "href": "uncertainty-communication/expressions-approximate.html#the-meaning-of-approximate-probability",
    "title": "Approximate probability",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nAn approximate probability is one of the possible expressions of uncertainty that might result from an implementation of EFSAs guidance on uncertainty analysis.\nIt is a quantitative expression of uncertainty with partial information on the probability.\nHere, you can think of probability as your uncertainty about a binary outcome, such as your uncertainty about whether the answer to a yes/no question is “yes”.\nAn approximate probability is the result of specifying a range for your probability instead of a precise value.\nYou could for example say that you are at least 70% certain that the answer is yes.\nIt is not necessary to express probabilities fully or precisely, and in practice they will always be approximate to some degree.\nThere are also other reasons EFSA use approximate probabilities.\nOne is that uncertainty quantified by an approximate probability is sometimes sufficient for decision-making.\nAnother reason is that experts might find it easier to express their probability judgment as a range rather than a single value. This range act as an upper or lower bound on their probability judgment. For example, an assessment might say “there is less than 10% probability that the mean exposure exceeds 10 mg per kg of body weight a day”. Here the range is 0 to 10% probability.\nThe meaning of such a range is that it is judged that the probability would lie in the range if more time was taken to assess uncertainty and specify the probability precisely."
  },
  {
    "objectID": "uncertainty-communication/expressions-approximate.html#efsas-approximate-probability-scale",
    "href": "uncertainty-communication/expressions-approximate.html#efsas-approximate-probability-scale",
    "title": "Approximate probability",
    "section": "EFSA’s Approximate probability scale",
    "text": "EFSA’s Approximate probability scale\n\nVideoText\n\n\n\n\n\n\nApproximate probabilities are used in EFSA’s approximate probability scale (Table 1).\nThis scale was introduced by EFSA to harmonise the use of qualitative verbal expressions for uncertainty.\nFor example, if uncertainty is quantified as being in the range 66 - 90% then the accompanying verbal expression should be “likely”.\nTo avoid ambiguity, the Guidance on communication of uncertainty recommends to always report the expression of uncertainty quantitatively as a probability or as an approximate probability.\nIf a verbal expression is also used, present the quantitative probability first, e.g. ’66–90% certain, and then “likely”, because it has been shown that this order leads to more consistent understanding, compared to if the verbal expression is presented first.\nAn approximate probability may comprise a range of probabilities chosen by the assessors from the approximate probability scale, or a different range of probabilities specified by the assessors.\nTo avoid inconsistency and misunderstanding, do not use the verbal terms in the approximate probability scale to refer to any probabilities or ranges of probabilities other than those shown in this table.\n\n\n\nThe probability ranges used in EFSA’s approximate probability scale (Table 1) are examples of approximate probability expressions. Assessors are not restricted to the ranges in the approximate probability scale and should use whatever ranges best reflect their judgement of the uncertainty (see UA)\n\n\n\n\nTable 1: Approximate probability scale\n\n\nProbability term\nSubjective probability range\nAdditional options\n\n\n\n\n\nAlmost certain\n99-100%\nMore likely than not: &gt;50%\nUnable to give any probability: range is 0-100%. Report as 'inconclusive', 'cannot conclude' or 'unknown'\n\n\nExtremely likely\n95-99%\n\n\nVery likely\n90-95%\n\n\nLikely\n66-90%\n\n\nAbout as likely as not\n33-66%\n\n\n\nUnlikely\n10-33%\n\n\nVery unlikely\n5-10%\n\n\nExtremely unlikely\n1-5%\n\n\nAlmost impossible\n0-1%"
  },
  {
    "objectID": "uncertainty-communication/expressions-approximate.html#eliciting-approximate-expert-judgments",
    "href": "uncertainty-communication/expressions-approximate.html#eliciting-approximate-expert-judgments",
    "title": "Approximate probability",
    "section": "Eliciting approximate expert judgments",
    "text": "Eliciting approximate expert judgments\n\nVideoText\n\n\n\n\n\n\nThe approximate probability scale is also suggested as an aid to elicit judgements from experts using Expert Knowledge Elicitation. EKE is defined by EFSA as “a systematic, documented and reviewable process to retrieve expert judgements from a group of experts, often in the form of a probability distribution”. The EKE guidance recommends three EKE protocols that have been developed to counter psychological biases and to manage the sharing and aggregation of judgements between experts.\nFormal elicitation requires significant time and resources, so it is not feasible to apply it to every source of uncertainty affecting an assessment. The supporting opinion of the Uncertainty analysis guidance describes how to modify the protocol using behavioural aggregation to fit the needs for using expert judgement in uncertainty analysis in an EFSA context. One modification is to allow for judgements expressed with approximate probabilities.\nThis is where the approximate probability scale can be used. The experts should be asked to select one or more categories from the table, to represent their probability judgement for the question of interest.\nIt is not intended that experts should be restricted to using the approximate probabilities in the table. On the contrary, they should be encouraged to specify other ranges, or precise probabilities, whenever these express better their judgement for the question or quantity under assessment.\nNote that the approximate probability scale is not a replacement of EKE, which means that the principles of EKE should be followed when using it. The supporting opinion of the uncertainty analysis guidance lists minimal requirements for a modified version of EKE (see requirements for semi-formal EKE [In written script add SO 9)."
  },
  {
    "objectID": "uncertainty-communication/expressions-approximate.html#combining-probability-bounds-or-approximate-probabilities",
    "href": "uncertainty-communication/expressions-approximate.html#combining-probability-bounds-or-approximate-probabilities",
    "title": "Approximate probability",
    "section": "Combining probability bounds or approximate probabilities",
    "text": "Combining probability bounds or approximate probabilities\n\nVideoText\n\n\n\n\n\n\nFinally, the toolbox for uncertainty analysis includes a method to combine approximate probabilities by calculation. Probability bound analysis can be used to combine probability bounds quantifying uncertainty in different parts of an assessment, to support the characterisation of overall uncertainty.\nAfter this introduction to approximate probability, we recommend you begin with the aspects that are most interesting for you, such as - how to communicate an approximate probability, how to make judgements expressed by approximate probability, or how to perform calculations with approximate probabilities."
  },
  {
    "objectID": "uncertainty-communication/expressions-inconclusive.html",
    "href": "uncertainty-communication/expressions-inconclusive.html",
    "title": "Inconclusive assessment",
    "section": "",
    "text": "This occurs in two situations:\n\nWhen decision-makers or legislation require an unqualified conclusion but assessors judge there is too much uncertainty to give one and report that they cannot conclude. The basis for this uncertainty expression should be documented in the body of the assessment report or an annex, and may include one or more uncertainty expressions\nWhen it is not required that conclusions must be unqualified, but the assessors are unable to give any quantitative expression of uncertainty or, where they judge that their probability for a conclusion could be anywhere between 0% and 100%. This should be accompanied by a qualitative description of the uncertainties (see description of a precise probability)\n\n\nVideoText\n\n\n\n\n\n\nAn assessment is inconclusive when the assessors are unable to give any quantitative expression of uncertainty or, where they judge that their probability for a conclusion could be anywhere between 0% and 100%.\nIn this instance, inconclusive implies that nothing further can be said about the conclusion. Therefore, the communication should avoid using language that might suggest otherwise. Include a description of the key sources of uncertainty that are responsible for the inconclusive assessment.\nIf applicable, also mention options or requirements for obtaining further data to - at best - reduce uncertainty in further assessment.\nAnother situation where assessment can be declared inconclusive is when decision-makers or legislation require a positive or negative conclusion without any expression of uncertainty. In the GD such conclusions are referred to as unqualified.\nBut as we all know, a scientific conclusion cannot be stated with absolute certainty.\nHowever, sometimes the certainty in the conclusion is “close enough” to 100% and therefore for practical purposes it can be declared as a yes.\n\n\n\n\n\nFigure 1: Region of practical certatinty\n\n\n\n\nThe threshold beyond which the probability can be rounded off to 0 or 100 % is called “practical certainty” in the Uncertainty Analysis Guidance and it should in principle be established by the decision makers (Figure 1).\nThe actual threshold for “practical certainty” may vary depending on the context, available decision options and their respective costs and benefits.\nWhen uncertainty in the conclusion is not within the margin of practical certainty, but decision-makers still require an unqualified conclusion then the assessment must be reported as inconclusive.\nIn this instance, “inconclusive” should be interpreted that the practical certainty has not been achieved.\nIn all cases, the basis for the inconclusive assessment should be documented in the assessment report."
  },
  {
    "objectID": "uncertainty-communication/expressions-inconclusive.html#communicating-an-inconclusive-assessment",
    "href": "uncertainty-communication/expressions-inconclusive.html#communicating-an-inconclusive-assessment",
    "title": "Inconclusive assessment",
    "section": "",
    "text": "This occurs in two situations:\n\nWhen decision-makers or legislation require an unqualified conclusion but assessors judge there is too much uncertainty to give one and report that they cannot conclude. The basis for this uncertainty expression should be documented in the body of the assessment report or an annex, and may include one or more uncertainty expressions\nWhen it is not required that conclusions must be unqualified, but the assessors are unable to give any quantitative expression of uncertainty or, where they judge that their probability for a conclusion could be anywhere between 0% and 100%. This should be accompanied by a qualitative description of the uncertainties (see description of a precise probability)\n\n\nVideoText\n\n\n\n\n\n\nAn assessment is inconclusive when the assessors are unable to give any quantitative expression of uncertainty or, where they judge that their probability for a conclusion could be anywhere between 0% and 100%.\nIn this instance, inconclusive implies that nothing further can be said about the conclusion. Therefore, the communication should avoid using language that might suggest otherwise. Include a description of the key sources of uncertainty that are responsible for the inconclusive assessment.\nIf applicable, also mention options or requirements for obtaining further data to - at best - reduce uncertainty in further assessment.\nAnother situation where assessment can be declared inconclusive is when decision-makers or legislation require a positive or negative conclusion without any expression of uncertainty. In the GD such conclusions are referred to as unqualified.\nBut as we all know, a scientific conclusion cannot be stated with absolute certainty.\nHowever, sometimes the certainty in the conclusion is “close enough” to 100% and therefore for practical purposes it can be declared as a yes.\n\n\n\n\n\nFigure 1: Region of practical certatinty\n\n\n\n\nThe threshold beyond which the probability can be rounded off to 0 or 100 % is called “practical certainty” in the Uncertainty Analysis Guidance and it should in principle be established by the decision makers (Figure 1).\nThe actual threshold for “practical certainty” may vary depending on the context, available decision options and their respective costs and benefits.\nWhen uncertainty in the conclusion is not within the margin of practical certainty, but decision-makers still require an unqualified conclusion then the assessment must be reported as inconclusive.\nIn this instance, “inconclusive” should be interpreted that the practical certainty has not been achieved.\nIn all cases, the basis for the inconclusive assessment should be documented in the assessment report."
  },
  {
    "objectID": "uncertainty-communication/expressions-direction.html",
    "href": "uncertainty-communication/expressions-direction.html",
    "title": "Qualitative description",
    "section": "",
    "text": "Words or an ordinal scale describing how much a source of uncertainty affects the assessment or its conclusion (e.g. low, medium or high uncertainty; conservative, very conservative or non-conservative; unlikely, likely or very likely; or symbols indicating the direction and magnitude of uncertainty: —, –,-,+, ++, +++) Because the meaning of such expressions is ambiguous, EFSA’s Uncertainty Analysis GD recommends that they should not be used unless they are accompanied by a quantitative definition (see UA)"
  },
  {
    "objectID": "uncertainty-communication/expressions-direction.html#qualitative-description-of-the-direction-andor-magnitude-of-uncertainty-using-words-or-symbols",
    "href": "uncertainty-communication/expressions-direction.html#qualitative-description-of-the-direction-andor-magnitude-of-uncertainty-using-words-or-symbols",
    "title": "Qualitative description",
    "section": "",
    "text": "Words or an ordinal scale describing how much a source of uncertainty affects the assessment or its conclusion (e.g. low, medium or high uncertainty; conservative, very conservative or non-conservative; unlikely, likely or very likely; or symbols indicating the direction and magnitude of uncertainty: —, –,-,+, ++, +++) Because the meaning of such expressions is ambiguous, EFSA’s Uncertainty Analysis GD recommends that they should not be used unless they are accompanied by a quantitative definition (see UA)"
  },
  {
    "objectID": "uncertainty-analysis.html",
    "href": "uncertainty-analysis.html",
    "title": "Uncertainty Analysis Guidance",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nWelcome to the Uncertainty analysis page\nThis page takes you through the guidance on uncertainty analysis for different steps and types of assessment.\nYou can access general guidance provided for different steps of an uncertainty analysis, from identifying uncertainties to reporting the results of an uncertainty analysis, using the sidebar menu on the left or links below.\n\nIdentifying uncertainties affecting the assessment\nPrioritising uncertainties within the assessment\nDividing the uncertainty analysis into parts\nEnsuring that the questions or quantities of interest are well-defined.\nCharacterising uncertainty for parts of the uncertainty analysis\nCombining uncertainty from different parts of the uncertainty analysis\nCharacterising overall uncertainty\nPrioritising uncertainties for future investigation.\nReporting uncertainty analysis\n\nThere is also specific guidance for different types of assessment. This guidance is provided through a series of flowcharts where you can learn about the recommended route for the type of assessment you are working with.\nYou access the flowcharts by clicking on one of the four assessment types.\nThere is further information below to help you identify the type that applies to your assessment.\nWhen you click on an assessment type, you will be taken to a flowchart. The flowchart guides you through the steps of an uncertainty analysis. It can also help you understand the options to make uncertainty analysis fit for purpose.\nThe EFSA guidance on uncertainty analysis is based on the principles and methods described in the scientific opinion supporting the guidance. You can learn more about the principles on the principles and methods page in this tutorial.\nNote that the text in this tutorial is a condensed extract from the EFSA guidance on Uncertainty Analysis. There is more detailed information in the guidance, in text and in footnotes.\nIf something is unclear or feels incomplete, or you want to learn more, we recommend you to consult the original document. Always use the original documents as your primary source.\nGood luck and happy cruising!"
  },
  {
    "objectID": "uncertainty-analysis.html#introduction-to-uncertainty-analysis",
    "href": "uncertainty-analysis.html#introduction-to-uncertainty-analysis",
    "title": "Uncertainty Analysis Guidance",
    "section": "",
    "text": "VideoText\n\n\n\n\n\n\nWelcome to the Uncertainty analysis page\nThis page takes you through the guidance on uncertainty analysis for different steps and types of assessment.\nYou can access general guidance provided for different steps of an uncertainty analysis, from identifying uncertainties to reporting the results of an uncertainty analysis, using the sidebar menu on the left or links below.\n\nIdentifying uncertainties affecting the assessment\nPrioritising uncertainties within the assessment\nDividing the uncertainty analysis into parts\nEnsuring that the questions or quantities of interest are well-defined.\nCharacterising uncertainty for parts of the uncertainty analysis\nCombining uncertainty from different parts of the uncertainty analysis\nCharacterising overall uncertainty\nPrioritising uncertainties for future investigation.\nReporting uncertainty analysis\n\nThere is also specific guidance for different types of assessment. This guidance is provided through a series of flowcharts where you can learn about the recommended route for the type of assessment you are working with.\nYou access the flowcharts by clicking on one of the four assessment types.\nThere is further information below to help you identify the type that applies to your assessment.\nWhen you click on an assessment type, you will be taken to a flowchart. The flowchart guides you through the steps of an uncertainty analysis. It can also help you understand the options to make uncertainty analysis fit for purpose.\nThe EFSA guidance on uncertainty analysis is based on the principles and methods described in the scientific opinion supporting the guidance. You can learn more about the principles on the principles and methods page in this tutorial.\nNote that the text in this tutorial is a condensed extract from the EFSA guidance on Uncertainty Analysis. There is more detailed information in the guidance, in text and in footnotes.\nIf something is unclear or feels incomplete, or you want to learn more, we recommend you to consult the original document. Always use the original documents as your primary source.\nGood luck and happy cruising!"
  },
  {
    "objectID": "uncertainty-analysis.html#types-of-assessment",
    "href": "uncertainty-analysis.html#types-of-assessment",
    "title": "Uncertainty Analysis Guidance",
    "section": "Types of assessment",
    "text": "Types of assessment\nThe recommended approach to uncertainty analysis depends on the nature of the scientific assessment in hand.\n\nVideoText\n\n\n\n\n\n\nAll aspects of scientific assessment, including uncertainty analysis, must be conducted at a scale and complexity proportionate to the needs of the problem. It also needs to be done within the time and resources agreed upon with the decision-makers. Achieving this is a fundamental practical requirement in EFSA’s work. (SO p38)\nIn other words, uncertainty analysis should be scaled to the needs of the scientific assessment and be fit for the purpose.\nThe guidance distinguishes between four main types of assessment to help assessors in planning an uncertainty analysis:\n\nStandardised assessments\nCase-specific assessments\nDevelopment or revision of guidance documents\nUrgent assessments\n\nAssessors are asked to identify which of these types most corresponds to their assessment, and then proceed to the corresponding section for guidance specific to that type.\nLet us take a closer look at the types of assessment:\nA standardised assessment follows a pre-established standardised procedure that covers every step of the assessment. Standardised procedures are often used in scientific assessments for regulated products, e.g. for premarket authorisation. The procedures are often set out in guidance documents or legislation and are accepted by the assessors and decision-makers as providing an appropriate basis for decision-making. Standardised procedures generally require data from the studies conducted according to the guidelines. The procedures specify how those data will be used in the assessment.\nThe second type, case-specific assessments are needed wherever there is no pre-established standardised procedure, and the assessors have to develop an assessment plan that is specific to the case in hand. Standardised elements (e.g. default values) may be used for some parts of the assessment, but other parts require a case-specific approach.\nA third type of assessment where uncertainty analysis is required is the development or revision of guidance documents.\nThe fourth type, urgent assessments, are assessments that, for any reason, must be completed within an unusually short period of time or with unusually limited resources. Since the time is limited, they require streamlined approaches to both assessment and uncertainty analysis.\nNote that the boundaries between the four types are not sharply defined. For example, there are varying degrees of urgency. (SO p36)\nUncertainty analysis can, in each of these four types of assessment, be planned as a tiered or refined approach. In some areas of EFSA’s work, the result of a standardised assessment may indicate the need for a ‘refined’ or ‘higher tier’ assessment in which one or more standardised elements are replaced by case-specific approaches.\nThe conclusion of a scientific assessment must be expressed in a well-defined manner, in order to be a proper scientific statement and useful to decision-makers. The Guidance on Uncertainty Analysis recommend that assessors should always try to express uncertainty in the conclusion quantitatively, using probability.\nAn assessment using qualitative methods or a conclusion expressed in qualitative terms does not imply that the uncertainty analysis must be qualitative. This is because any well-defined qualitative conclusion can be considered as an answer to a yes/no question.\nLet me summarise. EFSAs Guidance on uncertainty analysis is designed to help assessors plan and implement their uncertainty analysis in a way that is scalable to the type of assessment while ensuring the scientific rigour and transparency of the results.\n\n\n\nIdentify which of the following types your assessment most corresponds to and then proceed to the corresponding section for guidance specific to that type.\n\nStandardised Assessments Case-specific assessments   Urgent assessments Development or revision of guidance documents\n\n\n\n\n\n\n\nLearn more about:\n\n\n\n\nIdentifying uncertainties affecting the assessment\nPrioritising uncertainties within the assessment\nDividing the uncertainty analysis into parts\nEnsuring that the questions or quantities of interest are well-defined.\nCharacterising uncertainty for parts of the uncertainty analysis and combining them\nCharacterising overall uncertainty\nPrioritising uncertainties for future investigation.\nReporting uncertainty analysis"
  },
  {
    "objectID": "principles-and-methods/methods-qualitative.html",
    "href": "principles-and-methods/methods-qualitative.html",
    "title": "Qualitative methods",
    "section": "",
    "text": "Descriptive methods\n\n\n\n\n\nUsing narrative phrases or text to describe uncertainties.\n\nDescriptive expression is currently the main approach to characterising uncertainty in EFSA and elsewhere. However, there are reasons to move towards more quantitative forms of expression.\nWhen a descriptive expression of uncertainty is used, the inherent ambiguity of language means that care is needed to avoid misinterpretation. Ambiguity can be reduced by providing precise definitions that are consistently used across Panels, and by increased dialogue between assessors and decision-makers.\nWhen uncertainty is quantified, it may be useful to accompany it with descriptive expression, as the intuitive nature and general acceptance of descriptive expression make it a useful part of the overall communication.\nSpecial care is required to avoid using language that implies value judgements, unless accompanied by objective scientific definitions.\nDescriptive expression should be used to communicate the nature and causes of uncertainty. This is especially important for any uncertainties that are not included in the quantitative assessment.\n\nRead more in SO Annex B.1\n\n\n\n\n\n\n\n\n\nOrdinal scales\n\n\n\n\n\nCharacterising uncertainties using an ordered scale of categories with qualitative definitions (e.g. high, medium or low uncertainty).\n\nOrdinal scales are often defined in terms of the nature, amount, quality and consistency of evidence or the degree of agreement between experts. When used in this way, they should be described as scales for evidence or agreement and not as scales for uncertainty, as they do not describe uncertainty directly. However, they may help to inform subsequent judgements about the degree of uncertainty.\nOrdinal scales can also be used to describe the degree of uncertainty, if they are defined in terms of the range or probability of different answers.\nCalculations which treat ordinal scales as if they were quantitative are invalid and should not be used.\nOrdinal scales provide a useful way of summarising multiple sources of uncertainty to inform quantitative judgements about their combined impact, e.g. when assessing the combined effect of uncertainties which are for whatever reason not quantified individually in the assessment.\n\nRead more in SO Annex B.2\n\n\n\n\n\n\n\n\n\nMatrices for confidence and uncertainty\n\n\n\n\n\nProviding standardised rules for combining two or more ordinal scales describing different aspects or dimensions of uncertainty.\n\nMatrices with ordinal input and output scales that lack quantitative definitions are ambiguous and will be interpreted in different ways by different users.\nMatrices that specify a fixed relation between input and output should not be used unless a clear justification, based on theory or expert judgement, can be provided for the relationships involved.\nMatrices that do not specify a fixed relation between input and output might be regarded as a guide for expert judgement, reminding the user of the factors that should be considered when making judgements. However, users may be tempted to apply them as if they represented fixed rules, leading to inappropriate conclusions.\nEven when the above issues are avoided, matrices become cumbersome when more than two sources or aspects of uncertainty are involved, which is usual in EFSA assessment. The issues in (1–4) above are likely to limit the usefulness of matrices as a tool for assessing uncertainty in EFSA’s work.\n\nRead more in SO Annex B.3\n\n\n\n\n\n\n\n\n\nNUSAP method\n\n\n\n\n\nUsing a set of ordinal scales to characterise different dimensions of each source of uncertainty, and its influence on the assessment conclusion, and plotting these together to indicate which sources of uncertainty contribute most to the uncertainty of the assessment conclusion.\n\nThe NUSAP method can be used as a qualitative approach to help prioritise uncertain elements in risk assessment for quantitative analysis by other methods.\nNUSAP may be especially useful as a structured approach for qualitative characterisation of uncertainties which are not included in quantitative assessment.\nNUSAP practitioners encourage its use in a structured workshop format with groups of experts. As for other formal approaches, this requires additional time and resources but increases the chance of detecting relevant uncertainties and provides a more considered characterisation of their impact on the assessment.\nThe NUSAP method should be further evaluated in a series of case studies for EFSA.\nA common terminology should be developed for use in NUSAP assessments, which is understood by all involved.\n\nRead more in SO Annex B.4\n\n\n\n\n\n\n\n\n\nUncertainty tables for categorical questions\n\n\n\n\n\nA template for listing lines of evidence contributing to answering categorical questions (including yes/no questions), identifying their strengths and weaknesses, and expressing the uncertainty of answers to the questions.\n\nThis approach is potentially applicable to any type of binary question in all areas of EFSA’s work, and to all types of uncertainty affecting those questions.\nThe approach is new and would benefit from further case studies to evaluate its usefulness and identify improvements.\n\nRead more in SO Annex B.6\n\n\n\n\n\n\n\n\n\nStructured tools for evidence appraisal\n\n\n\n\n\nIncluding the templates for identifying and evaluating sources of uncertainty affecting validity of a single study and the whole body of evidence retrieved from the literature, and can also be adapted to evaluate studies submitted to EFSA for the assessment regulated products.\nSO Annex B.19"
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-expressions.html",
    "href": "principles-and-methods/uncertainty-analysis-expressions.html",
    "title": "Uncertainty expressions",
    "section": "",
    "text": "An expression of uncertainty requires two components:\n\nexpression of the range of possible true answers to a question of interest, or a range of possible true values for a quantity of interest, and\nexpression of the probabilities of the different answers or values.\n\n\n\n\n\n\n\nQuantitative approaches express one or both of these components on a numerical scale.\n\n\n\n\n\n\nGeneralExamples\n\n\n\nA complete quantitative expression of uncertainty would specify all the answers or values that are considered possible and probabilities for them all.\nPartial quantitative expression provides only partial information on the probabilities and in some cases partial information on the possibilities (specifying a selection of possible answers or values).\nPartial quantitative expression requires less information or judgements but may be sufficient for decision-making in some assessments, whereas other cases may require fuller quantitative expression.\n\n\n\nIndividual values: Uncertainty partially quantified by specifying some possible values, without specifying what other values are possible or setting upper or lower limits.\nBound: Uncertainty partially quantified by specifying either an upper limit or a lower limit on a quantitative scale, but not both.\nRange: Uncertainty partially quantified by specifying both a lower and upper limit on a quantitative scale, without expressing the probabilities of different values within the limits.\nProbability: Uncertainty about a binary outcome (including the answer to a yes/no question) fully quantified by specifying the probability or approximate probability of both possible outcomes.\nProbability bound: Uncertainty about a non-variable quantity partially quantified by specifying a bound or range with an accompanying probability or approximate probability. A probability interval, two-sided or one-sided, is according to this definition a probability bound. \nDistribution: Uncertainty about a non-variable quantity fully quantified by specifying the probability of all possible values on a quantitative scale.\n\n\n\n\n\n\n\n\n\n\n\n\nQualitative approaches express them using words, categories or labels.\n\n\n\n\n\n\nGeneralExamples\n\n\n\nThey may rank the magnitudes of different uncertainties, and are sometimes given numeric labels, but they do not quantify the magnitudes of the uncertainties nor their impact on an assessment conclusion.\n\n\n\nDescriptive expression: Uncertainty described in narrative text or characterised using verbal terms without any quantitative definition.\nOrdinal scale: Uncertainty described by ordered categories, where the magnitude of the difference between categories is not quantified."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-expressions.html#the-principal-reasons-for-preferring-quantitative-expressions-of-uncertainty",
    "href": "principles-and-methods/uncertainty-analysis-expressions.html#the-principal-reasons-for-preferring-quantitative-expressions-of-uncertainty",
    "title": "Uncertainty expressions",
    "section": "The principal reasons for preferring quantitative expressions of uncertainty",
    "text": "The principal reasons for preferring quantitative expressions of uncertainty\n\nQualitative expressions are ambiguous.\nDecision-making often depends on quantitative comparisons, for example, whether a risk exceeds some acceptable level, or whether benefits outweigh costs.\nIf assessors provide only a single answer or estimate and a qualitative expression of the uncertainty, decision-makers will have to make their own quantitative interpretation of how different the real answer or value might be. This judgement is better made by assessors, since they are better placed to understand the sources of uncertainty affecting the assessment and judge their effect on its conclusion.\nQualitative expressions often imply, or may be interpreted as implying, judgements about the implications of uncertainty for decision-making, which are outside the remit of EFSA.\nAssessors may assess uncertainty differently yet agree on a single qualitative expression, because they interpret it differently.\nExpressing uncertainties in terms of their quantitative impact on the assessment conclusion will reveal differences of opinion between experts working together on an assessment, enabling a more rigorous discussion and hence improving the quality of the final conclusion.\nIt has been demonstrated that people often perform poorly at judging combinations of probabilities. This implies they may perform poorly at judging how multiple uncertainties in an assessment combine. It may therefore be more reliable to divide the uncertainty analysis into parts and quantify uncertainty separately for those parts containing important sources of uncertainty, so that they can be combined by calculation.\nQuantifying uncertainty enables decision-makers to weigh the probabilities of different consequences against other relevant considerations."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-background.html",
    "href": "principles-and-methods/uncertainty-analysis-background.html",
    "title": "Background",
    "section": "",
    "text": "Uncertainty refers to all types of limitations in available knowledge that affect the range and probability of possible answers to an assessment question.\nAvailable knowledge refers here to the knowledge (evidence, data, etc.) available to assessors at the time the assessment is conducted and within the time and resources agreed for the assessment.\n\n\n\n\n\n\nTip\n\n\n\n\nSometimes ‘uncertainty’ is used to refer to a source of uncertainty, and sometimes to its impact on the conclusion of an assessment.\n\n\n\n\n\nUncertainty analysis is defined as the process of identifying and characterising uncertainty about questions of interest and/or quantities of interest in a scientific assessment.\nA question or quantity of interest may be the subject of the assessment as a whole, i.e. that which is required by the ToR for the assessment, or it may be the subject of a subsidiary part of the assessment which contributes to addressing the ToR (e.g. exposure and hazard assessment are subsidiary parts of risk assessment)."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-background.html#definitions",
    "href": "principles-and-methods/uncertainty-analysis-background.html#definitions",
    "title": "Background",
    "section": "",
    "text": "Uncertainty refers to all types of limitations in available knowledge that affect the range and probability of possible answers to an assessment question.\nAvailable knowledge refers here to the knowledge (evidence, data, etc.) available to assessors at the time the assessment is conducted and within the time and resources agreed for the assessment.\n\n\n\n\n\n\nTip\n\n\n\n\nSometimes ‘uncertainty’ is used to refer to a source of uncertainty, and sometimes to its impact on the conclusion of an assessment.\n\n\n\n\n\nUncertainty analysis is defined as the process of identifying and characterising uncertainty about questions of interest and/or quantities of interest in a scientific assessment.\nA question or quantity of interest may be the subject of the assessment as a whole, i.e. that which is required by the ToR for the assessment, or it may be the subject of a subsidiary part of the assessment which contributes to addressing the ToR (e.g. exposure and hazard assessment are subsidiary parts of risk assessment)."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-background.html#roles-of-assessors-and-decision-makers",
    "href": "principles-and-methods/uncertainty-analysis-background.html#roles-of-assessors-and-decision-makers",
    "title": "Background",
    "section": "Roles of assessors and decision-makers",
    "text": "Roles of assessors and decision-makers\n\nQuestions for assessment by EFSA\n\nQuestions for assessment by EFSA may be posed by the European Commission, the European Parliament, and EU Member State or by EFSA itself.\nMany questions to EFSA request assessment of consequences or current policy, conditions, practice or of consequences in alternative scenarios, e.g. under different risk management options.\nIt is important that the scenarios and consequences of interest are well-defined.\n\n\nBasic principles\n\nBasic principles for addressing uncertainty in risk analysis are stated in the Codex Working Principles for Risk Analysis:\n\n‘Constraints, uncertainties and assumptions having an impact on the risk assessment should be explicitly considered at each step in the risk assessment and documented in a transparent manner’\n‘Responsibility for resolving the impact of uncertainty on the risk management decision lies with the risk manager, not the risk assessors’\n\nThese principles apply equally to the treatment of uncertainty in all areas of science and decision-making.\n\n\nResponsibilities\nIn general,\n\nassessors are responsible for characterising uncertainty and\ndecision-makers are responsible for resolving the impact of uncertainty on decisions. Resolving the impact on decisions means deciding whether and in what way decision-making should be altered to take account of the uncertainty.\n\n\n\nRationale\nThe rationale for this division of roles is:\n\nassessing scientific uncertainty requires scientific expertise, while\nresolving the impact of uncertainty on decision-making involves weighing the scientific assessment against other considerations, such as economics, law and societal values, which require different expertise and are also subject to uncertainty.\n\n\n\nInteraction\nAlthough risk assessment and risk management are conceptually distinct activities,\n\ninteraction between assessors and risk managers with regard to the specification of the question for assessment and expression of uncertainty in conclusions is useful."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-background.html#information-required-for-decision-making",
    "href": "principles-and-methods/uncertainty-analysis-background.html#information-required-for-decision-making",
    "title": "Background",
    "section": "Information required for decision-making",
    "text": "Information required for decision-making\n\nUncertainty is always present\n\nUncertainty refers to limitations in knowledge, which are always present to some degree.\nDecision-makers need to know the range of possible answers, so they can consider whether any of them would imply risk of undesirable management consequences (e.g. adverse effects).\n\n\nUncertainty analysis is always required\n\nWhy?\n\nAll EFSA scientific assessments require at least a basic analysis of uncertainty.\n\nQuestions are posed to EFSA because the requestor does not know or is uncertain of the answer and that the amount of uncertainty affects decisions or actions they need to take.\nThe requestor seeks scientific advice from EFSA because they anticipate that this may reduce the uncertainty, or at least provide a more expert assessment of it.\nIf the uncertainty of the answer did not matter, then it would not be rational or economically justified for the requestor to pose the question to EFSA – the requestor would simply use their own judgement, or even a random guess.\nSo the fact that the question was asked implies that the amount of uncertainty matters for decision-making, and it follows that information about uncertainty is a necessary part of EFSA’s response.\nThis logic applies regardless of the nature or subject of the question, therefore providing information on uncertainty is relevant in all cases.\n\n\n\nImplication\nIt follows that uncertainty analysis is needed in all EFSA scientific assessments, though the form and extent of that analysis and the form in which the conclusions are expressed should be adapted to the needs of each case, in consultation with decision-makers.\n\n\n\nAcceptable level of uncertainty\n\nComplete certainty is never possible.\nDeciding how much certainty is required or, equivalently, what level of uncertainty would warrant precautionary action, is the responsibility of decision-makers, not assessors.\nIt may be helpful if the decision-makers can specify in advance how much uncertainty is acceptable for a particular question because it has implications for what outputs should be produced from uncertainty analysis.\nOften, however, the decision-makers may not be able to specify in advance the level of certainty that is sought or the level of uncertainty that is acceptable, e.g. because this may vary from case to case depending on the costs and benefits involved. Another option is for assessors to provide results for multiple levels of certainty, so that decision-makers can consider at a later stage what level of uncertainty to accept.\n\n\n“Practical certainty”\n\nAlternatively, partial information on uncertainty may be sufficient for the decision-makers provided it meets or exceeds their required level of certainty.\n\nFor some types of assessment, e.g. for regulated products, decision-makers need EFSA to provide an unqualified positive or negative conclusion to comply with the requirements of legislation, or of procedures established to implement legislation.\nIn general, the underlying assessment will be subject to at least some uncertainty, as is all scientific assessment. In such cases, therefore, the positive or negative conclusion refers to whether the level of certainty is sufficient for the purpose of decision-making, i.e. whether the assessment provides ‘practical certainty’\n\n\nMain sources of uncertainty\n\nInformation on the magnitude of uncertainty and the main sources of uncertainty is also important to inform decisions about whether it would be worthwhile to invest in obtaining further data or conducting more analysis, with the aim of reducing uncertainty."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-background.html#time-and-resource-constraints",
    "href": "principles-and-methods/uncertainty-analysis-background.html#time-and-resource-constraints",
    "title": "Background",
    "section": "Time and resource constraints",
    "text": "Time and resource constraints\n\n\n\nFit-for-purpose\nTo be fit for purpose, EFSA’s guidance on uncertainty analysis includes options for different levels of resource and different timescales, and methods that can be implemented at different levels of detail/refinement, to fit different timescales and levels of resource.\n\n\nRefine or reduce\nDecisions on how far to refine the assessment and whether to obtain additional data may be taken by assessors when they fall within the time and resources agreed for the assessment.\n\n\nSuffient for decision-making\nUltimately, it is for decision-makers to decide when the characterisation of uncertainty is sufficient for decision-making and when further refinement is needed, taking into account the time and costs involved."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-background.html#expression-of-uncertainty-in-assessment-conclusion",
    "href": "principles-and-methods/uncertainty-analysis-background.html#expression-of-uncertainty-in-assessment-conclusion",
    "title": "Background",
    "section": "Expression of uncertainty in assessment conclusion",
    "text": "Expression of uncertainty in assessment conclusion\n\nRanges and probabilities when question well-defined\n\nRanges and probabilities are the natural metric for quantifying uncertainty and can be applied to any well-defined question or quantity of interest.\nThe question for assessment, or at least the eventual conclusion, needs to be well-defined, in order for its uncertainty to be assessed.\n\n\nQualitative terms defined by quantitative expression\n\nIf qualitative terms are used to describe the degree of uncertainty, they should be clearly defined with objective scientific criteria.\nSpecifically, the definition should identify the quantitative expression of uncertainty associated with the qualitative term as is done, for example, in the EFSA approximate probability scale\n\n\n“Firm” conclusion\n\nFor some types of assessment, decision-makers need EFSA to provide an unqualified positive or negative conclusion. The positive or negative conclusion does not imply that there is complete certainty, since this is never achieved, but that the level of certainty is sufficient for the purpose of decision-making.\nIn such cases, the assessment conclusion and summary may simply report the positive or negative conclusion but, for transparency, the justification for the conclusion should be documented somewhere, e.g. in the body of the assessment or an annex.\n\n\nInconclusive\nIf the level of certainty is not sufficient, then either the uncertainty should be expressed quantitatively, or assessors should report that their assessment is inconclusive and that they ‘cannot conclude’ on the question.\n\n\nLanguage of no quantification\n\nWhen assessors do not quantify uncertainty, they must report that the probability of different answers is unknown and avoid using any language that could be interpreted as implying a probability statement as this would be misleading.\n\n\nAvoid value expressions\n\nThe assessors should avoid any verbal expressions that have risk management connotations in everyday language, such us ‘negligible’ and ‘concern’. When used without further definition, such expressions imply two simultaneous judgements: a judgement about the probability (or approximate probability) of adverse effects, and a judgement about the acceptability of that probability. The first of these judgements is within the remit of assessors, but the latter is not."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html",
    "title": "Key concepts",
    "section": "",
    "text": "The purpose of most EFSA scientific assessments is to determine what science can say about a quantity, event, proposition or state of the world that is of interest for decision-makers.\nIn order to express uncertainty about a question or quantity of interest in a clear and unambiguous way, it is necessary that the question or quantity itself is well-defined, so that it is interpreted in the same way by different people.\nThis applies both to the uncertainty analysis as a whole and to its parts.\n\n\n\n\n\n\nQuantities of interest\n\n\n\n\n\ncan take one of two forms:\n\nNon-variable quantities having a single value e.g. the total number of animals infected with a specified disease entering the EU in a given year. Many non-variable quantities in scientific assessment are parameters that describe variable quantities, which is potentially confusing. A common example of this is the mean body weight for a specified population at a specified time. The term non-variable is used in preference to ‘fixed’, to avoid giving the impression that the value is known with certainty.\nVariable quantities taking multiple values such the body weights in a population.\n\n\n\n\n\n\n\n\n\n\nCategorical questions of interest\n\n\n\n\n\nit is useful to distinguish between:\n\nYes/no questions e.g. questions referring to the presence or absence of some condition or state of the world, the occurrence or not of some event, or the exceedance or not of some quantitative threshold.\nQuestions with more than two categories of answer e.g. different types of effect.\n\n\n\n\n\n\n\n\n\n\nQualitative questions?\n\n\n\n\nReaders may be surprised that the list of types of questions of interest does not include ‘qualitative’. This is because if a question of interest is well-defined, which it should always be for the reasons discussed above, then it can be treated as a yes/no question.\n\n\n\n\n\n\n\n\nChallenging questions or quantities of interest\n\n\n\n\nThe questions or quantities of interest in some EFSA assessments refer to things that may seem challenging to define in terms of the result of a hypothetical experiment or study. Examples include the condition or property of being genotoxic, and calculated quantities such as a Margin of Exposure, neither of which can be directly measured or observed. In practice, however, such questions or quantities can be defined by the procedures for determining them, as established in legislation or official guidance, i.e. the data that are required, and the criteria for interpreting those data."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html#well-defined-questions-and-quantities-of-interest",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html#well-defined-questions-and-quantities-of-interest",
    "title": "Key concepts",
    "section": "",
    "text": "The purpose of most EFSA scientific assessments is to determine what science can say about a quantity, event, proposition or state of the world that is of interest for decision-makers.\nIn order to express uncertainty about a question or quantity of interest in a clear and unambiguous way, it is necessary that the question or quantity itself is well-defined, so that it is interpreted in the same way by different people.\nThis applies both to the uncertainty analysis as a whole and to its parts.\n\n\n\n\n\n\nQuantities of interest\n\n\n\n\n\ncan take one of two forms:\n\nNon-variable quantities having a single value e.g. the total number of animals infected with a specified disease entering the EU in a given year. Many non-variable quantities in scientific assessment are parameters that describe variable quantities, which is potentially confusing. A common example of this is the mean body weight for a specified population at a specified time. The term non-variable is used in preference to ‘fixed’, to avoid giving the impression that the value is known with certainty.\nVariable quantities taking multiple values such the body weights in a population.\n\n\n\n\n\n\n\n\n\n\nCategorical questions of interest\n\n\n\n\n\nit is useful to distinguish between:\n\nYes/no questions e.g. questions referring to the presence or absence of some condition or state of the world, the occurrence or not of some event, or the exceedance or not of some quantitative threshold.\nQuestions with more than two categories of answer e.g. different types of effect.\n\n\n\n\n\n\n\n\n\n\nQualitative questions?\n\n\n\n\nReaders may be surprised that the list of types of questions of interest does not include ‘qualitative’. This is because if a question of interest is well-defined, which it should always be for the reasons discussed above, then it can be treated as a yes/no question.\n\n\n\n\n\n\n\n\nChallenging questions or quantities of interest\n\n\n\n\nThe questions or quantities of interest in some EFSA assessments refer to things that may seem challenging to define in terms of the result of a hypothetical experiment or study. Examples include the condition or property of being genotoxic, and calculated quantities such as a Margin of Exposure, neither of which can be directly measured or observed. In practice, however, such questions or quantities can be defined by the procedures for determining them, as established in legislation or official guidance, i.e. the data that are required, and the criteria for interpreting those data."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html#conditional-nature-of-uncertainty",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html#conditional-nature-of-uncertainty",
    "title": "Key concepts",
    "section": "Conditional nature of uncertainty",
    "text": "Conditional nature of uncertainty\nThe uncertainty affecting a scientific assessment is a function of the knowledge that is relevant to the assessment and available to those conducting the assessment, at the time that it is conducted.\nExpressions of uncertainty are also conditional on the assessors involved. The task of uncertainty analysis is to express the uncertainty of the assessors regarding the question under assessment, at the time they conduct the assessment: there is no single ‘true’ uncertainty.\nThe conditional nature of knowledge and uncertainty means it is legitimate, and to be expected, that different experts within a group may give differing judgements of uncertainty for the same assessment question. Some structured approaches to eliciting judgements and characterising uncertainty elicit the judgement of the individual experts, explore the reasons for differing views and provide opportunities for convergence. A similar process occurs in reaching the consensus conclusion that is generally produced by an EFSA Panel.\nThe conditional nature of knowledge and uncertainty also contributes to cases where different groups of assessors reach diverging opinions on the same issue; again this is relevant information for decision-making. Where differences in opinion arise between EFSA and other EU or Member State bodies, Article 30 of the Food Regulation includes provision for resolving or clarifying them and identifying the uncertainties involved."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html#uncertainty-and-variability",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html#uncertainty-and-variability",
    "title": "Key concepts",
    "section": "Uncertainty and variability",
    "text": "Uncertainty and variability\nIt is important to take account of the distinction between uncertainty and variability, and also how they are related. Uncertainty refers to the state of knowledge, whereas variability refers to actual variation or heterogeneity in the real world. Both can be represented by probability distributions.\nUncertainty may be altered (either reduced or increased) by further research, because it results from limitations in knowledge, whereas variability cannot, because it refers to real differences that will not be altered by obtaining more knowledge. Our knowledge of variability is generally incomplete, so there is uncertainty about variability. In addition, some types of uncertainty are caused by variability. You can learn more about variability on this page.\nIt is important that assessors distinguish uncertainty and variability because they have different implications for decision-making, informing decisions about whether to invest resources in research aimed at reducing uncertainty or in management options aimed at influencing variability.\nHow variability and uncertainty for each component of an assessment should be treated depends on whether the assessment question refers to the population or to a particular member of that population, how each component of the assessment contributes to that, and how those contributions are represented in the assessment model. Care is needed to determine when variability and uncertainty should be separated and when they should be combined, as inappropriate treatment may give misleading results."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html#dependencies",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html#dependencies",
    "title": "Key concepts",
    "section": "Dependencies",
    "text": "Dependencies\nVariables in a scientific assessment can be interdependent. It is important to take account of dependencies between variables in assessment, because they can have a large effect on the result. This means that different combinations of values must be considered in proportion to their expected frequency, taking account of any dependencies, and excluding unrealistic or impossible combinations.\nSources of uncertainty can also be interdependent, such as where learning more about one question of quantity alters uncertainty about another. Dependencies between sources of uncertainty should be identified and accounted for, as they can greatly influence the overall uncertainty.\nProbabilistic calculations or probability bounds methods are preferable to expert judgments for assessing dependencies.\nDependencies also exist in assessments using qualitative methods, and assessors should evaluate their impact on uncertainty."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html#models-and-model-uncertainty",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html#models-and-model-uncertainty",
    "title": "Key concepts",
    "section": "Models and model uncertainty",
    "text": "Models and model uncertainty\nAll scientific assessments involve some form of model, which may be qualitative or quantitative, and most assessments are based on specialised models relevant to the type of assessment. Many assessments combine models of different kinds.\nEFSA uses different types of models, including conceptual models, hazard/exposure ratios, deterministic and probabilistic models, individual-based probabilistic models, statistical models, and logic models. Uncertainties in model structure and inputs need to be considered and quantified e.g. when characterising overall uncertainty. Model uncertainties should be expressed as probability distributions or bounds for the difference between model outputs and the real quantities they represent.\nModels are simplifications of the real world, and while some directly address specific scenarios, others may address simplified scenarios or surrogate questions. The extrapolation from simplified models to the desired scenarios should be considered as a model uncertainty. When a simplified model is used repeatedly for different assessments, the model should be tested. This is the reason for “calibration” of standardised assessment procedures."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html#evidence-agreement-confidence-and-weight-of-evidence",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html#evidence-agreement-confidence-and-weight-of-evidence",
    "title": "Key concepts",
    "section": "Evidence, agreement, confidence and weight of evidence",
    "text": "Evidence, agreement, confidence and weight of evidence\nEvidence, weight of evidence, agreement (e.g. between studies or between experts), and confidence are all concepts related to uncertainty. Increasing the amount, quality, consistency, and relevance of evidence or the degree of agreement between experts generally increases confidence and decreases uncertainty.\nThe relationship between these concepts is complex and variable. Measures of evidence and agreement alone are insufficient as measures of uncertainty because they do not provide information on the range and probability of possible answers or values. Expressing evidence and agreement on qualitative scales can help structure the assessment process and facilitate discussions among experts. Confidence can be used both quantitatively, as in statistical analysis, where it represents a measure of uncertainty in statistical estimates, and qualitatively, as a subjective measure of trust in a conclusion. Weight of evidence involves weighing multiple studies or lines of evidence against each other to assess the balance of evidence for or against different conclusions. Additional considerations, such as the selection of evidence and methods for evaluating and integrating evidence, must be taken into account in uncertainty analysis."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html#influence-sensitivity-and-prioritisation-of-uncertainties",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html#influence-sensitivity-and-prioritisation-of-uncertainties",
    "title": "Key concepts",
    "section": "Influence, sensitivity and prioritisation of uncertainties",
    "text": "Influence, sensitivity and prioritisation of uncertainties\nInfluence and sensitivity are terms used to refer to the extent to which plausible changes in the overall structure, parameters and assumptions used in an assessment produce a change in the results.\nAnalysis of sensitivity and influence can be used to evaluate the overall robustness of the conclusion with respect to choices made in the assessment. It can also be used to prioritise the most important sources of uncertainty for additional or refined analysis or data collection.\nThe term sensitivity analysis is often used in the context of a quantitative model, e.g. to measure the impact of changes to input values on the output of a mathematical model. Influence analysis is broader and considers changes resulting from uncertainties and choices made in the assessment, including the assessment’s structure and models used."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html#conservative-assessments",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html#conservative-assessments",
    "title": "Key concepts",
    "section": "Conservative assessments",
    "text": "Conservative assessments\nMany areas of EFSA’s work use deterministic assessments that are designed to be ‘conservative’. The word ‘conservative’ is generally used in the sense of being ‘on the safe side’ and can be applied either to the choice of protection goals, and hence to the question for assessment, or to dealing with uncertainty in the assessment itself. Conservative framing of the assessment question can simplify complex conditions by focusing the assessment on a conservative subset, which is protective of the rest.\nThe term “conservative” can also relate to two concepts: “coverage,” which refers to the probability that the real value is less adverse, and “degree of uncertainty,” which refers to the amount by which the real value might be less adverse and indicates how much the estimate might be reduced with further analysis. Decision-makers may view an assessment as insufficiently conservative if coverage is low or over-conservative if there is a high degree of uncertainty.\nDescribing an estimate as conservative requires specifying the quantity of interest, management objective, and an acceptable probability threshold. The first two elements involve risk management judgements, whereas the third element is evaluated by assessors. Asserting that an estimate is conservative without specifying the target quantity and probability conflates the roles of decision makers and assessors and is not transparent, because it implies acceptance of some probability of more adverse values without making clear either what is meant by adverse or what the probability is.\nSimilar considerations apply to qualitative assessments and assessments of categorical questions. As for quantitative assessments, asserting that a categorical assessment is conservative implies both a scientific judgement (what is the probability that the adverse category actually applies) and a value judgement (what probability would justify assigning the adverse category for management purposes).\nDeterministic assessments with conservative assumptions are simple and quick to use and provide an important tool for EFSA, provided that the required level of conservatism is defined and that the assessment procedure has been demonstrated to provide it. Calibration of conservatism is crucial when using the same set of conservative assumptions in multiple assessments.\nIt is not necessary for the assessor to express estimates or their probability as precise values, nor for the decision-maker to express the required level of conservatism precisely. Assessors may provide approximate values or bounds, while decision-makers can set upper limits on conservatism. Probability bounds analysis can be used to calculate a probability bound for assessment outputs by eliciting probability bounds for each input. Increased use of probability bounds analysis is recommended for case-specific assessments and calibrating standardised procedures."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html#expert-judgement",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html#expert-judgement",
    "title": "Key concepts",
    "section": "Expert judgement",
    "text": "Expert judgement\nAssessing uncertainty relies on expert judgement, as does science in general. Judgements are behind choices such as models, assumptions and assessment scenarios. Assessing reliability and relevance of data, as well as extrapolation, requires judgement. When using confidence intervals derived from statistical analysis of data, assessors must consider if it accounts for sources of uncertainty that affects its use in the assessment or whether some adjustment is required. When these various types of choices are made, the assessors implicitly consider the range of alternatives for each choice and how well they represent what is known about the problem in hand: in other words, their uncertainty. Thus, the subjective judgement of uncertainty is fundamental, ubiquitous and unavoidable in scientific assessment.\nExpert judgement includes an element of subjectivity because different people have different knowledge and experience. The Scientific Committee emphasises that expert judgement is not guesswork or a substitute for evidence. On the contrary, expert judgement must always be based on reasoned consideration of relevant evidence and expertise, which must be documented transparently, and experts should be knowledgeable or skilled in the topics on which they advise. Well-reasoned judgements are an essential ingredient of good science.\nExpert judgment is essential in scientific assessment but can be influenced by cognitive biases and group dynamics. Procedures are in place to manage conflicts of interest and mitigate biases. Formal approaches for expert knowledge elicitation (EKE) address psychological biases and facilitate aggregation of expert judgments. EFSA has published guidance on EKE and recognises the need for streamlined approaches. Selection of experts should represent a wide range of scientific opinions, and consensus should not imply compromise. Differences of opinion and scientific uncertainty should be reflected in the assessment report.\nThe Scientific Committee stresses that where suitable data provide most of the available information on an issue and are amenable to statistical analysis, this should be used in preference to relying solely on expert judgement. However, as noted above, most data are subject to some limitations in reliability or relevance, and further uncertainties arise in the choice of statistical model; the greater these limitations and uncertainties, the more the results of statistical analysis will need to be interpreted and/or augmented by expert judgement."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html#probability",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html#probability",
    "title": "Key concepts",
    "section": "Probability",
    "text": "Probability\nDecision-makers need to know the range and probability of possible answers for questions or quantities they submit for scientific assessment.\nThere are two major views on using probability as a measure for quantifying uncertainty. The frequentist view restricts probability to variability-based uncertainties and excludes uncertainties caused by knowledge limitations. The subjectivist view allows probability to represent all types of uncertainties, including knowledge limitations. Subjective probability offers comparability and can be applied to well-defined questions. It allows the use of mathematical tools to handle combinations of uncertainties. The guidance encourages the use of subjective probability, except when assessors find it too difficult to quantify uncertainty.\nThe subjectivist interpretation of probability does not exclude the frequentist interpretation. Frequentist probabilities must be reinterpreted as subjective probabilities to be combined appropriately. Estimates derived from statistical analysis may have additional uncertainties that require expert judgment.\nApproximate probabilities, expressed as ranges on subjective probability, can be used when precise values are difficult to provide or when this is sufficient for communicating uncertainty. Approximate probabilities can be computed using the same mathematics as precise probabilities.\nThere is more information on probability in this tutorial."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html#overall-uncertainty",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html#overall-uncertainty",
    "title": "Key concepts",
    "section": "Overall uncertainty",
    "text": "Overall uncertainty\nThe recommendation to quantify uncertainty applies specifically to overall uncertainty, which refers to the assessors’ uncertainty about the assessment conclusion, considering all relevant sources of uncertainty. Assessors should attempt to express the overall impact of identified uncertainties quantitatively, documenting qualitatively any uncertainties that cannot be quantified. In cases where qualitative reporting is required, quantitative evaluation of overall uncertainty is still necessary to determine a justified conclusion. However, overall uncertainty does not include information about unknown unknowns. The characterisation of uncertainty is dependent on the assessors, available evidence, time, and resources. Decision-makers should consider these factors when interpreting and using assessment conclusions. Learn more about overall uncertainty in this section of the tutorial."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html#unquantified-uncertainties",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html#unquantified-uncertainties",
    "title": "Key concepts",
    "section": "Unquantified uncertainties",
    "text": "Unquantified uncertainties\nThe term “unquantified uncertainties” refers to uncertainties which the assessors have identified as relevant to their assessment, but are unable to include in their quantitative expression of overall uncertainty. Assessors should strive to ensure that all questions or quantities in their assessments are well-defined. If they are unable to achieve this, then the uncertainty of those questions or quantities is literally unquantifiable. However, even when a question or quantity is well-defined, an assessor may sometimes be unable to make a quantitative judgement of the impact of one or more sources of uncertainty affecting it. Sources of uncertainty which impact on the conclusion are not quantified for either reason (inability to define or inability to quantify) are sometimes referred to as ‘deep’ uncertainties. Deep uncertainties often arise in complex or novel problems. When assessors cannot quantify the impact of some of the identified uncertainties, they should qualitatively describe them and report them alongside the quantitative expression of overall uncertainty. This is because the overall uncertainty will be conditional on assumptions made in the assessment regarding the sources of uncertainty that were not quantified. This has important implications for reporting and decision-making. It is therefore important to quantify the overall impact of as many as possible of the identified uncertainties, and identify any that cannot be quantified. The most direct way to achieve this is to try to quantify the overall impact of all identified uncertainties, as this will reveal any that cannot be quantified."
  },
  {
    "objectID": "principles-and-methods/uncertainty-analysis-key-concepts.html#conditionality-of-assessments",
    "href": "principles-and-methods/uncertainty-analysis-key-concepts.html#conditionality-of-assessments",
    "title": "Key concepts",
    "section": "Conditionality of assessments",
    "text": "Conditionality of assessments\n\nAssessments are conditional on any uncertainties not included in the quantitative assessment of overall uncertainty. This is because the assessment relies on assumptions about those uncertainties, and the assessment’s output applies only if those assumptions are true. All assessments are inherently conditional based on the current state of scientific knowledge, the available information to assessors, and their judgments about the question at hand. Assessments assume that all relevant uncertainties are identified and there are no unknown unknowns.\nAdditional conditionality arises when identified uncertainties are not considered when characterising the overall uncertainty. The quantitative expression of overall uncertainty becomes conditional on the assumptions made for those unquantified uncertainties.\nDecision-making should consider the implications of conditionality, recognising that assessments are based on scientific knowledge, do not account for unknown unknowns, and are influenced by the expertise and resources available.\nAssessors must provide a list of identified uncertainties not included in the quantitative assessment, along with descriptions and explanations. They must not use any language that implies a quantitative judgement about the probability of other conditions or their effect on the conclusion (e.g. ‘unlikely’, ‘negligible difference’). If the assessor feels able to use such language, this implies that they are in fact able to make a quantitative judgement. If so, they should express it quantitatively or use words with quantitative definitions for transparency, to avoid ambiguity, and to avoid the risk management connotations that verbal expressions often imply. Assessors should communicate clearly that they cannot assign probabilities or make quantitative judgments about the unquantified uncertainties. However, this information is still valuable for decision-makers, as it clarifies the limitations of science and guides further analysis or research.\nDecision-makers should decide how to address these unquantified uncertainties, potentially through further research or precautionary actions. Decision-makers may have the ability to influence some unquantified uncertainties, such as implementing specific practices or policies."
  },
  {
    "objectID": "principles-and-methods/methods.html",
    "href": "principles-and-methods/methods.html",
    "title": "Overview of methods for Uncertainty Analysis",
    "section": "",
    "text": "Identification of potentially relevant sources of uncertainty\n\n\n\n\n\n\n\nHowInputsAssessment methodology\n\n\nSources of uncertainty can affect scientific assessment at different levels.\nAssessors should be systematic in searching for sources of uncertainty affecting their assessment, by considering every part or component of their assessment in turn and checking whether different types of uncertainty are present.\n\n\nUncertainties affecting the inputs used in the scientific assessment are normally identified during the process of appraising the evidence, which is an intrinsic part of scientific assessment.\nSee questions to identify sources of uncertainty affecting inputs\n\n\nOther sources of uncertainties can be identified in relation to how the evidence is used in the assessment, including any models or reasoning that are used to draw conclusions.\nSee questions to identify sources of uncertainty affecting assessment methodolgoy\n\n\n\n\n\n\n\n\n\n\n\n\nMethods for obtaining expert judgements\n\n\n\n\n\n\n\nUse of expert judgementsFormal to less formal EKE\n\n\nAll scientific assessment involves the use of expert judgement\nWhere suitable data are available, this should be used in preference to relying solely on expert judgement.\nWhen data are strong, uncertainty may be quantified by statistical analysis, and any additional extrapolation or uncertainty addressed by minimal assessment (EFSA EKE GD), or collectively as part of the assessment of overall uncertainty.\nWhen data are weak or diverse, it may be better to quantify uncertainty by expert judgement, supported by consideration of the data.\n\n\n\nFormal expert knowledge elicitation (EKE) have been developed to counter psychological biases and to manage the sharing and aggregation of judgements between experts.\nFormal elicitation requires significant time and resources, so it is not feasible to apply it to every source of uncertainty affecting an assessment. Instead, semi-formal expert knowledge elicitation can be applied on less important sources of uncertainty.\nScientific judgements made, usually by a Working Group of experts preparing the assessment, are referred to in this document as judgements by expert group judgement.\nIn practice, there is not a dichotomy between more and less formal approaches to EKE, but rather a continuum.\n\n\n\n\n\n\n\n\n\n\n\n\nQualitative methods for analysing uncertainty\n\n\n\n\n\n\n\nUse of qualitative methods\nQualitative methods characterise uncertainty using descriptive expression or ordinal scales, without quantitative definitions\n\nThey rely on careful use of language and expert judgement.\nQualitative methods may provide a useful aid for experts when making quantitative judgements. Please see examples of qualitative methods.\n\n\n\n\n\n\n\n\n\n\nMethods for quantifying uncertainty\n\n\n\n\n\n\nSO Section 11 provides an overview of probabilistic and deterministic approaches for quantifying uncertainty. More detailed information on each method can be found in SO Annex B. While data play a crucial role, expert judgment should be complemented by statistical analysis whenever possible. The guidance emphasizes the use of probability as the preferred measure for quantifying uncertainty, since it offers a well-defined scale and allows for comparability between different uncertainties. Furthermore, probability enables the quantification of combined uncertainties in model outputs based on the probabilistic representation of input uncertainties. Another advantage of probability is its compatibility with Bayesian statistical analysis and the potential integration of results from non-Bayesian methods, facilitating uncertainty analysis.\nWhen the quantity of interest is variable, determining how to address variability in the assessment is crucial. The decision-maker should specify the aspect of variability they are interested in, whether it’s the entire distribution or a particular aspect such as the worst case or a specific percentile. The choice of quantifying uncertainty depends on the aspect of interest and any models used to relate the quantity of interest to other variables.\nPlease, see the examples of quantitative methods.\n\n\n\n\n\n\n\n\n\nInvestigating influence and sensitivity\n\n\n\n\n\nSensitivity analysis (SA) is a set of methods used to assess the impact of model inputs and uncertainty expressions on the output of a quantitative model. Its objectives include prioritizing sources of uncertainty, identifying inputs for refined quantification or additional data collection, investigating the sensitivity of assumptions, and examining the sensitivity of overall uncertainty.\n\n\nInfluenceSensitivity analysisMethods\n\n\nThe term “influence” refers to the overall impact of changes in the structure, parameters, and assumptions on assessment results. Sensitivity specifically refers to the quantitative impact of input uncertainties on output uncertainties in a quantitative model. Sensitivity analysis tools are discussed in SO Section 12.1. Other forms of influence can be explored by testing different scenarios and observing their effects on the assessment conclusion. Qualitative methods like the NUSAP approach and uncertainty tables can also be used. Expert judgment and formal/semi-formal elicitation techniques are valuable for assessing influence, particularly in determining which parameters should undergo formal sensitivity analysis.\n\n\nSA is typically performed for quantitative models but can also be applied to logic models to assess the sensitivity of conclusions. Expert judgments are crucial for defining the ranges of values to be investigated and choosing the analysis method.\nSA helps allocate uncertainty about the output to sources of uncertainty in the inputs, identifying the main contributors to output uncertainty. Two main approaches exist: local SA, which examines infinitesimal changes in inputs, and global SA, which explores the effects of input changes over their entire range. Global SA methods are more relevant for EFSA assessments due to the need to consider the full range of possible values.\nSA involves changing one parameter at a time (Nominal Range SA) or examining the combined effects of multiple changes, especially when interactions between inputs exist. However, SA cannot guide initial model design or the selection of sources of uncertainty for quantitative uncertainty analysis; expert judgment is necessary for these decisions. Subjective considerations of sensitivity should be carefully documented, and semi-formal expert elicitation may be appropriate when their impact on the assessment is significant.\n\n\nMethods for SA can be categorized as mathematical (deterministic), statistical (probabilistic), or graphical. They provide insights into which data sets, assumptions, or expert judgments require closer examination. Simple methods can be applied to assess the relative sensitivity of the output to individual variables and parameters. Separating the contributions of uncertainty and variability is an important issue in SA, and ongoing development is needed for situations where both components are present.\nSA contributes to uncertainty analysis by analyzing the contributions of individual sources of uncertainty to the overall uncertainty of the assessment conclusion. It expresses the sensitivity of the assessment output quantitatively and/or graphically to input changes. Its strengths include structured identification of influential sources of uncertainty/variability, but it faces challenges in separately assessing the sensitivity to sources of uncertainty and variability.\nRead more in SO Annex B.17\n\n\n\n\n\n\n\n\n\n\n\n\nCharacterisation of overall uncertainty\n\n\n\n\n\n\nThere are three options for quantifying the overall uncertainty, depending on the context:\n\nOption 1: Make a single judgement of the overall impact of all the identified uncertainties.\nOption 2: Quantify uncertainty separately in some parts of the assessment, combine them by calculation, and then adjust the result of the calculation by expert judgement to account for the additional uncertainties that are not yet included.\nOption 3: Quantify uncertainty separately in some parts of the assessment and combine them by calculation, as in Option 2. Then quantify the contribution of the additional uncertainties separately, by expert judgement, and combine it with the previously quantified uncertainty by calculation.\n\n\n\n\n\n\nFigure 1: Options for characterising overall uncertainty"
  },
  {
    "objectID": "principles-and-methods/uncertainty-communication-communicating.html",
    "href": "principles-and-methods/uncertainty-communication-communicating.html",
    "title": "Communicating scientific uncertainties",
    "section": "",
    "text": "The European Food Safety Authority (EFSA) is responsible for providing independent scientific advice, information, and risk communication to improve consumer confidence. The quality, independence, and transparency of EFSA’s scientific advice and information are critical for effective risk communication and ensuring public confidence. EFSA communicates the results of its scientific assessments to decision-makers, stakeholders, and the public at large through various communication channels. To be effective, EFSA’s risk communication must be tailored to the characteristics of the target audience and the perceived sensitivities of the topic."
  },
  {
    "objectID": "principles-and-methods/uncertainty-communication-communicating.html#efsas-risk-communication-mandate",
    "href": "principles-and-methods/uncertainty-communication-communicating.html#efsas-risk-communication-mandate",
    "title": "Communicating scientific uncertainties",
    "section": "",
    "text": "The European Food Safety Authority (EFSA) is responsible for providing independent scientific advice, information, and risk communication to improve consumer confidence. The quality, independence, and transparency of EFSA’s scientific advice and information are critical for effective risk communication and ensuring public confidence. EFSA communicates the results of its scientific assessments to decision-makers, stakeholders, and the public at large through various communication channels. To be effective, EFSA’s risk communication must be tailored to the characteristics of the target audience and the perceived sensitivities of the topic."
  },
  {
    "objectID": "principles-and-methods/uncertainty-communication-communicating.html#risk-perception-and-uncertainty",
    "href": "principles-and-methods/uncertainty-communication-communicating.html#risk-perception-and-uncertainty",
    "title": "Communicating scientific uncertainties",
    "section": "Risk perception and uncertainty",
    "text": "Risk perception and uncertainty\nEFSA’s risk communication plays a crucial role in how audiences perceive the risks and benefits of its assessments, and how they act upon the results. The level of technical knowledge of the target audience affects their understanding of the type and degree of uncertainties identified in the assessment, which is essential for informed decision-making. Communication contextualizes uncertainties in relation to perceived risks, emphasizes the transparency of the process, and explains how scientists can address information gaps in the future. While increased awareness of scientific uncertainties may reduce confidence in decision-making for non-technical audiences, in some cultural contexts, it can increase trust due to greater transparency. The primary roles of risk communication are to contextualize uncertainties, emphasize transparency, and explain how to address information gaps in the future."
  },
  {
    "objectID": "principles-and-methods/uncertainty-communication-communicating.html#challenges-of-communicating-uncertainty-in-scientific-assessments",
    "href": "principles-and-methods/uncertainty-communication-communicating.html#challenges-of-communicating-uncertainty-in-scientific-assessments",
    "title": "Communicating scientific uncertainties",
    "section": "Challenges of communicating uncertainty in scientific assessments",
    "text": "Challenges of communicating uncertainty in scientific assessments\nWhile there are arguments for and against communicating uncertainty, EFSA sees it as crucial to their mandate of being open and transparent. Communicating uncertainty helps decision-makers make informed decisions and increases transparency in the assessment process. However, communicating uncertainty in qualitative or quantitative terms is equivocal and can be understood differently by different people. EFSA has identified a need to differentiate the level of technicality in communication messages for different target audiences."
  },
  {
    "objectID": "principles-and-methods/uncertainty-communication-communicating.html#towards-best-practice-for-communicating-uncertainty",
    "href": "principles-and-methods/uncertainty-communication-communicating.html#towards-best-practice-for-communicating-uncertainty",
    "title": "Communicating scientific uncertainties",
    "section": "Towards best practice for communicating uncertainty",
    "text": "Towards best practice for communicating uncertainty\nThe IPCC recommends reciprocal statements to avoid value-laden interpretations. The European Food Safety Authority (EFSA) conducted two studies to evaluate the understanding and use of different types of uncertainty expressions by different audiences and developed a Companion Guidance document on Communicating Uncertainty in Scientific Assessments to provide practical advice and tools for communicators to explain uncertainty to different target audiences. The Companion Guidance will be implemented simultaneously with the implementation of the Guidance document in EFSA’s scientific assessments."
  }
]