---
title: "Principles of Uncertainty Analysis"
---

In this sections you can learn more about the underlying concepts behind the uncertainty analysis guidance. 

## Definition of uncertainty and uncertainty analysis
<!-- SO p11 -->
Uncertainty refers to all types of limitations in available knowledge that affect the range and probability of possible answers to an assessment question. 

Available knowledge refers here to the knowledge (evidence, data, etc.) available to assessors at the time the assessment is conducted and within the time and resources agreed for the assessment.

Uncertainty analysis is defined as the process of identifying and characterising uncertainty about questions of interest and/or quantities of interest in a scientific assessment. 

A question or quantity of interest may be the subject of the assessment as a whole, i.e. that which is required by the ToR for the assessment, or it may be the subject of a subsidiary part of the assessment which contributes to addressing the ToR (e.g. exposure and hazard assessment are subsidiary parts of risk assessment).

## Roles of assessors aand decision-makers
<!-- SO p14 -->
Basic principles for addressing uncertainty in risk analysis are stated in the Codex Working Principles for Risk Analysis:

- ‘Constraints, uncertainties and assumptions having an impact on the risk assessment should be explicitly considered at each step in the risk assessment and documented in a transparent manner’

- ‘Responsibility for resolving the impact of uncertainty on the risk management decision lies with the risk manager, not the risk assessors’

These principles apply equally to the treatment of uncertainty in all areas of science and decisionmaking.

In general, 

- assessors are responsible for characterising uncertainty and 

- decision-makers are responsible for resolving the impact of uncertainty on decisions. Resolving the impact on decisions means deciding whether and in what way decision-making should be altered to take account of the uncertainty.

The rational for this division of roles is: 

- assessing scientific uncertainty requires scientific expertise, while 

- resolving the impact of uncertainty on decision-making involves weighing the scientific assessment against other considerations, such as economics, law and societal values, which require different expertise and are also subject to uncertainty.

Although risk assessment and risk management are conceptually distinct activities, 

- interaction between assessors and risk managers with regard to the specification of the question for assessment and expression of uncertainty in conclusions is useful.

### Information required for decision-making
<!-- SO p14 -->
Uncertainty refers to limitations in knowledge, which are always present to some degree.

Decision-makers need to know the range of possible answers, so they can consider whether any of them would imply risk of undesirable management consequences (e.g. adverse effects).

<!-- SO p15 -->
For some types of assessment, e.g. for regulated products, decision-makers need EFSA to provide an unqualified positive or negative conclusion to comply with the requirements of legislation, or of procedures established to implement legislation. 

In general, the underlying assessment will be subject to at least some uncertainty, as is all scientific assessment. In such cases, therefore, the positive or negative conclusion refers to whether the level of certainty is sufficient for the purpose of decision-making, i.e. whether the assessment provides ‘practical certainty’

<!-- SO p15 -->
Information on the magnitude of uncertainty and the main sources of uncertainty is also important to inform decisions about whether it would be worthwhile to invest in obtaining further data or conducting more analysis, with the aim of reducing uncertainty.

<!-- SO p15 -->
All EFSA scientific assessments require at least a basic analysis of uncertainty. 

- Questions are posed to EFSA because the requestor does not know or is uncertain of the answer and that the amount of uncertainty affects decisions or actions they need to take. 

- The requestor seeks scientific advice from EFSA because they anticipate that this may reduce the uncertainty, or at least provide a more expert assessment of it. 

- If the uncertainty of the answer did not matter, then it would not be rational or economically justified for the requestor to pose the question to EFSA – the requestor would simply use their own judgement, or even a random guess. 

 - So the fact that the question was asked implies that the amount of uncertainty matters for decision-making, and it follows that information about uncertainty is a necessary part of EFSA’s response. 

 - This logic applies regardless of the nature or subject of the question, therefore providing information on uncertainty is relevant in all cases. 

It follows that uncertainty analysis is needed in all EFSA scientific assessments, though the form and extent of that analysis and the form in which the conclusions are expressed should be adapted to the needs of each case, in consultation with decision-makers.

### Time and resource constraints
<!-- SO p15 -->
Consideration of uncertainty is always required.

To be fit for purpose, EFSA’s guidance on uncertainty analysis includes options for different levels of resource and different timescales, and methods that can be implemented at different levels of detail/refinement, to fit different timescales and levels of resource.

Decisions on how far to refine the assessment and whether to obtain additional data may be taken by assessors when they fall within the time and resources agreed for the assessment.

Ultimately, it is for decision-makers to decide when the characterisation of uncertainty is sufficient for decision-making and when further refinement is needed, taking into account the time and costs involved.
### Questions for assessment by EFSA
<!-- SO p16 -->
Questions for assessment by EFSA may be posed by the European Commission, the European Parliament, and EU Member State or by EFSA itself. 
Many questions to EFSA request assessment of consequences or current policy, conditions, practice or of consequences in alternative scenarios, e.g. under different risk management options. 

It is important that the scenarios and consequences of interest are well-defined.

### Acceptable level of uncertainty
<!-- SO p16 -->
Complete certainty is never possible. 

Deciding how much certainty is required or, equivalently, what level of uncertainty would warrant precautionary action, is the responsibility of decision-makers, not assessors.

It may be helpful if the decision-makers can specify in advance how much uncertainty is acceptable for a particular question because it has implications for what outputs should be produced from uncertainty analysis.

Often, however, the decision-makers may not be able to specify in advance the level of certainty that is sought or the level of uncertainty that is acceptable, e.g. because this may vary from case to case depending on the costs and benefits involved. Another option is for assessors to provide results for multiple levels of certainty, so that decision-makers can consider at a later stage what level of uncertainty to accept.

Alternatively, partial information on uncertainty may be sufficient for the decision-makers provided it meets or exceeds their required level of certainty.

### Expression of uncertainty in assessment conclusions
<!-- SO p16 -->
Ranges and probabilities are the natural metric for quantifying uncertainty and can be applied to any well-defined question or quantity of interest.

The question for assessment, or at least the eventual conclusion, needs to be well-defined, in order for its uncertainty to be assessed.

<!-- SO p17 -->
If qualitative terms are used to describe the degree of uncertainty, they should be clearly defined with objective scientific criteria.
Specifically, the definition should identify the quantitative expression of uncertainty associated with the qualitative term as is done, for example, in the EFSA approximate probability scale LINK TO APPROX PROB SCALE

<!-- SO p17 -->
For some types of assessment, decision-makers need EFSA to provide an unqualified positive or negative conclusion. The positive or negative conclusion does not imply that there is complete certainty, since this is never achieved, but that the level of certainty is sufficient for the purpose of decision-making.
In such cases, the assessment conclusion and summary may simply report the positive or negative conclusion but, for transparency, the justification for the conclusion should be documented somewhere, e.g. in the body of the assessment or an annex.
If the level of certainty is not sufficient, then either the uncertainty should be expressed quantitatively, or assessors should report that their assessment is inconclusive and that they ‘cannot conclude’ on the question.

<!-- SO p17 -->
When assessors do not quantify uncertainty, they must report that the probability of different answers is unknown and avoid using any language that could be interpreted as implying a probability statement as this would be misleading. 

<!-- SO p17 -->
The assessors should avoid any verbal expressions that have risk management connotations in everyday language, such us ‘negligible’ and ‘concern’. When used without further definition, such expressions imply two simultaneous judgements: a judgement about the probability (or approximate probability) of adverse effects, and a judgement about the acceptability of that probability. The first of these judgements is within the remit of assessors, but the latter is not.

## Approaches to expressing uncertainty
<!-- SO p17 -->
Expression of uncertainty requires two components: expression of the range of possible true answers to a question of interest, or a range of possible true values for a quantity of interest, and some expression of the probabilities of the different answers or values. 
Quantitative approaches express one or both of these components on a numerical scale. 
Qualitative approaches express them using words, categories or labels. They may rank the magnitudes of different uncertainties, and are sometimes given numeric labels, but they do not quantify the magnitudes of the uncertainties nor their impact on an assessment conclusion.

<!-- SO p18 -->
A complete quantitative expression of uncertainty would specify all the answers or values that are considered possible and probabilities for them all. Partial quantitative expression provides only partial information on the probabilities and in some cases partial information on the possibilities (specifying a selection of possible answers or values). Partial quantitative expression requires less information or judgements but may be sufficient for decision-making in some assessments, whereas other cases may require fuller quantitative expression.

```{r}
#| label: tbl-approx-probscale
#| tbl-cap: "Approximate probability scale"
#| echo: false
#| message: false

# update rvest
# install.packages("rvest")
# install the latest version
# remotes::install_github("haozhu233/kableExtra")

library(kableExtra)
library(magrittr)
ptbl <- data.frame(
  term=c("Almost certain", "Extremely likely", "Very likely", "Likely", "About as likely as", "Unlikely", "Very unlikely", "Extremely unlikely", "Almost impossible"),
  range=c("99-100%", "95-99%", "90-95%", "66-90%", "33-66%", "10-33%", "5-10%", "1-5%", "0-1%"),
  addopt=c(rep("More likely than not: >50%", 4), rep(" ", 5)),
  addopt2=rep("Unable to give any probability: range is 0-100%. Report as 'inconclusive', 'cannot conclude' or 'unknown'", 9)
) 
tbl_col_names <- c("Probability term", "Subjective probability range", "Additional options", "")
ptbl %>% 
  kbl(col.names =tbl_col_names ) %>% 
  kable_paper(full_width = FALSE) %>%
  column_spec(1:3, width = "10em") %>% 
  collapse_rows(columns = 3:4, valign = "top")

```


### Qualitative expressions
<!-- Box 1 SO p18 -->
*Descriptive expression:* Uncertainty described in narrative text or characterised using verbal terms without any quantitative definition (@tbl-approx-probscale). 

*Ordinal scale:* Uncertainty described by ordered categories, where the magnitude of the difference between categories is not quantified.

### Quantitative expressions
<!-- Box 1 SO p18 -->
*Individual values:* Uncertainty partially quantified by specifying some possible values, without specifying what other values are possible or setting upper or lower limits.

*Bound:* Uncertainty partially quantified by specifying either an upper limit or a lower limit on a quantitative scale, but not both. 

*Range:* Uncertainty partially quantified by specifying both a lower and upper limit on a quantitative scale, without expressing the probabilities of different values within the limits. 

*Probability:* Uncertainty about a binary outcome (including the answer to a yes/no question) fully quantified by specifying the probability or approximate probability of both possible outcomes. 

*Probability bound:* Uncertainty about a non-variable quantity partially quantified by specifying a bound or range with an accompanying probability or approximate probability.

*Distribution:* Uncertainty about a non-variable quantity fully quantified by specifying the probability of all possible values on a quantitative scale.

## The role of quantitative expressions of uncertainty
The principal reasons for preferring quantitative expressions of uncertainty are as follows:

 - Qualitative expressions are ambiguous.

 - Decision-making often depends on quantitative comparisons, for example, whether a risk exceeds some acceptable level, or whether benefits outweigh costs.

  - If assessors provide only a single answer or estimate and a qualitative expression of the uncertainty, decision-makers will have to make their own quantitative interpretation of how different the real answer or value might be. This judgement is better made by assessors, since they are better placed to understand the sources of uncertainty affecting the assessment and judge their effect on its conclusion.

  - Qualitative expressions often imply, or may be interpreted as implying, judgements about the implications of uncertainty for decision-making, which are outside the remit of EFSA.

  - Assessors may assess uncertainty differently yet agree on a single qualitative expression, because they interpret it differently.

  - Expressing uncertainties in terms of their quantitative impact on the assessment conclusion will reveal differences of opinion between experts working together on an assessment, enabling a more rigorous discussion and hence improving the quality of the final conclusion.

  - It has been demonstrated that people often perform poorly at judging combinations of probabilities. This implies they may perform poorly at judging how multiple uncertainties in an assessment combine. It may therefore be more reliable to divide the uncertainty analysis into parts and quantify uncertainty separately for those parts containing important sources of uncertainty, so that they can be combined by calculation.

  - Quantifying uncertainty enables decision-makers to weigh the probabilities of different consequences against other relevant considerations.
  
### Concerns and objections to quantitative expressions of uncertainty
  Many concerns and objections to quantitative expression of uncertainty have been raised by various parties during the public consultation and trial period for this document and in the literature. These are listed in Box 2; many, not all, relate to the role of expert judgement in quantifying uncertainty. The Scientific Committee has considered these concerns carefully and concludes that all of them can be addressed, either by improved explanation of the principles involved or through the use of appropriate methods for obtaining and using quantitative expressions.
  
  BOX 2

### Assessors should express uncertainty in quantitative terms
<!-- SO p20 -->
Having considered the advantages of quantitative expression, and addressed the concerns, the Scientific Committee concludes that assessors should express in quantitative terms the combined effect of as many as possible of the identified sources of uncertainty, while recognising that how this is reported must be compatible with the requirements of decision-makers and legislation.

Any sources of uncertainty that assessors are unable to include in their quantitative expression, for whatever reason, must be documented qualitatively and reported alongside it, because they will have significant implications for decision-making.

Together, the quantified uncertainty and the description of unquantified uncertainties provide the overall characterisation of uncertainty, and express it as unambiguously as is possible.

This recommended approach is thus consistent with the requirement of the FAO/WHO Codex Working Principles for Risk Analysis and the EFSA Guidance on Transparency from 2010, which state that uncertainty be ‘quantified to the extent that is scientifically achievable’.

 - ‘scientifically achievable’ should be interpreted as referring to including as many as possible of the identified sources of uncertainty within the quantitative assessment of overall uncertainty, and omitting only those which the assessors are unable to quantify.
 - ‘scientifically achievable’ does **not** mean that uncertainties should be quantified using the most sophisticated scientific methods available.

The recommended approach does not imply a requirement to quantify ‘unknown unknowns’ or ignorance. These type of sources of uncertainty are always potentially present, but cannot be included in assessment, as the assessors are unaware of them


### Common concerns and objections to quantitative expression of uncertainty
<!-- Box 2 SO p21 -->
Common concerns and objections to quantitative expression of uncertainty and how they are addressed by the approach developed in this document and the accompanying Guidance.

::: {.callout-tip collapse="true" appearance="minimal"}
# Quantifying uncertainty requires complex computations, or excessive time or resource: 
:::: {.panel-tabset}
most of the options in the Guidance do not require complex computations, and the methods are scalable to any time and resource limitation, including urgent situations.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Quantifying uncertainty requires extensive data: 
:::: {.panel-tabset}
uncertainty can be quantified by expert judgement for any well-defined question or quantity, provided there is at least some relevant evidence.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Data are preferable to expert judgement: 
:::: {.panel-tabset}
the Guidance recommends use of relevant data where available.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Subjectivity is unscientific: 
:::: {.panel-tabset}
All judgement is subjective, and judgement is a necessary part of all scientific assessment. Even when good data are available, expert judgement is involved in evaluating and analysing them, and when using them in risk assessment.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Subjective judgements are guesswork and speculation: 
:::: {.panel-tabset}
all judgements in EFSA assessments will be based on evidence and reasoning, which will be documented transparently.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Expert judgement is subject to psychological biases: 
:::: {.panel-tabset}
EFSA’s guidance on uncertainty analysis and expert knowledge elicitation use methods designed to counter those biases.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Quantitative judgements are over-precise: 
:::: {.panel-tabset}
EFSA’s methods produce judgements that reflect the experts’ uncertainty – if they feel they are over-precise, they should adjust them accordingly.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Uncertainty is exaggerated: 
:::: {.panel-tabset}
identify your reasons for thinking the uncertainty is exaggerated, and revise your judgements to take them into account.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# There are too many uncertainties: 
:::: {.panel-tabset}
whenever experts draw conclusions, they are necessarily making judgements about all the uncertainties they are aware of. The Guidance provides methods for assessing uncertainties collectively that increase the rigour and transparency of those judgements.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Probability judgements are themselves uncertain: 
:::: {.panel-tabset}
take the uncertainty of your judgement into account as part of the judgement, e.g. by giving a range, or making it wider.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Giving precise quantiles for uncertainty is over-confident: 
:::: {.panel-tabset}
the quantiles will not be treated as precise, but as a step in deriving a distribution for you to review and adjust. If there is concern about the choice of distribution, its impact on the analysis can be assessed by sensitivity analysis. Alternatively, approximate probabilities could be used.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# There are some uncertainties I cannot make a probability judgement for: 
:::: {.panel-tabset}
in principle, probability judgements can be given for all well-defined questions or quantities. However, the Guidance recognises that experts may be unable to make probability judgements for some uncertainties, and provides options for dealing with this.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Different experts will make different judgements: 
:::: {.panel-tabset}
this is expected and inevitable, whether the judgements are quantitative or not. An advantage of quantitative expression is that those differences are made explicit and can be discussed, leading to better conclusions. These points apply to experts working on the same assessment, and also to different assessments of the same question by different experts or institutions.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# I cannot give a probability for whether a model is correct: 
:::: {.panel-tabset}
no model is entirely correct. Model uncertainty is better expressed by making a probability judgement for how different the model result might be from the real value.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Uncertainty should be addressed by conservative assumptions:
:::: {.panel-tabset}
 choosing a conservative assumption involves two judgements – the probability that the assumption is valid, and the acceptability of that probability. The Guidance improves the rigour and transparency of the first judgement, providing a better basis for the second (which is part of risk management).
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Probabilities cannot be given for qualitative conclusions:
:::: {.panel-tabset}
 Probability judgements can be made for any well-defined conclusion, and all EFSA conclusions should be well-defined.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# You cannot make judgements about unknown unknowns:
:::: {.panel-tabset}
no such judgements are implied. All scientific advice is conditional on assumptions about unknown unknowns.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Uncertainty is unquantifiable by definition: 
:::: {.panel-tabset}
this is the Knightian view. The Guidance uses subjective probability, which Knight recognised as an option.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Probabilities cannot be given unless all the possibilities can be specified: 
:::: {.panel-tabset}
provided an answer to a question is well-defined, a probability judgement can be made for it without specifying or knowing all possible alternative answers. However, assessors should guard against a tendency to underestimate the probability of other answers when they are not differentiated.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# None of the ranges in the approximate probability scale properly represent my judgement: 
:::: {.panel-tabset}
specify a range that does.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Lack of evidence: 
:::: {.panel-tabset}
if there really is no evidence, no probability judgement can be made – and no scientific conclusion can be drawn. 
<!-- This was added by the tutors:  -->
Another way to see it, is that if the experts can make a conclusion (that is not inconclusive), they should also be able to express their level of certainty about it. 
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# It is not valid to combine probabilities derived from data with probabilities derived by expert judgement: 
:::: {.panel-tabset}
there is a well-established theoretical basis for using probability calculations to combine probability judgements elicited from experts (including probability judgements informed by non Bayesian statistical analysis) with probabilities obtained from Bayesian statistical analysis of data.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# The result of the uncertainty analysis is incompatible with, or undermines, our conclusion: 
:::: {.panel-tabset}
reconsider both the uncertainty analysis and the conclusion, and revise one or both so they (a) match and (b) properly represent what the science supports. A justifiable conclusion takes account of uncertainty, so there should be no inconsistency.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Decision-makers require us to say whether a thing is safe or not safe, not give a probability for being safe: 
:::: {.panel-tabset}
‘safe’ implies some acceptable level of certainty, so if that is defined then positive or negative conclusion may be given without qualification.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Risk managers and the public do not want to know about uncertainty: 
:::: {.panel-tabset}
actually many do, and as a matter of principle, decision-makers need information on uncertainty to make rational decisions.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Communicating uncertainty will undermine public confidence in scientific assessment: 
:::: {.panel-tabset}
some evidence supports this, but other evidence suggests communicating uncertainty can increase confidence. EFSA’s approach on communicating uncertainty is designed to achieve the latter. 
::::
:::

## The role of qualitative expressions of uncertainty

<!-- SO p22 -->
Qualitative methods in uncertainty analysis are specifically recommended for the following purposes: 

 - As a simple approach for prioritising uncertainties. 
 
 - At intermediate points in an uncertainty analysis, to characterise individual sources of uncertainty qualitatively, as an aid to quantifying their combined impact by probability judgement. This may be useful either for individual parts of the uncertainty analysis, or as a preliminary step when characterising the overall uncertainty of the conclusion.
 
 - When quantifying uncertainty by expert judgement, and when communicating the results of that, it may in some cases be helpful to use an approximate probability scale with accompanying qualitative descriptors  
<!-- added by tutors -->
(verbal expressions).
 
 - At the end of uncertainty analysis, for describing uncertainties that the assessors are unable to include in their quantitative evaluation. 
 
 - When reporting the assessment, for expressing the assessment conclusion in qualitative terms when this is required by decision-makers or legislation.
 
# Key concepts

## Well-defined questions and quantities of interest

## Conditional nature of uncertainty

## Uncertainty and variability

## Dependencies

## Models and model uncertainty

## Evidence, agreement, confidence and weight of evidence

## Influence, sensitivity and prioritisation of uncertainties

## Conservative assessments

## Expert judgement

## Probability

## Overall uncertainty

<!-- SO p20, last bullet point -->
It is therefore important to quantify the overall impact of as many as possible of the identified uncertainties, and identify any that cannot be quantified. The most direct way to achieve this is to try to quantify the overall impact of all identified uncertainties, as this will reveal any that cannot be quantified.

## Unquantified uncertainties

## Conditionality of assessments

<!-- Move Types of assessment distinguished for uncertainty analysis to the U GD page -->
# Main elements of uncertainty analysis

<!-- Box 3 SO p38 -->
Some assessments require only some elements, and each element can be implemented in various ways with various methods. 

<!-- SO p37 -->
What is needed depends in part on the general type of the assessment (standardised assessment, case-specific assessment, development, review of a standardised procedure or urgent assessment), and partly on the specific needs of the individual assessment, which need to be decided by assessors.

<!-- Box 3 SO p38 -->

::: {.callout-tip collapse="true" appearance="minimal"}
# Identifying uncertainties affecting the assessment.
:::: {.panel-tabset}
This is necessary in every assessment, and should be done in a structured way to minimise the chance of overlooking relevant uncertainties. In assessments that follow standardised procedures, it is only necessary to identify non-standard uncertainties.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Prioritising uncertainties within the assessment 
:::: {.panel-tabset}
plays an important role in the planning the uncertainty analysis, enabling the assessor to focus detailed analysis on the most important uncertainties and address others collectively when evaluating overall uncertainty. Often prioritisation will be done by expert judgement during the planning process, but in more complex assessments it may be done explicitly, using influence or sensitivity analysis.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Dividing the uncertainty analysis into parts (when appropriate). 
:::: {.panel-tabset}
In some assessments, it may be sufficient to characterise overall uncertainty for the whole assessment directly, by expert judgement. In other cases, it may be preferable to evaluate uncertainty for some or all parts of the assessment separately and then combine them, either by calculation or expert judgement.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Ensuring the questions or quantities of interest are well-defined. 
:::: {.panel-tabset}
This is necessary in every assessment. Some assessments follow standardised procedures, within which the questions and/or quantities of interest should be predefined. In other assessments, the assessors will need to identify and define the questions and/or quantities of interest case by case.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Characterising uncertainty for parts of the uncertainty analysis. 
:::: {.panel-tabset}
This is needed for assessments where the assessors choose to divide the uncertainty analysis into parts, but may only be done for some of the parts, with the other parts being considered when characterising overall uncertainty.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Combining uncertainty from different parts of the uncertainty analysis.  
:::: {.panel-tabset}
This is needed for assessments where the assessors quantify uncertainty separately for two or more parts of the uncertainty analysis.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Characterising overall uncertainty. 
:::: {.panel-tabset}
Expressing quantitatively the overall impact of as many as possible of the identified uncertainties, and describing qualitatively any that remain unquantified. This is necessary in all assessments except standardised assessments where no non-standard uncertainties are identified.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Prioritising uncertainties for future investigation. 
:::: {.panel-tabset}
This is implicit or explicit in any assessment where recommendations are made for future data collection or research, and may be informed by influence or sensitivity analysis.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Reporting uncertainty analysis. 
:::: {.panel-tabset}
Required for all assessments, but extremely brief in standardised assessments where no non-standard uncertainties are identified.
::::
:::

# Key principles

## Key principles when conducting case-specific assessments
<!-- SO p39 -->
:::{.callout-tip}
# What is a case-specific assessment?
A case-specific assessment is needed when there is no standardised procedure for the type of assessment in hand, and when parts of the assessment use standardised procedure but other parts are case-specific or deviate from the standardised procedure (e.g. for refinement or urgency), and for calibrating standardised procedures when they are first established or revised. 
:::

- The uncertainty analysis should start at a level that is appropriate to the assessment in hand. For assessments where data to quantify uncertainty is available and/or where suitable quantitative methods are already established, this may be included in the initial assessment. In other assessments, it may be best to start with a simple approach, unless it is evident at the outset that more complex approaches are needed.

- Uncertainty analysis should be refined as far as is needed to inform decision-making. 
  -- This point is reached either when there is sufficient certainty about the question or quantity of interest for the decision-makers to make a decision with the level of certainty they require, or 
  -- if it becomes apparent that achieving the desired level of uncertainty is unfeasible or too costly and the decision-makers decide instead to manage the uncertainty without further refinement of the analysis.

- Refinements of the uncertainty analysis using more complex or resource-intensive methods and options should be targeted on those sources of uncertainty where they will contribute most efficiently to improving the characterisation of uncertainty, taking account of their influence on the assessment conclusion and the cost and feasibility of the refinement. 

- The characterisation of overall uncertainty must integrate the contributions of identified sources of uncertainties that have been expressed in different ways.

## Key principles when using standardise procedures 
<!-- SO p40 -->
:::{.callout-tip}
# What is a standardised assessment procedure?
Standardised assessment procedures with accepted provision for uncertainty are common in many areas of EFSA’s work, especially for regulated products, and are subject to periodic review. 
ULLRIKA TO PUT MORE HERE
:::


## Key principles for urgent assessments 
<!-- SO p41 -->
:::{.callout-tip}
# What is an urgent assessment?
TO BE TUNED _ TAKE FRO GLOSSARY? 

In some situations, e.g. emergencies, EFSA may be required to provide an urgent assessment in very limited time and the approach taken must be adapted accordingly. Uncertainty is generally increased in such situations, and may be a major driver for decision-making. Characterisation of uncertainty is therefore still necessary, despite the urgency of the assessment. However, the approach to providing it must be scaled to fit within the time and resources available.
:::

