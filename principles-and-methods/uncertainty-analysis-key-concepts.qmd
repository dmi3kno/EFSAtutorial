---
title: "Key concepts"
---

## Well-defined questions and quantities of interest
<!-- SO p22 -->
The purpose of most EFSA scientific assessments is to determine what science can say about a quantity, event, proposition or state of the world that is of interest for decision-makers.

In order to express uncertainty about a question or quantity of interest in a clear and unambiguous way, it is necessary that the question or quantity itself is well-defined, so that it is interpreted in the same way by different people.

This applies both to the uncertainty analysis as a whole and to its parts.

::: {.callout-tip collapse="true" appearance="minimal"}
## Quantities of interest

can take one of two forms:

- *Non-variable quantities having a single value*
e.g. the total number of animals infected with a specified disease entering the EU in a given year. Many non-variable quantities in scientific assessment are parameters that describe variable quantities, which is potentially confusing. A common example of this is the mean body weight for a specified population at a specified time. The term non-variable is used in preference to ‘fixed’, to avoid giving the impression that the value is known with certainty. 
- *Variable quantities taking multiple values* such the body weights in a population. 
:::

::: {.callout-tip collapse="true" appearance="minimal"}
## Categorical questions of interest 

it is useful to distinguish between: 

- *Yes/no questions* e.g. questions referring to the presence or absence of some condition or state of the world, the occurrence or not of some event, or the exceedance or not of some quantitative threshold.

- *Questions with more than two categories of answer* e.g. different types of effect.
:::

::: {.callout-tip}
## Qualitative questions?
<!-- SO p24 -->
Readers may be surprised that the list of types of questions of interest does not include ‘qualitative’. This is because if a question of interest is well-defined, which it should always be for the reasons discussed above, then it can be treated as a yes/no question.
:::

::: {.callout-tip}
## Challenging questions or quantities of interest
<!-- SO p23 -->
The questions or quantities of interest in some EFSA assessments refer to things that may seem challenging to define in terms of the result of a hypothetical experiment or study. Examples include the condition or property of being genotoxic, and calculated quantities such as a Margin of Exposure, neither of which can be directly measured or observed. In practice, however, such questions or quantities can be defined by the procedures for determining them, as established in legislation or official guidance, i.e. the data that are required, and the criteria for interpreting those data.
:::

::: {.callout-tip collapse="true" appearance="minimal"}
## Conditional nature of uncertainty

The uncertainty in a scientific assessment depends on the available knowledge and resources at the time of assessment. Information limitations and accessibility issues contribute to uncertainty, even if more knowledge exists elsewhere. Rapid assessments often have higher uncertainty due to time constraints. Expressions of uncertainty vary depending on the assessors involved, as there is no singular "true" uncertainty. Different experts may have differing judgments of uncertainty for the same assessment question.

Assessments are conditional on knowledge, resources, expert judgments, and the specific question being addressed. Data can yield varying levels of uncertainty depending on different extrapolations or dependencies required. Experts within a group may have diverse judgments due to their expertise, experience, and social contexts. Various techniques, such as expert elicitation, aim to elicit and aggregate expert judgments while addressing biases.

Differences in opinions among assessors or groups are part of collective uncertainty and provide relevant information for decision-making. Procedures exist to resolve or clarify diverging opinions between EFSA and other EU or Member State bodies, considering the uncertainties involved.
:::

::: {.callout-tip collapse="true" appearance="minimal"}
## Uncertainty and variability 

It is important to distinguish between uncertainty and variability. Uncertainty refers to the state of knowledge, while variability refers to actual differences in the real world. Both can be represented by probability distributions. Uncertainty can be altered through further research, while variability remains unchanged. Assessors should distinguish between uncertainty and variability as they have different implications for decision-making. Variability exists in populations of various entities, and our knowledge of it is generally incomplete. When dealing with variability, it is important to define the population and any relevant subpopulations. The time period of interest should also be specified. Variability causes uncertainty in measurements and sampling, referred to as 'aleatory' uncertainty. Epistemic uncertainty arises from other limitations in knowledge. How variability and uncertainty are treated depends on the assessment question and the contribution of each component. Variability may be quantified depending on the specific question. In risk assessments, variability and uncertainty may need to be separated or combined appropriately.
:::

::: {.callout-tip collapse="true" appearance="minimal"}
## Dependencies

Variables in scientific assessments can be interdependent, affecting the outcomes significantly. Dependencies between variables need to be considered, including their expected frequency and realistic combinations. Uncertainty sources can also be interdependent, where learning about one quantity alters uncertainty about another. Dependencies between uncertainty sources should be identified and accounted for, as they can greatly influence overall uncertainty. Probabilistic calculations or probability bounds methods are preferable to expert judgments for assessing dependencies. Dependencies exist in both quantitative and qualitative assessments, and assessors should evaluate their impact on uncertainty. Expert judgment may be challenging, prompting the need for quantitative reformulation of the assessment and uncertainty analysis.
:::

::: {.callout-tip collapse="true" appearance="minimal"}
## Models and model uncertainty

Scientific assessments involve various models, both qualitative and quantitative. EFSA uses different types of models, including conceptual models, hazard/exposure ratios, deterministic and probabilistic models, individual-based probabilistic models, statistical models, and logic models. Uncertainties in model structure and inputs need to be considered and quantified where possible. Model uncertainties should be expressed as probability distributions or bounds for the difference between model outputs and the real quantities they represent. Models are simplifications of the real world, and while some directly address specific scenarios, others may address simplified scenarios or surrogate questions. The extrapolation from simplified models to the desired scenarios should be considered as a model uncertainty. Calibration of standardized assessment procedures is important for ensuring the reliability of simplified models.
:::

::: {.callout-tip collapse="true" appearance="minimal"}
## Evidence, agreement, confidence and weight of evidence

Evidence, weight of evidence, agreement, and confidence are all concepts related to uncertainty. Increasing the quantity, quality, consistency, and relevance of evidence or the degree of agreement between experts generally increases confidence and decreases uncertainty. However, the relationship between these concepts is complex and variable. Measures of evidence and agreement alone are insufficient as measures of uncertainty because they do not provide information on the range and probability of possible answers or values. Expressing evidence and agreement on qualitative scales can help structure the assessment process and facilitate discussions among experts. Confidence can be used both quantitatively, as in statistical analysis, where it represents a measure of uncertainty, and qualitatively, as a subjective measure of trust in a conclusion. Weight of evidence involves weighing multiple studies or lines of evidence against each other to assess the balance of evidence for or against different conclusions. Weight of evidence assessment and uncertainty analysis are closely related but do not capture all aspects of uncertainty. Additional considerations, such as the selection of evidence and methods for evaluating and integrating evidence, must be taken into account in uncertainty analysis.
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Influence, sensitivity and prioritisation of uncertainties

In uncertainty analysis, "influence" and "sensitivity" refer to the extent to which changes in the structure, parameters, and assumptions of an assessment affect the results. Sensitivity analysis quantitatively measures the impact of input changes on the output of a mathematical model, while influence analysis considers changes resulting from uncertainties and choices made in the assessment, including the assessment's structure and models used. Assessing influence can be done quantitatively or qualitatively, while sensitivity analysis focuses on quantitative measurements. These analyses are useful for evaluating the robustness of conclusions, prioritizing uncertainties for further analysis or data collection, and providing recommendations for future research. They play a key role in refining assessments iteratively and informing decisions on monitoring, data collection, and research priorities.
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Conservative assessments

Deterministic assessments used by EFSA are often designed to be "conservative," meaning they err on the side of caution and prioritize safety. Being conservative can refer to the choice of protection goals or dealing with uncertainty in the assessment itself. Conservative framing of the assessment question can simplify complex conditions or manage uncertainty in risk management. The term "conservative" can also relate to two concepts: "coverage," which refers to the probability of less adverse values, and "degree of uncertainty," which indicates how much the estimate might be reduced with further analysis. Decision-makers may view an assessment as insufficiently conservative if coverage is low or over-conservative if there is a high degree of uncertainty. Describing an estimate as conservative requires specifying the quantity of interest, management objective, and an acceptable probability threshold. Transparent communication requires clear definitions of adversity and probability. Similar considerations apply to qualitative and categorical assessments. Deterministic assessments with conservative assumptions are valuable if the required level of conservatism is defined and the assessment procedure is proven to provide it. Calibration of conservatism is crucial when using the same set of conservative assumptions in multiple assessments. Assessors may provide approximate values or bounds, while decision-makers can set upper limits on conservatism. Probability bounds analysis can be used to calculate a probability bound for assessment outputs by eliciting probability bounds for each input. Increased use of probability bounds analysis is recommended for case-specific assessments and calibrating standardized procedures.
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Expert judgement 

Assessing uncertainty involves expert judgment, which is subjective due to variations in knowledge and experience. Choices such as models, assessment scenarios, and data reliability rely on expert judgment. Even with hard data, assessing relevance and reliability involves subjectivity. Judgments about extrapolation and confidence intervals also require subjective consideration. Expert judgment is essential in scientific assessment but can be influenced by cognitive biases and group dynamics. Procedures are in place to manage conflicts of interest and mitigate biases. Formal approaches for expert knowledge elicitation (EKE) address psychological biases and facilitate aggregation of expert judgments. Extended participation of stakeholders and the public may be considered in cases with severe limits to knowledge. EFSA has published guidance on EKE and recognizes the need for streamlined approaches. Selection of experts should represent a wide range of scientific opinions, and consensus should not imply compromise. Differences of opinion and scientific uncertainty should be reflected in the assessment report. Data-driven analysis should be preferred when suitable, but expert judgment is necessary when data have limitations. Concerns about quantifying uncertainty using expert judgment have been addressed, and appropriate training should be provided to experts. Combining uncertainties through calculations is recommended when possible to inform judgments about overall uncertainty. Assumptions and additional uncertainties associated with calculation models should be considered, and combinations of inputs that cannot occur together in reality should be avoided. If uncertainty sources are combined through expert judgment, the added uncertainty should be taken into account.
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Probability 

Decision-makers require information on the range and probability of possible answers when dealing with uncertainty. There are two major views on using probability to quantify uncertainty. The frequentist view restricts probability to variability-based uncertainties and excludes uncertainties caused by knowledge limitations. The subjectivist (Bayesian) view allows probability to represent all types of uncertainties, including knowledge limitations. Subjective probability offers comparability and can be applied to well-defined questions. It allows the use of mathematical tools to handle combinations of uncertainties. The guidance encourages the use of subjective probability, except when quantifying uncertainty is too challenging. Frequentist probabilities must be reinterpreted as subjective probabilities to be combined appropriately. Probabilities derived from statistical analysis may have additional uncertainties that require expert judgment. Approximate probabilities, expressed as ranges, can be used when precise values are difficult to provide. They can reflect confidence in probability judgments and simplify assessments. Approximate probabilities can be computed using the same mathematics as precise probabilities.
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Overall uncertainty 

The recommendation to quantify uncertainty applies specifically to overall uncertainty, which refers to the assessors' uncertainty about the assessment conclusion, considering all relevant sources of uncertainty. Assessors should attempt to express the overall impact of identified uncertainties quantitatively, documenting qualitatively any uncertainties that cannot be quantified. In cases where qualitative reporting is required, quantitative evaluation of overall uncertainty is still necessary to determine a justified conclusion. However, overall uncertainty does not include information about unknown unknowns. The characterisation of uncertainty is dependent on the assessors, available evidence, time, and resources. Decision-makers should consider these factors when interpreting and using assessment conclusions.

:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Unquantified uncertainties 

The term "unquantified uncertainties" refers to uncertainties identified by assessors that cannot be included in the quantitative expression of overall uncertainty. This section discusses the limits of quantifiability and explores what can be done about uncertainties that cannot be quantified. Assessors should strive to ensure that all questions or quantities in their assessments are well-defined. However, there are cases where assessors are unable to quantify uncertainties due to the inability to judge their magnitude or impact. These unquantified uncertainties, also known as "deep" uncertainties, often arise in complex or novel problems. Some authors distinguish between unquantifiable uncertainty and quantifiable risk, based on a frequentist view of probability. However, the guidance recommends using subjective probability to express all types of uncertainty, including variability, as long as the question or quantity of interest is well-defined. Nevertheless, assessors may still be unable to quantify certain uncertainties even when the questions or quantities are well-defined. Stirling's matrix defines different conditions of incertitude (risk, uncertainty, ambiguity, and ignorance) and suggests various methods for dealing with each condition. While some methods involve stakeholder participation and consideration of values, EFSA's role is limited to scientific assessment. EFSA's responsibility is to identify and quantify scientific sources of uncertainty, describe unquantified uncertainties transparently, and provide this information to decision-makers. When assessors cannot quantify identified uncertainties, they should qualitatively describe them and report them alongside the quantitative expression of overall uncertainty. This has important implications for reporting and decision-making.

<!-- SO p20, last bullet point -->
It is therefore important to quantify the overall impact of as many as possible of the identified uncertainties, and identify any that cannot be quantified. The most direct way to achieve this is to try to quantify the overall impact of all identified uncertainties, as this will reveal any that cannot be quantified.
:::

::: {.callout-tip collapse="true" appearance="minimal"}
# Conditionality of assessments

<!-- Move Types of assessment distinguished for uncertainty analysis to the U GD page -->

Assessments are conditional on any uncertainties not included in the quantitative assessment of overall uncertainty. This is because the assessment relies on assumptions about those uncertainties, and the assessment's output applies only if those assumptions are true. All assessments are inherently conditional based on the current state of scientific knowledge, the available information to assessors, and their judgments about the question at hand. Assessments assume that all relevant uncertainties are identified and there are no unknown unknowns. Additional conditionality arises when identified uncertainties are not quantified in the overall uncertainty assessment. The quantitative expression of overall uncertainty becomes conditional on the assumptions made for those unquantified uncertainties. Decision-making should consider the implications of conditionality, recognizing that assessments are based on scientific knowledge, do not account for unknown unknowns, and are influenced by the expertise and resources available. Assessors must provide a list of identified uncertainties not included in the quantitative assessment, along with descriptions and explanations. Decision-makers must decide how to address these unquantified uncertainties, potentially through further research or precautionary actions. Assessors should communicate clearly that they cannot assign probabilities or make quantitative judgments about the unquantified uncertainties. However, this information is still valuable for decision-makers, as it clarifies the limitations of science and guides further analysis or research. Decision-makers may have the ability to influence some unquantified uncertainties, such as implementing specific practices or policies.

:::