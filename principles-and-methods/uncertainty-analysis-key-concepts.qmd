---
title: "Key concepts"
---

## Well-defined questions and quantities of interest
<!-- SO p22 -->
The purpose of most EFSA scientific assessments is to determine what science can say about a quantity, event, proposition or state of the world that is of interest for decision-makers.

In order to express uncertainty about a question or quantity of interest in a clear and unambiguous way, it is necessary that the question or quantity itself is well-defined, so that it is interpreted in the same way by different people.

This applies both to the uncertainty analysis as a whole and to its parts.

::: {.callout-tip collapse="true" appearance="minimal"}
## Quantities of interest

can take one of two forms:

- *Non-variable quantities having a single value*
e.g. the total number of animals infected with a specified disease entering the EU in a given year. Many non-variable quantities in scientific assessment are parameters that describe variable quantities, which is potentially confusing. A common example of this is the mean body weight for a specified population at a specified time. The term non-variable is used in preference to ‘fixed’, to avoid giving the impression that the value is known with certainty. 
- *Variable quantities taking multiple values* such the body weights in a population. 
:::

::: {.callout-tip collapse="true" appearance="minimal"}
## Categorical questions of interest 

it is useful to distinguish between: 

- *Yes/no questions* e.g. questions referring to the presence or absence of some condition or state of the world, the occurrence or not of some event, or the exceedance or not of some quantitative threshold.

- *Questions with more than two categories of answer* e.g. different types of effect.
:::

::: {.callout-tip}
## Qualitative questions?
<!-- SO p24 -->
Readers may be surprised that the list of types of questions of interest does not include ‘qualitative’. This is because if a question of interest is well-defined, which it should always be for the reasons discussed above, then it can be treated as a yes/no question.
:::

::: {.callout-tip}
## Challenging questions or quantities of interest
<!-- SO p23 -->
The questions or quantities of interest in some EFSA assessments refer to things that may seem challenging to define in terms of the result of a hypothetical experiment or study. Examples include the condition or property of being genotoxic, and calculated quantities such as a Margin of Exposure, neither of which can be directly measured or observed. In practice, however, such questions or quantities can be defined by the procedures for determining them, as established in legislation or official guidance, i.e. the data that are required, and the criteria for interpreting those data.
:::

## Conditional nature of uncertainty

The uncertainty affecting a scientific assessment is a function of the knowledge that is relevant to the assessment and available to those conducting the assessment, at the time that it is conducted. 

Expressions of uncertainty are also conditional on the assessors involved. The task of uncertainty analysis is to express the uncertainty of the assessors regarding the question under assessment, at the time they conduct the assessment: there is no single ‘true’ uncertainty.

The conditional nature of knowledge and uncertainty means it is legitimate, and to be expected, that different experts within a group may give differing judgements of uncertainty for the same assessment question. Some structured approaches to eliciting judgements and characterising uncertainty elicit the judgement of the individual experts, explore the reasons for differing views and provide opportunities for convergence. A similar process occurs in reaching the consensus conclusion that is generally produced by an EFSA Panel.

The conditional nature of knowledge and uncertainty also contributes to cases where different
groups of assessors reach diverging opinions on the same issue; again this is relevant information for decision-making. Where differences in opinion arise between EFSA and other EU or Member State bodies, Article 30 of the Food Regulation includes provision for resolving or clarifying them and identifying the uncertainties involved.

## Uncertainty and variability 

It is important to take account of the distinction between uncertainty and variability, and also how they are related. Uncertainty refers to the state of knowledge, whereas variability refers to actual variation or heterogeneity in the real world. Both can be represented by probability distributions.

Uncertainty may be altered (either reduced or increased) by further research, because it results from limitations in knowledge, whereas variability cannot, because it refers to real differences that will not be altered by obtaining more knowledge. Our knowledge of variability is generally incomplete, so there is uncertainty about variability. In addition, some types of uncertainty are caused by variability. You can learn more about variability [on this page](../uncertainty-communication/expressions-distribution.qmd).

It is important that assessors distinguish uncertainty and variability because they have different implications for decision-making, informing decisions about whether to invest resources in research aimed at reducing uncertainty or in management options aimed at influencing variability.

How variability and uncertainty for each component of an assessment should be treated depends on whether the assessment question refers to the population or to a particular member of that population, how each component of the assessment contributes to that, and how those contributions are represented in the assessment model. Care is needed to determine when variability and uncertainty should be separated and when they should be combined, as inappropriate treatment may give misleading results.

## Dependencies

Variables in a scientific assessment can be interdependent. It is important to take account of dependencies between variables in assessment, because they can have a large effect on the result. This means that different combinations of values must be considered in proportion to their expected frequency, taking account of any dependencies, and excluding unrealistic or impossible combinations.

Sources of uncertainty can also be interdependent, such as where learning more about one question of quantity alters uncertainty about another. Dependencies between sources of uncertainty should be identified and accounted for, as they can greatly influence the overall uncertainty. 

Probabilistic calculations or probability bounds methods are preferable to expert judgments for assessing dependencies. 

Dependencies also exist in assessments using qualitative methods, and assessors should evaluate their impact on uncertainty. 


## Models and model uncertainty

Scientific assessments involve various models, both qualitative and quantitative. EFSA uses different types of models, including conceptual models, hazard/exposure ratios, deterministic and probabilistic models, individual-based probabilistic models, statistical models, and logic models. Uncertainties in model structure and inputs need to be considered and quantified where possible. Model uncertainties should be expressed as probability distributions or bounds for the difference between model outputs and the real quantities they represent. Models are simplifications of the real world, and while some directly address specific scenarios, others may address simplified scenarios or surrogate questions. The extrapolation from simplified models to the desired scenarios should be considered as a model uncertainty. Calibration of standardized assessment procedures is important for ensuring the reliability of simplified models.

## Evidence, agreement, confidence and weight of evidence

Evidence, weight of evidence, agreement, and confidence are all concepts related to uncertainty. Increasing the quantity, quality, consistency, and relevance of evidence or the degree of agreement between experts generally increases confidence and decreases uncertainty. However, the relationship between these concepts is complex and variable. Measures of evidence and agreement alone are insufficient as measures of uncertainty because they do not provide information on the range and probability of possible answers or values. Expressing evidence and agreement on qualitative scales can help structure the assessment process and facilitate discussions among experts. Confidence can be used both quantitatively, as in statistical analysis, where it represents a measure of uncertainty, and qualitatively, as a subjective measure of trust in a conclusion. Weight of evidence involves weighing multiple studies or lines of evidence against each other to assess the balance of evidence for or against different conclusions. Weight of evidence assessment and uncertainty analysis are closely related but do not capture all aspects of uncertainty. Additional considerations, such as the selection of evidence and methods for evaluating and integrating evidence, must be taken into account in uncertainty analysis.

## Influence, sensitivity and prioritisation of uncertainties

In uncertainty analysis, "influence" and "sensitivity" refer to the extent to which changes in the structure, parameters, and assumptions of an assessment affect the results. Sensitivity analysis quantitatively measures the impact of input changes on the output of a mathematical model, while influence analysis considers changes resulting from uncertainties and choices made in the assessment, including the assessment's structure and models used. Assessing influence can be done quantitatively or qualitatively, while sensitivity analysis focuses on quantitative measurements. These analyses are useful for evaluating the robustness of conclusions, prioritizing uncertainties for further analysis or data collection, and providing recommendations for future research. They play a key role in refining assessments iteratively and informing decisions on monitoring, data collection, and research priorities.

## Conservative assessments

Deterministic assessments used by EFSA are often designed to be "conservative," meaning they err on the side of caution and prioritize safety. Being conservative can refer to the choice of protection goals or dealing with uncertainty in the assessment itself. Conservative framing of the assessment question can simplify complex conditions or manage uncertainty in risk management. The term "conservative" can also relate to two concepts: "coverage," which refers to the probability of less adverse values, and "degree of uncertainty," which indicates how much the estimate might be reduced with further analysis. Decision-makers may view an assessment as insufficiently conservative if coverage is low or over-conservative if there is a high degree of uncertainty. Describing an estimate as conservative requires specifying the quantity of interest, management objective, and an acceptable probability threshold. Transparent communication requires clear definitions of adversity and probability. Similar considerations apply to qualitative and categorical assessments. Deterministic assessments with conservative assumptions are valuable if the required level of conservatism is defined and the assessment procedure is proven to provide it. Calibration of conservatism is crucial when using the same set of conservative assumptions in multiple assessments. Assessors may provide approximate values or bounds, while decision-makers can set upper limits on conservatism. Probability bounds analysis can be used to calculate a probability bound for assessment outputs by eliciting probability bounds for each input. Increased use of probability bounds analysis is recommended for case-specific assessments and calibrating standardized procedures.

## Expert judgement 

Assessing uncertainty involves expert judgment, which is subjective due to variations in knowledge and experience. Choices such as models, assessment scenarios, and data reliability rely on expert judgment. Even with hard data, assessing relevance and reliability involves subjectivity. Judgments about extrapolation and confidence intervals also require subjective consideration. Expert judgment is essential in scientific assessment but can be influenced by cognitive biases and group dynamics. Procedures are in place to manage conflicts of interest and mitigate biases. Formal approaches for expert knowledge elicitation (EKE) address psychological biases and facilitate aggregation of expert judgments. Extended participation of stakeholders and the public may be considered in cases with severe limits to knowledge. EFSA has published guidance on EKE and recognizes the need for streamlined approaches. Selection of experts should represent a wide range of scientific opinions, and consensus should not imply compromise. Differences of opinion and scientific uncertainty should be reflected in the assessment report. Data-driven analysis should be preferred when suitable, but expert judgment is necessary when data have limitations. Concerns about quantifying uncertainty using expert judgment have been addressed, and appropriate training should be provided to experts. Combining uncertainties through calculations is recommended when possible to inform judgments about overall uncertainty. Assumptions and additional uncertainties associated with calculation models should be considered, and combinations of inputs that cannot occur together in reality should be avoided. If uncertainty sources are combined through expert judgment, the added uncertainty should be taken into account.

## Probability 

Decision-makers require information on the range and probability of possible answers when dealing with uncertainty. There are two major views on using probability to quantify uncertainty. The frequentist view restricts probability to variability-based uncertainties and excludes uncertainties caused by knowledge limitations. The subjectivist (Bayesian) view allows probability to represent all types of uncertainties, including knowledge limitations. Subjective probability offers comparability and can be applied to well-defined questions. It allows the use of mathematical tools to handle combinations of uncertainties. The guidance encourages the use of subjective probability, except when quantifying uncertainty is too challenging. Frequentist probabilities must be reinterpreted as subjective probabilities to be combined appropriately. Probabilities derived from statistical analysis may have additional uncertainties that require expert judgment. Approximate probabilities, expressed as ranges, can be used when precise values are difficult to provide. They can reflect confidence in probability judgments and simplify assessments. Approximate probabilities can be computed using the same mathematics as precise probabilities.

## Overall uncertainty 

The recommendation to quantify uncertainty applies specifically to overall uncertainty, which refers to the assessors' uncertainty about the assessment conclusion, considering all relevant sources of uncertainty. Assessors should attempt to express the overall impact of identified uncertainties quantitatively, documenting qualitatively any uncertainties that cannot be quantified. In cases where qualitative reporting is required, quantitative evaluation of overall uncertainty is still necessary to determine a justified conclusion. However, overall uncertainty does not include information about unknown unknowns. The characterisation of uncertainty is dependent on the assessors, available evidence, time, and resources. Decision-makers should consider these factors when interpreting and using assessment conclusions.

## Unquantified uncertainties 

The term "unquantified uncertainties" refers to uncertainties identified by assessors that cannot be included in the quantitative expression of overall uncertainty. This section discusses the limits of quantifiability and explores what can be done about uncertainties that cannot be quantified. Assessors should strive to ensure that all questions or quantities in their assessments are well-defined. However, there are cases where assessors are unable to quantify uncertainties due to the inability to judge their magnitude or impact. These unquantified uncertainties, also known as "deep" uncertainties, often arise in complex or novel problems. Some authors distinguish between unquantifiable uncertainty and quantifiable risk, based on a frequentist view of probability. However, the guidance recommends using subjective probability to express all types of uncertainty, including variability, as long as the question or quantity of interest is well-defined. Nevertheless, assessors may still be unable to quantify certain uncertainties even when the questions or quantities are well-defined. Stirling's matrix defines different conditions of incertitude (risk, uncertainty, ambiguity, and ignorance) and suggests various methods for dealing with each condition. While some methods involve stakeholder participation and consideration of values, EFSA's role is limited to scientific assessment. EFSA's responsibility is to identify and quantify scientific sources of uncertainty, describe unquantified uncertainties transparently, and provide this information to decision-makers. When assessors cannot quantify identified uncertainties, they should qualitatively describe them and report them alongside the quantitative expression of overall uncertainty. This has important implications for reporting and decision-making.

<!-- SO p20, last bullet point -->
It is therefore important to quantify the overall impact of as many as possible of the identified uncertainties, and identify any that cannot be quantified. The most direct way to achieve this is to try to quantify the overall impact of all identified uncertainties, as this will reveal any that cannot be quantified.

## Conditionality of assessments

<!-- Move Types of assessment distinguished for uncertainty analysis to the U GD page -->

Assessments are conditional on any uncertainties not included in the quantitative assessment of overall uncertainty. This is because the assessment relies on assumptions about those uncertainties, and the assessment's output applies only if those assumptions are true. All assessments are inherently conditional based on the current state of scientific knowledge, the available information to assessors, and their judgments about the question at hand. Assessments assume that all relevant uncertainties are identified and there are no unknown unknowns. Additional conditionality arises when identified uncertainties are not quantified in the overall uncertainty assessment. The quantitative expression of overall uncertainty becomes conditional on the assumptions made for those unquantified uncertainties. Decision-making should consider the implications of conditionality, recognizing that assessments are based on scientific knowledge, do not account for unknown unknowns, and are influenced by the expertise and resources available. Assessors must provide a list of identified uncertainties not included in the quantitative assessment, along with descriptions and explanations. Decision-makers must decide how to address these unquantified uncertainties, potentially through further research or precautionary actions. Assessors should communicate clearly that they cannot assign probabilities or make quantitative judgments about the unquantified uncertainties. However, this information is still valuable for decision-makers, as it clarifies the limitations of science and guides further analysis or research. Decision-makers may have the ability to influence some unquantified uncertainties, such as implementing specific practices or policies.
