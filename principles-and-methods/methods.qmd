---
title: "Overview of methods for Uncertainty Analysis"
---

::: {.callout-tip collapse="true" appearance="minimal"}
## Identification of potentially relevant sources of uncertainty
<!-- Ch 8 SO -->
:::: {.panel-tabset}
### How
Sources of uncertainty can affect scientific assessment at different levels.

Assessors should be systematic in searching for sources of uncertainty affecting their assessment, by considering every part or component of their assessment in turn and checking whether different types of uncertainty are present.

### Inputs
Uncertainties affecting the inputs used in the scientific assessment are normally identified during the process of appraising the evidence, which is an intrinsic part of scientific assessment.

See [questions to identify sources of uncertainty affecting inputs](methods-sources-inputs.qmd)

### Assessment methodology
Other sources of uncertainties can be identified in relation to how the evidence is used in the assessment, including any models or reasoning that are used to draw conclusions.

See [questions to identify sources of uncertainty affecting assessment methodolgoy](methods-sources-methodology.qmd)

::::
:::


::: {.callout-tip collapse="true" appearance="minimal"}
## Methods for obtaining expert judgements
<!-- Ch 9 SO -->
:::: {.panel-tabset}
### Use of expert judgements
All scientific assessment involves the use of expert judgement

Where suitable data are available, this should be used in preference to relying solely on expert judgement. 

When data are strong, uncertainty may be quantified by statistical analysis, and any additional extrapolation or uncertainty addressed by **minimal assessment** (EFSA EKE GD), or collectively as part of the assessment of overall uncertainty.

When data are weak or diverse, it may be better to quantify uncertainty by expert judgement, supported by consideration of the data.

### Formal to less formal EKE
<!-- SO p49 -->
**Formal expert knowledge elicitation (EKE)** have been developed to counter psychological biases and to manage the sharing and aggregation of judgements between experts.

Formal elicitation requires significant time and resources, so it is not feasible to apply it to every source of uncertainty affecting an assessment. Instead, **semi-formal expert knowledge elicitation** can be applied on less important sources of uncertainty. 

Scientific judgements made, usually by a Working Group of experts preparing the assessment, are referred to in this document as judgements by **expert group judgement**.

In practice, there is not a dichotomy between more and less formal approaches to EKE, but rather a continuum.
::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
## Qualitative methods for analysing uncertainty
<!-- Ch 10 SO -->
### Use of qualitative methods
Qualitative methods characterise uncertainty using descriptive expression or ordinal scales, without quantitative definitions

<!-- SO p50 -->
They rely on careful use of language and expert judgement.

Qualitative methods may provide a useful aid for experts when making quantitative judgements. Please see examples of [qualitative methods](methods-qualitative.qmd).
:::


::: {.callout-tip collapse="true" appearance="minimal"}
## Methods for quantifying uncertainty
<!-- Ch 11 SO -->

[SO Section 11](https://efsa.onlinelibrary.wiley.com/doi/full/10.2903/j.efsa.2018.5122#efs25122-sec-0070-title) provides an overview of probabilistic and deterministic approaches for quantifying uncertainty. More detailed information on each method can be found in [SO Annex B](https://efsa.onlinelibrary.wiley.com/doi/full/10.2903/j.efsa.2018.5122#efs25122-sec-1002-title). While data play a crucial role, expert judgment should be complemented by statistical analysis whenever possible. The guidance emphasizes the use of probability as the preferred measure for quantifying uncertainty, since it offers a well-defined scale and allows for comparability between different uncertainties. Furthermore, probability enables the quantification of combined uncertainties in model outputs based on the probabilistic representation of input uncertainties. Another advantage of probability is its compatibility with Bayesian statistical analysis and the potential integration of results from non-Bayesian methods, facilitating uncertainty analysis. 

When the quantity of interest is variable, determining how to address variability in the assessment is crucial. The decision-maker should specify the aspect of variability they are interested in, whether it's the entire distribution or a particular aspect such as the worst case or a specific percentile. The choice of quantifying uncertainty depends on the aspect of interest and any models used to relate the quantity of interest to other variables.

Please, see the examples of [quantitative methods](methods-quantitative.qmd).

:::

::: {.callout-tip collapse="true" appearance="minimal"}
### Investigating influence and sensitivity

Sensitivity analysis (SA) is a set of methods used to assess the impact of model inputs and uncertainty expressions on the output of a quantitative model. Its objectives include prioritizing sources of uncertainty, identifying inputs for refined quantification or additional data collection, investigating the sensitivity of assumptions, and examining the sensitivity of overall uncertainty.

<!-- Ch 12 SO -->
:::: {.panel-tabset}

### Influence


The term "influence" refers to the overall impact of changes in the structure, parameters, and assumptions on assessment results. Sensitivity specifically refers to the quantitative impact of input uncertainties on output uncertainties in a quantitative model. Sensitivity analysis tools are discussed in [SO Section 12.1](https://efsa.onlinelibrary.wiley.com/doi/full/10.2903/j.efsa.2018.5122#efs25122-sec-0099-title). Other forms of influence can be explored by testing different scenarios and observing their effects on the assessment conclusion. Qualitative methods like the [NUSAP approach](methods-qualitative.qmd) and [uncertainty tables](methods-qualitative.qmd) can also be used. Expert judgment and formal/semi-formal elicitation techniques are valuable for assessing influence, particularly in determining which parameters should undergo formal sensitivity analysis.

### Sensitivity analysis

SA is typically performed for quantitative models but can also be applied to logic models to assess the sensitivity of conclusions. Expert judgments are crucial for defining the ranges of values to be investigated and choosing the analysis method.

SA helps allocate uncertainty about the output to sources of uncertainty in the inputs, identifying the main contributors to output uncertainty. Two main approaches exist: local SA, which examines infinitesimal changes in inputs, and global SA, which explores the effects of input changes over their entire range. Global SA methods are more relevant for EFSA assessments due to the need to consider the full range of possible values.

SA involves changing one parameter at a time (Nominal Range SA) or examining the combined effects of multiple changes, especially when interactions between inputs exist. However, SA cannot guide initial model design or the selection of sources of uncertainty for quantitative uncertainty analysis; expert judgment is necessary for these decisions. Subjective considerations of sensitivity should be carefully documented, and semi-formal expert elicitation may be appropriate when their impact on the assessment is significant.

### Methods

Methods for SA can be categorized as mathematical (deterministic), statistical (probabilistic), or graphical. They provide insights into which data sets, assumptions, or expert judgments require closer examination. Simple methods can be applied to assess the relative sensitivity of the output to individual variables and parameters. Separating the contributions of uncertainty and variability is an important issue in SA, and ongoing development is needed for situations where both components are present.

SA contributes to uncertainty analysis by analyzing the contributions of individual sources of uncertainty to the overall uncertainty of the assessment conclusion. It expresses the sensitivity of the assessment output quantitatively and/or graphically to input changes. Its strengths include structured identification of influential sources of uncertainty/variability, but it faces challenges in separately assessing the sensitivity to sources of uncertainty and variability.

Read more in [SO Annex B.17](https://efsa.onlinelibrary.wiley.com/doi/full/10.2903/j.efsa.2018.5122#efs25122-sec-0351-title)

::::
:::

::: {.callout-tip collapse="true" appearance="minimal"}
### Characterisation of overall uncertainty
<!-- Ch 14 SO -->

There are three options for quantifying the overall uncertainty, depending on the context: 

- Option 1: Make a single judgement of the overall impact of all the identified uncertainties. 

- Option 2: Quantify uncertainty separately in some parts of the assessment, combine them by calculation, and then adjust the result of the calculation by expert judgement to account for the additional uncertainties that are not yet included. 

- Option 3: Quantify uncertainty separately in some parts of the assessment and combine them by calculation, as in Option 2. Then quantify the contribution of the additional uncertainties separately, by expert judgement, and combine it with the previously quantified uncertainty by calculation.

```{r}
#| label: fig-overall-options
#| fig-cap: "Options for characterising overall uncertainty"
#| fig-alt: "Figure showing options for characterising overall uncertainty. Option 1-3 from quickest to most complex."
#| echo: false
knitr::include_graphics("../img/options_for_characterising_overall_uncertainty.png")
```

:::

::: {.callout-tip collapse="true" appearance="minimal"}
### Reporting uncertainty analysis in scientific assessments
<!-- Ch 15 SO -->

## Types of assessment

In reporting uncertainty analysis, EFSA's guidelines state that standardized assessments should mention the standard procedure followed and confirm the absence of non-standard uncertainties. If the applicability of the standard procedure is unclear, an explanation should be provided. For other assessments, uncertainty analysis should be reported with consideration for time constraints in urgent cases. In standardized assessments with non-standard uncertainties, the analysis should focus on those specific to the assessment.

## Sources of uncertainty

Transparency and consistency with EFSA's principles are essential in reporting uncertainty analysis. Sources of uncertainty should be identified, documented, evaluated, and combined, including the use of data, expert judgment, and chosen methodologies. Reference to other documents may suffice for described methods. The location of uncertainty analysis information should maximize accessibility in the assessment report.

Assessments must clearly identify sources of uncertainty and characterize their impact on the conclusion. Depending on the type of assessment, decision-makers or legislation may require specific reporting formats, such as qualitative descriptors or unqualified conclusions. In cases where practical certainty cannot be achieved, assessors should report inconclusive findings and comply with documentation requirements.

## Unquantified uncertainties

The assessment conclusion should include a clear statement of quantified uncertainty and a description of unquantified uncertainties. Quantified uncertainty can be expressed using probabilities or probability distributions. Unquantified uncertainties should be described in terms of their nature, cause, effects, difficulty in quantification, assumptions made, and suggestions for reduction or better characterization. Words implying magnitude or likelihood should be avoided.

## Overall uncertainty

A concise summary of the overall uncertainty characterization, suitable for the executive summary, should provide a simplified quantitative expression of the combined effect of quantified uncertainties and a brief description of unquantified uncertainties. Assessors should ensure compatibility between the reporting of uncertainty analysis and the assessment conclusions, revising both if necessary.

In some assessments, information on the main contributors to uncertainty may be useful for decision-makers, informing further actions such as data gathering and refinement of the assessment. Prioritization methods can be employed to identify these main contributors.

:::
